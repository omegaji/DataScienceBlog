{
  
    
        "post0": {
            "title": "Title",
            "content": "Hello welcome to my notebook on Covid DataAnalysis&lt;/p&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; import pandas as pd import numpy as np train_df=pd.read_csv(&quot;train.csv&quot;) train_df=train_df.drop(&quot;Province_State&quot;,axis=1) from datetime import datetime train_df[&quot;Date&quot;]=train_df[&#39;Date&#39;].apply(lambda x:datetime.strptime(x, &#39;%Y-%m-%d&#39;)) train_df[&quot;week&quot;]=&quot;week_&quot;+ str(train_df[&quot;Date&quot;].dt.week) train_df[&quot;week&quot;]=train_df[&quot;Date&quot;].dt.week.apply(lambda x: x) train_df[&quot;day&quot;]=train_df[&quot;Date&quot;].dt.day.apply(lambda x: x) . Above we used the data column to generate days and week number for out dataset starting from week 1 to week 17 . &lt;/p&gt; now below i will print the dataframe . &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; train_df.head() . Id Country_Region Date ConfirmedCases Fatalities week day . 0 1 | Afghanistan | 2020-01-22 | 0.0 | 0.0 | 4 | 22 | . 1 2 | Afghanistan | 2020-01-23 | 0.0 | 0.0 | 4 | 23 | . 2 3 | Afghanistan | 2020-01-24 | 0.0 | 0.0 | 4 | 24 | . 3 4 | Afghanistan | 2020-01-25 | 0.0 | 0.0 | 4 | 25 | . 4 5 | Afghanistan | 2020-01-26 | 0.0 | 0.0 | 4 | 26 | . from sklearn.preprocessing import LabelEncoder,StandardScaler,MinMaxScaler from sklearn.model_selection import train_test_split . now below we do the necessary processing of our data to make it more viable for our Scaling and model preparation . train_df[&quot;month&quot;]=train_df[&quot;Date&quot;].dt.month.apply(lambda x: int(x)) . train_df[&#39;ConfirmedCases&#39;] = train_df[&#39;ConfirmedCases&#39;].apply(int) train_df[&#39;Fatalities&#39;] = train_df[&#39;Fatalities&#39;].apply(int) . now we will take out the data related to our country India in a seperate dataframe . india_df=train_df[train_df[&quot;Country_Region&quot;]==&quot;India&quot;] india_df.head() . Id Country_Region Date ConfirmedCases Fatalities week day month . 13160 15961 | India | 2020-01-22 | 0 | 0 | 4 | 22 | 1 | . 13161 15962 | India | 2020-01-23 | 0 | 0 | 4 | 23 | 1 | . 13162 15963 | India | 2020-01-24 | 0 | 0 | 4 | 24 | 1 | . 13163 15964 | India | 2020-01-25 | 0 | 0 | 4 | 25 | 1 | . 13164 15965 | India | 2020-01-26 | 0 | 0 | 4 | 26 | 1 | . import matplotlib.pyplot as plt #print(plt.style.available) plt.style.use(&#39;seaborn-darkgrid&#39;) . india_df.groupby([&quot;week&quot;]).max().reset_index().plot(kind=&quot;bar&quot;,x=&quot;week&quot;,y=&quot;ConfirmedCases&quot;) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7f43c0b401d0&gt; . india_df.groupby([&quot;week&quot;]).max().reset_index().plot(kind=&quot;bar&quot;,x=&quot;week&quot;,y=&quot;Fatalities&quot;) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7f43c1e07e90&gt; . The proceeses done are seperate the confirmed cases to a seperate variable called case(Y variable) | we use the LabelEncoder() instead of one hot encoding to encode the country regions | we then use the fit_transform function to label encode | We then use MinMaxScaler for normalizing the values as there are various values from 0 to more than 1000 | &lt;/p&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; cases = train_df.ConfirmedCases fatal=train_df.Fatalities lb = LabelEncoder() del train_df[&quot;Date&quot;] del train_df[&quot;Fatalities&quot;] del train_df[&quot;ConfirmedCases&quot;] #del train_df[&quot;Id&quot;] train_df[&#39;Country_Region&#39;] = lb.fit_transform(train_df[&#39;Country_Region&#39;]) scaler = MinMaxScaler() x_train = scaler.fit_transform(train_df.values) X_train, X_test, y_train, y_test = train_test_split(train_df, cases, test_size=0.2, random_state=0) X_train = scaler.fit_transform(X_train.values) X_test = scaler.fit_transform(X_test.values) . Here We can see we Check on XGBoost Regressor with below hyperparameters . from xgboost import XGBRegressor model = XGBRegressor(n_estimators = 2500 , random_state = 0 , max_depth = 27) model.fit(X_train,y_train) . XGBRegressor(base_score=0.5, booster=None, colsample_bylevel=1, colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1, importance_type=&#39;gain&#39;, interaction_constraints=None, learning_rate=0.300000012, max_delta_step=0, max_depth=27, min_child_weight=1, missing=nan, monotone_constraints=None, n_estimators=2500, n_jobs=0, num_parallel_tree=1, objective=&#39;reg:squarederror&#39;, random_state=0, reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1, tree_method=None, validate_parameters=False, verbosity=None) . model.score(X_train,y_train) . 0.99999999999996 . Here we plot the number of cases in india month wise(1,1.5..4(april))&lt;/p&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; import seaborn as sns ig=india_df.groupby([&quot;month&quot;]).max().reset_index() sns.lineplot(ig[&quot;month&quot;],ig[&quot;ConfirmedCases&quot;],ci=None) ig[&quot;month&quot;] . 0 1 1 2 2 3 3 4 Name: month, dtype: int64 . below we predict the values using our X_test,y_test we created . Next we also plot the predicted values and acutal values of the model in a scatter plot and as we can see the points form more of a X=Y line(meaning that the predicted and actual values are almost equal) . print(&quot;the score for the test dataaset is &quot;+str(model.score(X_test,y_test))) predicted=model.predict(X_test) import seaborn as sns import matplotlib.pyplot as plt #sns.lineplot(y_train) plt.xlabel(&quot;Predicted Value&quot;) plt.ylabel(&quot;Actual Value&quot;) plt.title(&quot;the graph is similar to x=y curve&quot;) plt.scatter(predicted,y_test) . the score for the test dataaset is 0.991170270240357 . &lt;matplotlib.collections.PathCollection at 0x7f43c1eee590&gt; . below we predict the values using our X_test,y_test we created Next we also plot the predicted values and acutal values of the model in a scatter plot and as we can see the points form more of a X=Y line(meaning that the predicted and actual values are almost equal) . train_predict=model.predict(X_train) plt.xlabel(&quot;Predicted Value(training)&quot;) plt.ylabel(&quot;Actual Value(traiing)&quot;) plt.title(&quot;the graph is similar to x=y curve&quot;) plt.scatter(train_predict,y_train) . &lt;matplotlib.collections.PathCollection at 0x7f43c1f4e490&gt; . from sklearn.metrics import explained_variance_score y_pred=model.predict(X_test) . Now we Calculate 3 types of errors, explained variance score, max error score,mean squared error&lt;/p&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; from sklearn.metrics import explained_variance_score from sklearn.metrics import max_error from sklearn.metrics import mean_squared_error print(&quot;The expalined variance score is==&gt;&quot;+ str(explained_variance_score(y_test,y_pred))) print(&quot;The max error score is ==&gt;&quot;+str(max_error(y_test,y_pred))) print(&quot;The mean squared error is ==&gt;&quot;+str(mean_squared_error(y_test,y_pred))) . The expalined variance score is==&gt;0.9912504663704602 The max error score is ==&gt;24370.158203125 The mean squared error is ==&gt;1096115.2159031187 . pred_df=pd.DataFrame() pred_df[&quot;Predictions&quot;]=pd.Series(np.around(y_pred,decimals=0)) pred_df[&quot;True_values&quot;]=pd.Series(y_test.values) . Here we form a dataframe consisting predicted data and true data&lt;/p&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; pred_df . Predictions True_values . 0 0.0 | 0 | . 1 417.0 | 447 | . 2 2.0 | 0 | . 3 319.0 | 318 | . 4 0.0 | 0 | . ... ... | ... | . 5880 -0.0 | 0 | . 5881 245.0 | 245 | . 5882 -0.0 | 0 | . 5883 0.0 | 0 | . 5884 -0.0 | 0 | . 5885 rows × 2 columns . here we use the STACKING ALGORITHIM we choose 3 algorithims XGBoost(the one above we used) | RandomForest regressor | SVM Regressor | &lt;/p&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; from sklearn.ensemble import RandomForestRegressor from sklearn import svm rf = RandomForestRegressor(n_estimators = 500, random_state = 42) models = [ svm.SVR(), RandomForestRegressor(random_state=0, n_jobs=-1, n_estimators=500, max_depth=3), model ] . !pip install vecstack . Requirement already satisfied: vecstack in /home/omegaji/anaconda3/lib/python3.7/site-packages (0.4.0) Requirement already satisfied: numpy in /home/omegaji/anaconda3/lib/python3.7/site-packages (from vecstack) (1.18.1) Requirement already satisfied: scipy in /home/omegaji/anaconda3/lib/python3.7/site-packages (from vecstack) (1.4.1) Requirement already satisfied: scikit-learn&gt;=0.18 in /home/omegaji/anaconda3/lib/python3.7/site-packages (from vecstack) (0.22.1) Requirement already satisfied: joblib&gt;=0.11 in /home/omegaji/anaconda3/lib/python3.7/site-packages (from scikit-learn&gt;=0.18-&gt;vecstack) (0.14.1) . from vecstack import stacking from sklearn.metrics import r2_score S_train, S_test = stacking(models, X_train, y_train, X_test, regression=True, mode=&#39;oof_pred_bag&#39;, needs_proba=False, save_dir=None, metric=r2_score, n_folds=5, #stratified=True, shuffle=True, random_state=0, verbose=2) . task: [regression] metric: [r2_score] mode: [oof_pred_bag] n_models: [3] model 0: [SVR] fold 0: [-0.01849384] fold 1: [-0.02135447] fold 2: [-0.01936172] fold 3: [-0.01934595] fold 4: [-0.01877062] - MEAN: [-0.01946532] + [0.00100202] FULL: [-0.01898844] model 1: [RandomForestRegressor] fold 0: [0.16408069] fold 1: [0.23211451] fold 2: [0.11892497] fold 3: [0.13180512] fold 4: [0.14895139] - MEAN: [0.15917534] + [0.03954222] FULL: [0.15628386] model 2: [XGBRegressor] fold 0: [0.98925196] fold 1: [0.99284200] fold 2: [0.99307733] fold 3: [0.99309471] fold 4: [0.99233834] - MEAN: [0.99212087] + [0.00146021] FULL: [0.99187249] . final_model = model.fit(S_train, y_train) y_pred = final_model.predict(S_test) . the training score For the Stacked Model is predicted below . final_model.score(S_train,y_train) . 0.9999999999999664 . The testing score is predicted below . final_model.score(S_test,y_test) . 0.9869640680032792 . Similary we plot the predicted data vs actual confirmed cases for both training and testing accuracy as shown bellow . . we can see the plot still resembles and x=y curve hence a good prediction value close to the actual value . plt.xlabel(&quot;Predicted Value(training)&quot;) train_pred=model.predict(S_train) plt.ylabel(&quot;Actual Value(training)&quot;) plt.title(&quot;the graph is similar to x=y curve&quot;) plt.scatter(train_pred,y_train) . &lt;matplotlib.collections.PathCollection at 0x7f43c1f4ead0&gt; . plt.xlabel(&quot;Predicted Value(testing)&quot;) train_pred=model.predict(S_train) plt.ylabel(&quot;Actual Value(testing)&quot;) plt.title(&quot;the graph is similar to x=y curve&quot;) plt.scatter(y_pred,y_test) . &lt;matplotlib.collections.PathCollection at 0x7f43c2a34550&gt; . Now we test each mode individually on how it scores and out best model is the Xgboost which we tested earlier and also the curve was very similar to x=y We start with using SVM Regressor which is the 0th model in our MODELS list (hence below code contains model[0]) . . print(&quot;USING SVM REGRESSOR MODEL &quot;) models[0].fit(S_train,y_train) print(&quot;the training score is&quot;) print(models[0].score(S_train,y_train)) print(&quot;the testing score is&quot;) print(models[0].score(S_test,y_test)) . USING SVM REGRESSOR MODEL the training score is -0.0017980567557247529 the testing score is 0.001936770722922643 . we can see the training accuracy and testing accuracy are not very efficient and cannot even be consider a good model it is a bad model . following code plots the models training predictions to actual values and testing predictions to actual test values . train_pred=models[0].predict(S_train) plt.xlabel(&quot;Predicted Value(training)&quot;) plt.ylabel(&quot;Actual Value(training)&quot;) plt.title(&quot;the graph is NOT an x=y curve at any level &quot;) plt.scatter(train_pred,y_train) . &lt;matplotlib.collections.PathCollection at 0x7f43c287a410&gt; . plt.xlabel(&quot;Predicted Value(testing)&quot;) train_pred=models[0].predict(S_test) plt.ylabel(&quot;Actual Value(testing)&quot;) plt.title(&quot;the graph is also NOT X=y&quot;) plt.scatter(train_pred,y_test) . &lt;matplotlib.collections.PathCollection at 0x7f43c33a2c50&gt; . NOW WE DO THE SAME FOR OUR RANDOM FOREST MODEL WHICH SHOWS A GOOD SCORE TOO&lt;/p&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; print(&quot;USING RANDOM FOREST MODEL &quot;) models[1].fit(S_train,y_train) print(&quot;the training score is&quot;) print(models[1].score(S_train,y_train)) print(&quot;the testing score is&quot;) print(models[1].score(S_test,y_test)) print(models[1]) . USING RANDOM FOREST MODEL the training score is 0.9850564601052662 the testing score is 0.9839541082692184 RandomForestRegressor(bootstrap=True, ccp_alpha=0.0, criterion=&#39;mse&#39;, max_depth=3, max_features=&#39;auto&#39;, max_leaf_nodes=None, max_samples=None, min_impurity_decrease=0.0, min_impurity_split=None, min_samples_leaf=1, min_samples_split=2, min_weight_fraction_leaf=0.0, n_estimators=500, n_jobs=-1, oob_score=False, random_state=0, verbose=0, warm_start=False) . train_pred=models[1].predict(S_train) plt.xlabel(&quot;Predicted Value(training)&quot;) plt.ylabel(&quot;Actual Value(training)&quot;) plt.title(&quot;the graph is NOT an x=y curve at any level&quot; ) plt.scatter(train_pred,y_train) . &lt;matplotlib.collections.PathCollection at 0x7f43e08e5150&gt; . plt.xlabel(&quot;Predicted Value(testing)&quot;) train_pred=models[1].predict(S_test) plt.ylabel(&quot;Actual Value(testing)&quot;) plt.title(&quot;the graph is also NOT X=y&quot;) plt.scatter(train_pred,y_test) . &lt;matplotlib.collections.PathCollection at 0x7f43c1dfee10&gt; . THANK You!!&lt;/p&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; # from sklearn import neighbors # knn = neighbors.KNeighborsRegressor(1000, weights=&quot;uniform&quot;) # from sklearn.linear_model import ElasticNet # en=ElasticNet(random_state=0) # models = [ # knn, # RandomForestRegressor(random_state=0, n_jobs=-1, # n_estimators=1000, max_depth=3), # model, # en # ] # S_train, S_test = stacking(models, # X_train, y_train[&quot;ConfirmedCases&quot;], X_test, # regression=True, # mode=&#39;oof_pred_bag&#39;, # needs_proba=False, # save_dir=None, # metric=r2_score, # shuffle=True, # random_state=0, # verbose=2) # final_model_2 = model.fit(S_train, y_train[&quot;ConfirmedCases&quot;]) # y_pred = final_model_2.predict(S_test) # final_model.score(S_train,y_train[&quot;ConfirmedCases&quot;]) . &lt;/div&gt; . . . .",
            "url": "https://omegaji.github.io/DataScienceBlog/2020/05/22/try_train.html",
            "relUrl": "/2020/05/22/try_train.html",
            "date": " • May 22, 2020"
        }
        
    
  
    
        ,"post1": {
            "title": "Title",
            "content": "#&quot;Covid-19 India analysis and prediction&quot; &gt; &quot;Covid-19 India analysis and prediction&quot; - toc: false - branch: master - badges: true - comments: true - categories: [fastpages, jupyter] - image: images/some_folder/your_image.png - hide: false - search_exclude: true - metadata_key1: metadata_value1 - metadata_key2: metadata_value2 from IPython.display import HTML HTML(fig.to_html()) . Hello Welcome to my kernel this is my first Proper kernel with some EDA and choropleth maps DO UPVOTE IF YOU LIKE IT :D let&#39;s dive into what I have done below i have simply loaded the kaggle provided datasets . # This Python 3 environment comes with many helpful analytics libraries installed # It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python # For example, here&#39;s several helpful packages to load in import numpy as np # linear algebra import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv) # Input data files are available in the &quot;../input/&quot; directory. # For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory import os for dirname, _, filenames in os.walk(&#39;/kaggle/input&#39;): for filename in filenames: print(os.path.join(dirname, filename)) # Any results you write to the current directory are saved as output. . /kaggle/input/covid19-global-forecasting-week-4/test.csv /kaggle/input/covid19-global-forecasting-week-4/train.csv /kaggle/input/covid19-global-forecasting-week-4/submission.csv /kaggle/input/covid19-in-india/StatewiseTestingDetails.csv /kaggle/input/covid19-in-india/IndividualDetails.csv /kaggle/input/covid19-in-india/covid_19_india.csv /kaggle/input/covid19-in-india/HospitalBedsIndia.csv /kaggle/input/covid19-in-india/ICMRTestingDetails.csv /kaggle/input/covid19-in-india/population_india_census2011.csv /kaggle/input/covid19-in-india/AgeGroupDetails.csv /kaggle/input/covid19-in-india/ICMRTestingLabs.csv /kaggle/input/india-shape/ind_shape/IND_adm1.prj /kaggle/input/india-shape/ind_shape/IND_adm2.shx /kaggle/input/india-shape/ind_shape/IND_adm3.shx /kaggle/input/india-shape/ind_shape/IND_adm0.dbf /kaggle/input/india-shape/ind_shape/IND_adm3.csv /kaggle/input/india-shape/ind_shape/IND_adm0.csv /kaggle/input/india-shape/ind_shape/IND_adm1.csv /kaggle/input/india-shape/ind_shape/IND_adm0.shp /kaggle/input/india-shape/ind_shape/IND_adm3.prj /kaggle/input/india-shape/ind_shape/IND_adm1.dbf /kaggle/input/india-shape/ind_shape/IND_adm2.shp /kaggle/input/india-shape/ind_shape/IND_adm3.shp /kaggle/input/india-shape/ind_shape/IND_adm0.shx /kaggle/input/india-shape/ind_shape/IND_adm1.cpg /kaggle/input/india-shape/ind_shape/IND_adm2.cpg /kaggle/input/india-shape/ind_shape/IND_adm2.prj /kaggle/input/india-shape/ind_shape/IND_adm0.cpg /kaggle/input/india-shape/ind_shape/license.txt /kaggle/input/india-shape/ind_shape/IND_adm2.csv /kaggle/input/india-shape/ind_shape/IND_adm2.dbf /kaggle/input/india-shape/ind_shape/IND_adm0.prj /kaggle/input/india-shape/ind_shape/IND_adm3.dbf /kaggle/input/india-shape/ind_shape/IND_adm1.shx /kaggle/input/india-shape/ind_shape/IND_adm1.shp /kaggle/input/india-shape/ind_shape/IND_adm3.cpg /kaggle/input/countryshape/ne_110m_admin_0_countries.shp /kaggle/input/countryshape/ne_110m_admin_0_countries.README.html /kaggle/input/countryshape/ne_110m_admin_0_countries.VERSION.txt /kaggle/input/countryshape/ne_110m_admin_0_countries.dbf /kaggle/input/countryshape/ne_110m_admin_0_countries.prj /kaggle/input/countryshape/ne_110m_admin_0_countries.shx /kaggle/input/countryshape/ne_110m_admin_0_countries.cpg . train_df=pd.read_csv(&quot;/kaggle/input/covid19-global-forecasting-week-4/train.csv&quot;) . Below I have taken the india part out of the dataframe provided and did some plotting . train_df[train_df[&quot;Country_Region&quot;]==&quot;India&quot;] . Id Province_State Country_Region Date ConfirmedCases Fatalities . 13440 15961 | NaN | India | 2020-01-22 | 0.0 | 0.0 | . 13441 15962 | NaN | India | 2020-01-23 | 0.0 | 0.0 | . 13442 15963 | NaN | India | 2020-01-24 | 0.0 | 0.0 | . 13443 15964 | NaN | India | 2020-01-25 | 0.0 | 0.0 | . 13444 15965 | NaN | India | 2020-01-26 | 0.0 | 0.0 | . ... ... | ... | ... | ... | ... | ... | . 13531 16052 | NaN | India | 2020-04-22 | 21370.0 | 681.0 | . 13532 16053 | NaN | India | 2020-04-23 | 23077.0 | 721.0 | . 13533 16054 | NaN | India | 2020-04-24 | 24530.0 | 780.0 | . 13534 16055 | NaN | India | 2020-04-25 | 26283.0 | 825.0 | . 13535 16056 | NaN | India | 2020-04-26 | 27890.0 | 881.0 | . 96 rows × 6 columns . india_df=train_df[train_df[&quot;Country_Region&quot;]==&quot;India&quot;] #india_df[&quot;day&quot;]=india_df[&quot;Date&quot;].apply(lambda x:int(x[-2:]) ) #india_df[&quot;Month&quot;]=india_df[&quot;Date&quot;].apply(months ) india_df[&quot;ConfirmedCases&quot;]=india_df[&quot;ConfirmedCases&quot;].apply(lambda x: int(x)) . /opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:5: SettingWithCopyWarning: A value is trying to be set on a copy of a slice from a DataFrame. Try using .loc[row_indexer,col_indexer] = value instead See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy &#34;&#34;&#34; . NOW IN THIS WHOLE NOTEBOOK I HAVE SPLIT THE DATE INTO WEEKS.....IF YOU SEE WEEK 4 IT MEANS IT IS THE 4th WEEK OF THE YEAR!!!!! NOT MONTH . from datetime import datetime india_df[&quot;Date&quot;]=india_df[&#39;Date&#39;].apply(lambda x:datetime.strptime(x, &#39;%Y-%m-%d&#39;)) . /opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:2: SettingWithCopyWarning: A value is trying to be set on a copy of a slice from a DataFrame. Try using .loc[row_indexer,col_indexer] = value instead See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy . . Below I have made a week column and added it to the dataframe and did some simple plottings I have done using SEABORN CATPLOT . india_df[&quot;week&quot;]=&quot;week_&quot;+ str(india_df[&quot;Date&quot;].dt.week) . /opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:1: SettingWithCopyWarning: A value is trying to be set on a copy of a slice from a DataFrame. Try using .loc[row_indexer,col_indexer] = value instead See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy &#34;&#34;&#34;Entry point for launching an IPython kernel. . india_df[&quot;week&quot;]=india_df[&quot;Date&quot;].dt.week.apply(lambda x: x) . /opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:1: SettingWithCopyWarning: A value is trying to be set on a copy of a slice from a DataFrame. Try using .loc[row_indexer,col_indexer] = value instead See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy &#34;&#34;&#34;Entry point for launching an IPython kernel. . import matplotlib.pyplot as plt import seaborn as sns #fig.set_size_inches(12, 18) sns.catplot(data=india_df.groupby([&quot;week&quot;]).max().reset_index(),x=&quot;week&quot;,y=&quot;ConfirmedCases&quot;,kind=&quot;bar&quot;) . &lt;seaborn.axisgrid.FacetGrid at 0x7f1bad636f98&gt; . train_df[&quot;day&quot;]=train_df[&quot;Date&quot;].apply(lambda x:int(x[-2:]) ) #train_df[&quot;Month&quot;]=train_df[&quot;Date&quot;].apply(months ) train_df[&quot;ConfirmedCases&quot;]=train_df[&quot;ConfirmedCases&quot;].apply(lambda x: int(x)) . sns.catplot(data=india_df.groupby([&quot;week&quot;]).max().reset_index(),x=&quot;week&quot;,y=&quot;Fatalities&quot;,kind=&quot;bar&quot;) . &lt;seaborn.axisgrid.FacetGrid at 0x7f1be5865898&gt; . train_df=train_df.drop(&quot;Province_State&quot;,axis=1) from datetime import datetime from datetime import datetime train_df[&quot;Date&quot;]=train_df[&#39;Date&#39;].apply(lambda x:datetime.strptime(x, &#39;%Y-%m-%d&#39;)) train_df[&quot;week&quot;]=&quot;week_&quot;+ str(train_df[&quot;Date&quot;].dt.week) train_df[&quot;week&quot;]=train_df[&quot;Date&quot;].dt.week.apply(lambda x: x) train_df[&quot;day&quot;]=train_df[&quot;Date&quot;].dt.day.apply(lambda x: x) train_df[&quot;month&quot;]=train_df[&quot;Date&quot;].dt.month.apply(lambda x: int(x)) . train_df=train_df.drop(&quot;Date&quot;,axis=1) . Below I have used Plotly to create a CHOROPLETH Map of The CoronaVirus to the latest week . import numpy as np import pandas as pd import plotly as py import plotly.express as px import plotly.graph_objs as go from plotly.subplots import make_subplots from plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot init_notebook_mode(connected=True) country_df=train_df.groupby([&#39;Country_Region&#39;, &#39;week&#39;]).max().reset_index().sort_values(&#39;week&#39;, ascending=False) country_df = country_df.drop_duplicates(subset = [&#39;Country_Region&#39;]) country_df = country_df[country_df[&#39;ConfirmedCases&#39;]&gt;0] data = dict(type=&#39;choropleth&#39;, locations = country_df[&#39;Country_Region&#39;], locationmode = &#39;country names&#39;, z = country_df[&#39;ConfirmedCases&#39;], text = country_df[&#39;Country_Region&#39;], colorbar = {&#39;title&#39;:&#39;CONFIRMED CASES&#39;}, colorscale=[[0, &#39;rgb(224,255,255)&#39;], [0.01, &#39;rgb(166,206,227)&#39;], [0.02, &#39;rgb(31,120,180)&#39;], [0.03, &#39;rgb(178,223,138)&#39;], [0.05, &#39;rgb(51,160,44)&#39;], [0.10, &#39;rgb(251,154,153)&#39;], [0.20, &#39;rgb(255,255,0)&#39;], [1, &#39;rgb(227,26,28)&#39;]], reversescale = False ) layout = dict(title=&#39;COVID-19 CASES AROUND THE WORLD&#39;, geo = dict(showframe = True, projection={&#39;type&#39;:&#39;mercator&#39;})) choromap = go.Figure(data = [data], layout = layout) iplot(choromap, validate=False) . This again using plotly I have created the map which you can interact with the slider to see how the spread of coronavirus has affected the Countries starting from the 4th week of the year that was in January and till now in April YOU CAN HOVER FOR INFO OF THE CASES . df_countrydate = train_df[train_df[&#39;ConfirmedCases&#39;]&gt;0] df_countrydate = df_countrydate.groupby([&#39;week&#39;,&#39;Country_Region&#39;]).max().reset_index() df_countrydate fig = px.choropleth(df_countrydate, locations=&quot;Country_Region&quot;, locationmode = &quot;country names&quot;, color=&quot;ConfirmedCases&quot;, hover_name=&quot;Country_Region&quot;, animation_frame=&quot;week&quot;, color_continuous_scale=&quot;Greens&quot; ) fig.update_layout( title_text = &#39;Global Spread of Coronavirus&#39;, title_x = 0.5, geo=dict( showframe = False, showcoastlines = False, )) fig.show() . df_countrydate[df_countrydate[&quot;Country_Region&quot;]==&quot;India&quot;] . week Country_Region Id ConfirmedCases Fatalities day month . 19 5 | India | 15972 | 2 | 0.0 | 31 | 2 | . 45 6 | India | 15979 | 3 | 0.0 | 9 | 2 | . 72 7 | India | 15986 | 3 | 0.0 | 16 | 2 | . 99 8 | India | 15993 | 3 | 0.0 | 23 | 2 | . 146 9 | India | 16000 | 3 | 0.0 | 29 | 3 | . 223 10 | India | 16007 | 39 | 0.0 | 8 | 3 | . 342 11 | India | 16014 | 113 | 2.0 | 15 | 3 | . 494 12 | India | 16021 | 396 | 7.0 | 22 | 3 | . 665 13 | India | 16028 | 1024 | 27.0 | 29 | 3 | . 844 14 | India | 16035 | 3588 | 99.0 | 31 | 4 | . 1027 15 | India | 16042 | 9205 | 331.0 | 12 | 4 | . 1211 16 | India | 16049 | 17615 | 559.0 | 19 | 4 | . 1395 17 | India | 16056 | 27890 | 881.0 | 26 | 4 | . I would like to thank Mr. SRK for the dataset on COVID-19 IN INDIA ===&gt; https://www.kaggle.com/sudalairajkumar/covid19-in-india which i have used below . df_india=pd.read_csv(&quot;/kaggle/input/covid19-in-india/covid_19_india.csv&quot;) df_india[&quot;Date&quot;]=df_india[&#39;Date&#39;].apply(lambda x:datetime.strptime(x, &#39;%d/%m/%y&#39;)) df_india[&quot;week&quot;]=&quot;week_&quot;+ str(df_india[&quot;Date&quot;].dt.week) df_india[&quot;week&quot;]=df_india[&quot;Date&quot;].dt.week.apply(lambda x: x) df_india.head() df_india_grouped=df_india.groupby([&quot;State/UnionTerritory&quot;,&quot;week&quot;]).max().reset_index().sort_values(&quot;week&quot;,ascending=False) df_india_grouped=df_india_grouped.drop_duplicates(subset=[&quot;State/UnionTerritory&quot;]) . The Bar Plot provides info About the Statewise Confirmed Cases, You can hover on them . df_india_grouped fig = px.scatter(df_india_grouped, x=&quot;Confirmed&quot;, y=&quot;State/UnionTerritory&quot;, title=&quot;COVID CASES CONFIRMED IN INDIAN STATES&quot;, labels={&quot;COVID CASES CONFIRMED IN INDIAN STATES&quot;} # customize axis label ) fig = px.bar(df_india_grouped, x=&#39;Confirmed&#39;, y=&#39;State/UnionTerritory&#39;, hover_data=[&#39;Confirmed&#39;, &#39;State/UnionTerritory&#39;], color=&#39;Confirmed&#39;, orientation=&#39;h&#39;, text=&quot;Confirmed&quot;, height=1400) fig.update_traces( textposition=&#39;outside&#39;) fig.update_layout(uniformtext_minsize=8, uniformtext_mode=&#39;show&#39;) fig.show() . Here I have added a new column called pending which is basically how many patients are still being treated , I am going to use this in the below piechart I have created for each state depicting the states and how many cases are cured, deaths and pending . df_india_grouped[&quot;pending&quot;]=df_india_grouped[&quot;Confirmed&quot;]-df_india_grouped[&quot;Deaths&quot;]-df_india_grouped[&quot;Cured&quot;] . &#39;&#39;&#39;df_india_grouped labels=df_india_grouped[&quot;State/UnionTerritory&quot;] values=df_india_grouped[&quot;Confirmed&quot;] fig = go.Figure(data=[go.Pie(labels=labels, values=values, hole=.3)]) fig.show()&#39;&#39;&#39; l=list(df_india_grouped[&quot;State/UnionTerritory&quot;]) fig = make_subplots(rows=11, cols=3,subplot_titles=l,specs=[[{&#39;type&#39;:&#39;domain&#39;}, {&#39;type&#39;:&#39;domain&#39;},{&#39;type&#39;:&#39;domain&#39;}],[{&#39;type&#39;:&#39;domain&#39;}, {&#39;type&#39;:&#39;domain&#39;},{&#39;type&#39;:&#39;domain&#39;}],[{&#39;type&#39;:&#39;domain&#39;}, {&#39;type&#39;:&#39;domain&#39;},{&#39;type&#39;:&#39;domain&#39;}],[{&#39;type&#39;:&#39;domain&#39;}, {&#39;type&#39;:&#39;domain&#39;},{&#39;type&#39;:&#39;domain&#39;}],[{&#39;type&#39;:&#39;domain&#39;}, {&#39;type&#39;:&#39;domain&#39;},{&#39;type&#39;:&#39;domain&#39;}],[{&#39;type&#39;:&#39;domain&#39;}, {&#39;type&#39;:&#39;domain&#39;},{&#39;type&#39;:&#39;domain&#39;}],[{&#39;type&#39;:&#39;domain&#39;}, {&#39;type&#39;:&#39;domain&#39;},{&#39;type&#39;:&#39;domain&#39;}],[{&#39;type&#39;:&#39;domain&#39;}, {&#39;type&#39;:&#39;domain&#39;},{&#39;type&#39;:&#39;domain&#39;}],[{&#39;type&#39;:&#39;domain&#39;}, {&#39;type&#39;:&#39;domain&#39;},{&#39;type&#39;:&#39;domain&#39;}],[{&#39;type&#39;:&#39;domain&#39;}, {&#39;type&#39;:&#39;domain&#39;},{&#39;type&#39;:&#39;domain&#39;}],[{&#39;type&#39;:&#39;domain&#39;}, {&#39;type&#39;:&#39;domain&#39;},{&#39;type&#39;:&#39;domain&#39;}]]) a=1 b=1 for i in l: temp_df=df_india_grouped[df_india_grouped[&quot;State/UnionTerritory&quot;]==i] #print(int(temp_df[&quot;Deaths&quot;])) values=[int(temp_df[&quot;Deaths&quot;]),int(temp_df[&quot;Cured&quot;]),int(temp_df[&quot;pending&quot;])] labels=[&quot;Deaths&quot;,&quot;Cured&quot;,&quot;pending&quot;] #annot.append(dict(text=i,font_size=10, showarrow=False)) fig.add_trace(go.Pie(labels=labels, textposition=&quot;inside&quot;,values=values, name=i),a, b) if b==3 and a&lt;11: a=a+1 if b+1&gt;3: b=1 else: b=b+1 fig.update_traces(hole=.4) fig.update_layout( height=1900,width=1000 ) fig.update(layout_title_text=&#39;StateWise analysis of Positive cases&#39;) #fig = go.Figure(fig) fig.show() #iplot(fig) . testing_df=pd.read_csv(&quot;/kaggle/input/covid19-in-india/StatewiseTestingDetails.csv&quot;) . testing_df[&quot;Date&quot;]=testing_df[&#39;Date&#39;].apply(lambda x:datetime.strptime(x, &#39;%Y-%m-%d&#39;)) testing_df[&quot;week&quot;]=&quot;week_&quot;+ str(testing_df[&quot;Date&quot;].dt.week) testing_df[&quot;week&quot;]=testing_df[&quot;Date&quot;].dt.week.apply(lambda x: x) testing_df.head() testing_df_grouped=testing_df.groupby([&quot;State&quot;,&quot;week&quot;]).max().reset_index().sort_values(&quot;week&quot;,ascending=False) . testing_df_grouped=testing_df_grouped.drop_duplicates(subset=[&quot;State&quot;]) . states=list(testing_df_grouped[&quot;State&quot;]) fig = go.Figure(data=[ go.Bar(name=&#39;Negative&#39;, x=states, y=list(testing_df_grouped[&quot;Negative&quot;])), go.Bar(name=&#39;Positive&#39;, x=states, y=list(testing_df_grouped[&quot;Positive&quot;])), ]) fig.update_layout(barmode=&#39;stack&#39;) fig.show() . Now it is time to plot another choropleth map but this time for India staetwise, for this I added a dataset containing the shape files indian state . import geopandas as gpd shapefile=&quot;/kaggle/input/india-shape/ind_shape/IND_adm1.shp&quot; gdf=gpd.read_file(shapefile)[[&quot;NAME_1&quot;,&quot;geometry&quot;]] gdf.columns = [&#39;states&#39;,&#39;geometry&#39;] gdf.loc[31,&quot;states&quot;]=&quot;Telengana&quot; gdf.loc[34,&quot;states&quot;]=&quot;Uttarakhand&quot; gdf.loc[25,&quot;states&quot;]=&quot;Odisha&quot; #gdf[gdf[&quot;states&quot;]==&quot;Orissa&quot;] . Below I have merged the Geopandas dataframe containing geometry and state names with our dataset of covid-19 indian states and used a json converted to convert it into json . merged_grouped = gdf.merge(df_india_grouped, left_on = &#39;states&#39;, right_on = &#39;State/UnionTerritory&#39;).drop([&quot;Date&quot;],axis=1) import json merged_json_grouped = json.loads(merged_grouped.to_json()) json_data_grouped = json.dumps(merged_json_grouped) . I have used Bokeh instaed of plotly here instead of plotly to demonstarte another method that we can create Choropleth map although we can see it requires more code and can get complicated . from bokeh.io import output_notebook, show, output_file from bokeh.plotting import figure from bokeh.models import GeoJSONDataSource, LinearColorMapper, ColorBar,LabelSet from bokeh.palettes import brewer from bokeh.models import Slider, HoverTool geosource = GeoJSONDataSource(geojson = json_data_grouped) palette = brewer[&#39;YlGnBu&#39;][8] palette = palette[::-1] color_mapper = LinearColorMapper(palette = palette, low = 0, high = max(merged_grouped[&quot;Confirmed&quot;])) tick_labels = {&#39;0&#39;: &#39;0&#39;, &#39;100&#39;: &#39;100&#39;, &#39;200&#39;:&#39;200&#39;, &#39;400&#39;:&#39;400&#39;, &#39;800&#39;:&#39;800&#39;, &#39;1200&#39;:&#39;1200&#39;, &#39;1400&#39;:&#39;1400&#39;,&#39;1800&#39;:&#39;1800&#39;, &#39;2000&#39;: &#39;2000&#39;} hover = HoverTool(tooltips = [ (&#39;states&#39;,&#39;@states&#39;),(&#39;Confirmed_Cases&#39;, &#39;@Confirmed&#39;)]) color_bar = ColorBar(color_mapper=color_mapper, label_standoff=8,width = 500, height = 20, border_line_color=None,location = (0,0), orientation = &#39;horizontal&#39;, major_label_overrides = tick_labels) p = figure(title = &#39;CoronaVirus Confirmed States(HOVER MOUSE FOR INFO)&#39;, plot_height = 600 , plot_width = 950, toolbar_location = None,tools=[hover]) p.xgrid.grid_line_color = None p.ygrid.grid_line_color = None p.patches(&#39;xs&#39;,&#39;ys&#39;, source = geosource,fill_color = {&#39;field&#39; :&#39;Confirmed&#39;, &#39;transform&#39; : color_mapper},name=&quot;states&quot;, line_color = &#39;black&#39;, line_width = 0.25, fill_alpha = 1) labels = LabelSet(x=&#39;xs&#39;, y=&#39;ys&#39;, text=&#39;states&#39;, x_offset=5, y_offset=5, source=geosource) p.add_layout(color_bar, &#39;below&#39;) output_notebook() #Display figure. show(p) . Loading BokehJS ... country_df=df_india.groupby([&quot;week&quot;,&quot;State/UnionTerritory&quot;]).max().reset_index() country_df.drop([&quot;Date&quot;,&quot;ConfirmedIndianNational&quot;,&quot;ConfirmedForeignNational&quot;,&quot;Deaths&quot;,&quot;Cured&quot;,&quot;Time&quot;],axis=1,inplace=True) . Now over here the same way we created the animation of the world map before , we want to create it similary for inidian states, we are using plotly instead of Bokeh because for Bokeh we needed to create a bokeh server to get that interactivity , but we can simply get it more easily with plotly ALSO NOTE:** Below in the code i have used geoseries function SIMPLIFY() as the plot created was very laggy due to the multiploygon geometry of the indian states so using SIMPLIFY(Tolerance=0.02) which kind of straightens some wiggles and curves to a line, but still I think a 0.02 tolerance provides an accurate shape of the map . shapefile=&quot;/kaggle/input/india-shape/ind_shape/IND_adm1.shx&quot; gdf=gpd.read_file(shapefile)[[&quot;NAME_1&quot;,&quot;geometry&quot;]] gdf[&quot;geometry&quot;]=gdf[&quot;geometry&quot;].simplify(0.02, preserve_topology=True) gdf gdf.columns = [&#39;states&#39;,&#39;geometry&#39;] gdf.loc[31,&quot;states&quot;]=&quot;Telengana&quot; gdf.loc[34,&quot;states&quot;]=&quot;Uttarakhand&quot; gdf.loc[25,&quot;states&quot;]=&quot;Odisha&quot; merged_grouped = gdf#.merge(df_india_grouped[[&quot;State/UnionTerritory&quot;,&quot;geo&quot;]], left_on = &#39;states&#39;, right_on = &#39;State/UnionTerritory&#39;) merged_json_grouped = json.loads(merged_grouped.to_json()) json_data_grouped = json.dumps(merged_json_grouped) for i in merged_json_grouped[&quot;features&quot;]: i[&quot;id&quot;]=i[&quot;properties&quot;][&quot;states&quot;] . country_df=df_india.groupby([&quot;State/UnionTerritory&quot;,&quot;week&quot;]).max().reset_index().sort_values(&quot;week&quot;,ascending=True) country_df=country_df.drop([&#39;Sno&#39;, &#39;Date&#39;, &#39;Time&#39;, &#39;ConfirmedIndianNational&#39;, &#39;ConfirmedForeignNational&#39;, &#39;Cured&#39;, &#39;Deaths&#39;,],axis=1) . This Map created using plotly is interactive starting from week 4 , we can see it started from kerala and within few week it was massively spread over the Indian States . fig = px.choropleth(country_df, geojson=merged_json_grouped, locations=&quot;State/UnionTerritory&quot;, color=&quot;Confirmed&quot;, hover_name=&quot;State/UnionTerritory&quot;, animation_frame=&quot;week&quot;, color_continuous_scale=[&quot;yellow&quot;,&quot;orange&quot;,&quot;red&quot;], labels={&#39;Confirmed&#39;:&#39;Confirmed&#39;} ) fig.update_geos(fitbounds=&quot;locations&quot;, visible=False,projection_type=&quot;natural earth&quot;) fig.update_layout( title_text = &#39;India Spread of Coronavirus&#39;, title_x = 0.5, geo=dict( showframe = False, showcoastlines = False, )) fig.show() . Now we go for modelling our data , I am going to use XGBOOST although I am still working and on different models so this could be updated again , If you have any suggestions please do tell me in the comments :D . newtestdf=pd.read_csv(&quot;/kaggle/input/covid19-global-forecasting-week-4/test.csv&quot;) newtestdf . ForecastId Province_State Country_Region Date . 0 1 | NaN | Afghanistan | 2020-04-02 | . 1 2 | NaN | Afghanistan | 2020-04-03 | . 2 3 | NaN | Afghanistan | 2020-04-04 | . 3 4 | NaN | Afghanistan | 2020-04-05 | . 4 5 | NaN | Afghanistan | 2020-04-06 | . ... ... | ... | ... | ... | . 13454 13455 | NaN | Zimbabwe | 2020-05-10 | . 13455 13456 | NaN | Zimbabwe | 2020-05-11 | . 13456 13457 | NaN | Zimbabwe | 2020-05-12 | . 13457 13458 | NaN | Zimbabwe | 2020-05-13 | . 13458 13459 | NaN | Zimbabwe | 2020-05-14 | . 13459 rows × 4 columns . using inbuilt pandas encoder i encoded the names of the country regions . train_df.head() . Id Country_Region ConfirmedCases Fatalities day week month . 0 1 | Afghanistan | 0 | 0.0 | 22 | 4 | 1 | . 1 2 | Afghanistan | 0 | 0.0 | 23 | 4 | 1 | . 2 3 | Afghanistan | 0 | 0.0 | 24 | 4 | 1 | . 3 4 | Afghanistan | 0 | 0.0 | 25 | 4 | 1 | . 4 5 | Afghanistan | 0 | 0.0 | 26 | 4 | 1 | . train_df.columns . Index([&#39;Id&#39;, &#39;Country_Region&#39;, &#39;ConfirmedCases&#39;, &#39;Fatalities&#39;, &#39;day&#39;, &#39;week&#39;, &#39;month&#39;], dtype=&#39;object&#39;) . I took inspiration of the hyperparamters from here https://www.kaggle.com/pradeepkumarrajkumar/xgb-regressor . We scale using minmaxscaler and also transform the country data into numeric using label encoding(not get_dummies as i did not get a good score before :D) . from sklearn.preprocessing import LabelEncoder,StandardScaler,MinMaxScaler train_df[&#39;ConfirmedCases&#39;] = train_df[&#39;ConfirmedCases&#39;].apply(int) train_df[&#39;Fatalities&#39;] = train_df[&#39;Fatalities&#39;].apply(int) cases = train_df.ConfirmedCases fatal=train_df.Fatalities lb = LabelEncoder() del train_df[&quot;Fatalities&quot;] del train_df[&quot;ConfirmedCases&quot;] #del train_df[&quot;Id&quot;] train_df[&#39;Country_Region&#39;] = lb.fit_transform(train_df[&#39;Country_Region&#39;]) scaler = MinMaxScaler() X_train = scaler.fit_transform(train_df.drop([&quot;Id&quot;,&quot;week&quot;],axis=1).values) . from xgboost import XGBRegressor model = XGBRegressor(n_estimators = 2500 , random_state = 0 , max_depth = 27) model.fit(X_train,cases) . XGBRegressor(base_score=0.5, booster=None, colsample_bylevel=1, colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1, importance_type=&#39;gain&#39;, interaction_constraints=None, learning_rate=0.300000012, max_delta_step=0, max_depth=27, min_child_weight=1, missing=nan, monotone_constraints=None, n_estimators=2500, n_jobs=0, num_parallel_tree=1, objective=&#39;reg:squarederror&#39;, random_state=0, reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1, tree_method=None, validate_parameters=False, verbosity=None) . model.score(X_train,cases) . 0.5526724827782268 . newtestdf[&quot;Date&quot;]=newtestdf[&#39;Date&#39;].apply(lambda x:datetime.strptime(x, &#39;%Y-%m-%d&#39;)) newtestdf[&quot;week&quot;]=&quot;week_&quot;+ str(newtestdf[&quot;Date&quot;].dt.week) newtestdf[&quot;week&quot;]=newtestdf[&quot;Date&quot;].dt.week.apply(lambda x: x) newtestdf[&quot;day&quot;]=newtestdf[&quot;Date&quot;].dt.day.apply(lambda x: x) newtestdf[&quot;month&quot;]=newtestdf[&quot;Date&quot;].dt.month.apply(lambda x: int(x)) newtestdf[&#39;Country_Region&#39;] = lb.fit_transform(newtestdf[&#39;Country_Region&#39;]) newtestdf=newtestdf.drop([&quot;Province_State&quot;,&quot;Date&quot;],axis=1) newtestdf . ForecastId Country_Region week day month . 0 1 | 0 | 14 | 2 | 4 | . 1 2 | 0 | 14 | 3 | 4 | . 2 3 | 0 | 14 | 4 | 4 | . 3 4 | 0 | 14 | 5 | 4 | . 4 5 | 0 | 15 | 6 | 4 | . ... ... | ... | ... | ... | ... | . 13454 13455 | 183 | 19 | 10 | 5 | . 13455 13456 | 183 | 20 | 11 | 5 | . 13456 13457 | 183 | 20 | 12 | 5 | . 13457 13458 | 183 | 20 | 13 | 5 | . 13458 13459 | 183 | 20 | 14 | 5 | . 13459 rows × 5 columns . X_test = scaler.fit_transform(newtestdf.drop([&quot;ForecastId&quot;,&quot;week&quot;],axis=1).values) cases_pred = model.predict(X_test) . cases_pred = np.around(cases_pred,decimals = 0) x_train_cas = [] for i in range(len(X_train)): x = list(X_train[i]) x.append(cases[i]) x_train_cas.append(x) . x_train_cas = np.array(x_train_cas) model = XGBRegressor(n_estimators = 2500 , random_state = 0 , max_depth = 27) model.fit(x_train_cas,fatal) . XGBRegressor(base_score=0.5, booster=None, colsample_bylevel=1, colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1, importance_type=&#39;gain&#39;, interaction_constraints=None, learning_rate=0.300000012, max_delta_step=0, max_depth=27, min_child_weight=1, missing=nan, monotone_constraints=None, n_estimators=2500, n_jobs=0, num_parallel_tree=1, objective=&#39;reg:squarederror&#39;, random_state=0, reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1, tree_method=None, validate_parameters=False, verbosity=None) . x_test_cas = [] for i in range(len(X_test)): x = list(X_test[i]) x.append(cases_pred[i]) x_test_cas.append(x) x_test_cas[0] . [0.0, 0.034482758620689655, 0.0, -0.0] . x_test_cas = np.array(x_test_cas) fatalities_pred =model.predict(x_test_cas) fatalities_pred = np.around(fatalities_pred,decimals = 0) . submission = pd.read_csv(&quot;../input/covid19-global-forecasting-week-4/submission.csv&quot;) submission[&#39;ConfirmedCases&#39;] = cases_pred submission[&#39;Fatalities&#39;] = fatalities_pred submission.to_csv(&quot;submission.csv&quot; , index = False) . THANK YOU :D PLEASE DO UPVOTE!!!!! . THANK YOU!!!! . File &#34;&lt;ipython-input-44-99bf4749447d&gt;&#34;, line 1 THANK YOU!!!! ^ SyntaxError: invalid syntax .",
            "url": "https://omegaji.github.io/DataScienceBlog/2020/05/21/covid-19-india-maps-eda-xgboost.html",
            "relUrl": "/2020/05/21/covid-19-india-maps-eda-xgboost.html",
            "date": " • May 21, 2020"
        }
        
    
  
    
        ,"post2": {
            "title": "Fastpages Notebook Blog Post",
            "content": "About . This notebook is a demonstration of some of capabilities of fastpages with notebooks. . With fastpages you can save your jupyter notebooks into the _notebooks folder at the root of your repository, and they will be automatically be converted to Jekyll compliant blog posts! . Front Matter . The first cell in your Jupyter Notebook or markdown blog post contains front matter. Front matter is metadata that can turn on/off options in your Notebook. It is formatted like this: . # &quot;My Title&quot; &gt; &quot;Awesome summary&quot; - toc:true- branch: master- badges: true- comments: true - author: Hamel Husain &amp; Jeremy Howard - categories: [fastpages, jupyter] . Setting toc: true will automatically generate a table of contents | Setting badges: true will automatically include GitHub and Google Colab links to your notebook. | Setting comments: true will enable commenting on your blog post, powered by utterances. | . The title and description need to be enclosed in double quotes only if they include special characters such as a colon. More details and options for front matter can be viewed on the front matter section of the README. . Markdown Shortcuts . A #hide comment at the top of any code cell will hide both the input and output of that cell in your blog post. . A #hide_input comment at the top of any code cell will only hide the input of that cell. . The comment #hide_input was used to hide the code that produced this. . put a #collapse-hide flag at the top of any cell if you want to hide that cell by default, but give the reader the option to show it: . #collapse-hide import pandas as pd import altair as alt . . put a #collapse-show flag at the top of any cell if you want to show that cell by default, but give the reader the option to hide it: . #collapse-show cars = &#39;https://vega.github.io/vega-datasets/data/cars.json&#39; movies = &#39;https://vega.github.io/vega-datasets/data/movies.json&#39; sp500 = &#39;https://vega.github.io/vega-datasets/data/sp500.csv&#39; stocks = &#39;https://vega.github.io/vega-datasets/data/stocks.csv&#39; flights = &#39;https://vega.github.io/vega-datasets/data/flights-5k.json&#39; . . Interactive Charts With Altair . Charts made with Altair remain interactive. Example charts taken from this repo, specifically this notebook. . Example 1: DropDown . # single-value selection over [Major_Genre, MPAA_Rating] pairs # use specific hard-wired values as the initial selected values selection = alt.selection_single( name=&#39;Select&#39;, fields=[&#39;Major_Genre&#39;, &#39;MPAA_Rating&#39;], init={&#39;Major_Genre&#39;: &#39;Drama&#39;, &#39;MPAA_Rating&#39;: &#39;R&#39;}, bind={&#39;Major_Genre&#39;: alt.binding_select(options=genres), &#39;MPAA_Rating&#39;: alt.binding_radio(options=mpaa)} ) # scatter plot, modify opacity based on selection alt.Chart(movies).mark_circle().add_selection( selection ).encode( x=&#39;Rotten_Tomatoes_Rating:Q&#39;, y=&#39;IMDB_Rating:Q&#39;, tooltip=&#39;Title:N&#39;, opacity=alt.condition(selection, alt.value(0.75), alt.value(0.05)) ) . Example 2: Tooltips . alt.Chart(movies).mark_circle().add_selection( alt.selection_interval(bind=&#39;scales&#39;, encodings=[&#39;x&#39;]) ).encode( x=&#39;Rotten_Tomatoes_Rating:Q&#39;, y=alt.Y(&#39;IMDB_Rating:Q&#39;, axis=alt.Axis(minExtent=30)), # use min extent to stabilize axis title placement tooltip=[&#39;Title:N&#39;, &#39;Release_Date:N&#39;, &#39;IMDB_Rating:Q&#39;, &#39;Rotten_Tomatoes_Rating:Q&#39;] ).properties( width=600, height=400 ) . Example 3: More Tooltips . # select a point for which to provide details-on-demand label = alt.selection_single( encodings=[&#39;x&#39;], # limit selection to x-axis value on=&#39;mouseover&#39;, # select on mouseover events nearest=True, # select data point nearest the cursor empty=&#39;none&#39; # empty selection includes no data points ) # define our base line chart of stock prices base = alt.Chart().mark_line().encode( alt.X(&#39;date:T&#39;), alt.Y(&#39;price:Q&#39;, scale=alt.Scale(type=&#39;log&#39;)), alt.Color(&#39;symbol:N&#39;) ) alt.layer( base, # base line chart # add a rule mark to serve as a guide line alt.Chart().mark_rule(color=&#39;#aaa&#39;).encode( x=&#39;date:T&#39; ).transform_filter(label), # add circle marks for selected time points, hide unselected points base.mark_circle().encode( opacity=alt.condition(label, alt.value(1), alt.value(0)) ).add_selection(label), # add white stroked text to provide a legible background for labels base.mark_text(align=&#39;left&#39;, dx=5, dy=-5, stroke=&#39;white&#39;, strokeWidth=2).encode( text=&#39;price:Q&#39; ).transform_filter(label), # add text labels for stock prices base.mark_text(align=&#39;left&#39;, dx=5, dy=-5).encode( text=&#39;price:Q&#39; ).transform_filter(label), data=stocks ).properties( width=700, height=400 ) . Data Tables . You can display tables per the usual way in your blog: . movies = &#39;https://vega.github.io/vega-datasets/data/movies.json&#39; df = pd.read_json(movies) # display table with pandas df[[&#39;Title&#39;, &#39;Worldwide_Gross&#39;, &#39;Production_Budget&#39;, &#39;Distributor&#39;, &#39;MPAA_Rating&#39;, &#39;IMDB_Rating&#39;, &#39;Rotten_Tomatoes_Rating&#39;]].head() . Title Worldwide_Gross Production_Budget Distributor MPAA_Rating IMDB_Rating Rotten_Tomatoes_Rating . 0 The Land Girls | 146083.0 | 8000000.0 | Gramercy | R | 6.1 | NaN | . 1 First Love, Last Rites | 10876.0 | 300000.0 | Strand | R | 6.9 | NaN | . 2 I Married a Strange Person | 203134.0 | 250000.0 | Lionsgate | None | 6.8 | NaN | . 3 Let&#39;s Talk About Sex | 373615.0 | 300000.0 | Fine Line | None | NaN | 13.0 | . 4 Slam | 1087521.0 | 1000000.0 | Trimark | R | 3.4 | 62.0 | . Images . Local Images . You can reference local images and they will be copied and rendered on your blog automatically. You can include these with the following markdown syntax: . ![](my_icons/fastai_logo.png) . . Remote Images . Remote images can be included with the following markdown syntax: . ![](https://image.flaticon.com/icons/svg/36/36686.svg) . . Animated Gifs . Animated Gifs work, too! . ![](https://upload.wikimedia.org/wikipedia/commons/7/71/ChessPawnSpecialMoves.gif) . . Captions . You can include captions with markdown images like this: . ![](https://www.fast.ai/images/fastai_paper/show_batch.png &quot;Credit: https://www.fast.ai/2020/02/13/fastai-A-Layered-API-for-Deep-Learning/&quot;) . . Other Elements . GitHub Flavored Emojis . Typing I give this post two :+1:! will render this: . I give this post two :+1:! . Tweetcards . Typing &gt; twitter: https://twitter.com/jakevdp/status/1204765621767901185?s=20 will render this: Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 . Youtube Videos . Typing &gt; youtube: https://youtu.be/XfoYk_Z5AkI will render this: . Boxes / Callouts . Typing &gt; Warning: There will be no second warning! will render this: . Warning: There will be no second warning! . Typing &gt; Important: Pay attention! It&#39;s important. will render this: . Important: Pay attention! It&#8217;s important. . Typing &gt; Tip: This is my tip. will render this: . Tip: This is my tip. . Typing &gt; Note: Take note of this. will render this: . Note: Take note of this. . Typing &gt; Note: A doc link to [an example website: fast.ai](https://www.fast.ai/) should also work fine. will render in the docs: . Note: A doc link to an example website: fast.ai should also work fine. . Footnotes . You can have footnotes in notebooks, however the syntax is different compared to markdown documents. This guide provides more detail about this syntax, which looks like this: . For example, here is a footnote {% fn 1 %}. And another {% fn 2 %} {{ &#39;This is the footnote.&#39; | fndetail: 1 }} {{ &#39;This is the other footnote. You can even have a [link](www.github.com)!&#39; | fndetail: 2 }} . For example, here is a footnote 1. . And another 2 . 1. This is the footnote.↩ . 2. This is the other footnote. You can even have a link!↩ .",
            "url": "https://omegaji.github.io/DataScienceBlog/jupyter/2020/02/20/test.html",
            "relUrl": "/jupyter/2020/02/20/test.html",
            "date": " • Feb 20, 2020"
        }
        
    
  
    
        ,"post3": {
            "title": "An Example Markdown Post",
            "content": "Example Markdown Post . Basic setup . Jekyll requires blog post files to be named according to the following format: . YEAR-MONTH-DAY-filename.md . Where YEAR is a four-digit number, MONTH and DAY are both two-digit numbers, and filename is whatever file name you choose, to remind yourself what this post is about. .md is the file extension for markdown files. . The first line of the file should start with a single hash character, then a space, then your title. This is how you create a “level 1 heading” in markdown. Then you can create level 2, 3, etc headings as you wish but repeating the hash character, such as you see in the line ## File names above. . Basic formatting . You can use italics, bold, code font text, and create links. Here’s a footnote 1. Here’s a horizontal rule: . . Lists . Here’s a list: . item 1 | item 2 | . And a numbered list: . item 1 | item 2 | Boxes and stuff . This is a quotation . . You can include alert boxes …and… . . You can include info boxes Images . . Code . You can format text and code per usual . General preformatted text: . # Do a thing do_thing() . Python code and output: . # Prints &#39;2&#39; print(1+1) . 2 . Formatting text as shell commands: . echo &quot;hello world&quot; ./some_script.sh --option &quot;value&quot; wget https://example.com/cat_photo1.png . Formatting text as YAML: . key: value - another_key: &quot;another value&quot; . Tables . Column 1 Column 2 . A thing | Another thing | . Tweetcards . Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 Footnotes . This is the footnote. &#8617; . |",
            "url": "https://omegaji.github.io/DataScienceBlog/markdown/2020/01/14/test-markdown-post.html",
            "relUrl": "/markdown/2020/01/14/test-markdown-post.html",
            "date": " • Jan 14, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "This is where you put the contents of your About page. Like all your pages, it’s in Markdown format. . This website is powered by fastpages 1. . a blogging platform that natively supports Jupyter notebooks in addition to other formats. &#8617; . |",
          "url": "https://omegaji.github.io/DataScienceBlog/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://omegaji.github.io/DataScienceBlog/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}