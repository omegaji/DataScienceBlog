{
  
    
        "post0": {
            "title": "COVID-19 RESEARCH PPRINT DATA EDA",
            "content": "import pandas as pd df=pd.read_csv(&quot;COVID-19-Preprint-Data_ver2.csv&quot;) . Welcome to this notebook, Here lets get a glimpse of the data and list the columns . print(df.columns) df.head() . Index([&#39;DOI&#39;, &#39;Date of Upload&#39;, &#39;Title of preprint&#39;, &#39;Preprint Link&#39;, &#39;Abstract&#39;, &#39;Number of Authors&#39;, &#39;Authors&#39;, &#39;Author(s) Institutions&#39;, &#39;Uploaded Site&#39;], dtype=&#39;object&#39;) . DOI Date of Upload Title of preprint Preprint Link Abstract Number of Authors Authors Author(s) Institutions Uploaded Site . 0 10.1101/2020.05.13.20088732 | 2020-05-16 | MRI of the lungs in patients with COVID-19: cl... | http://medrxiv.org/cgi/content/short/2020.05.1... | Objective: To evaluate an applicability of lun... | 16 | [&#39;Yuriy Vasilev&#39;, &#39;Kristina Sergunova&#39;, &#39;Alexa... | {&quot;Research and Practical Clinical Center for D... | medrxiv | . 1 10.1101/2020.05.15.098616 | 2020-05-15 | SARS-CoV2 (COVID-19) Structural/Evolution Dyna... | http://biorxiv.org/cgi/content/short/2020.05.1... | The SARS-CoV-2 pandemic, starting in 2019, has... | 22 | [&#39;Ruchir Gupta&#39;, &#39;Jacob Charron&#39;, &#39;Cynthia Ste... | {&quot;Michigan State University&quot;: 9, &quot;University o... | biorxiv | . 2 10.1101/2020.05.10.20069732 | 2020-05-15 | COVID-19 and Environmental factors. A PRISMA-c... | http://medrxiv.org/cgi/content/short/2020.05.1... | The emergence of a novel human coronavirus, SA... | 3 | [&#39;Apostolos Vantarakis&#39;, &#39;Ioanna Chatziprodrom... | {&quot;University of Patras&quot;: 2, &quot;Department of Phy... | medrxiv | . 3 10.1101/2020.05.14.097311 | 2020-05-15 | Analytical and Clinical Comparison of Three Nu... | http://biorxiv.org/cgi/content/short/2020.05.1... | Severe Acute Respiratory Syndrome Coronavirus ... | 6 | [&#39;Elizabeth Smith&#39;, &#39;Wei Zhen&#39;, &#39;Ryhana Manji&#39;... | {&quot;Northwell Health Laboratories&quot;: 5, &quot;Northwel... | biorxiv | . 4 10.1101/2020.05.14.093054 | 2020-05-15 | A single dose SARS-CoV-2 simulating particle v... | http://biorxiv.org/cgi/content/short/2020.05.1... | Coronavirus disease 2019 (COVID-19) is caused ... | 17 | [&#39;Yujia Cai&#39;, &#39;Di Yin&#39;, &#39;Sikai Ling&#39;, &#39;Xiaolon... | {&quot;Shanghai Jiao Tong University&quot;: 11, &quot;Fudan U... | biorxiv | . Ah of course the first thing I always would do is to split the Datetime into day,month,year and day in year(like 1st febuary is the 32nd day of the year , this helps in plotting :D) . from datetime import datetime df[&quot;day&quot;]=df[&quot;Date of Upload&quot;].apply(lambda x: int(datetime.strptime(x,&#39;%Y-%m-%d&#39;).day)) df[&quot;month&quot;]=df[&quot;Date of Upload&quot;].apply(lambda x:int( datetime.strptime(x,&#39;%Y-%m-%d&#39;).month)) df[&quot;year&quot;]=df[&quot;Date of Upload&quot;].apply(lambda x: int( datetime.strptime(x,&#39;%Y-%m-%d&#39;).year)) df[&quot;day_in_year&quot;]=df[&quot;Date of Upload&quot;].apply(lambda x:int( datetime.strptime(x,&#39;%Y-%m-%d&#39;).timetuple().tm_yday)) . I am going to drop the columns below as I do not aim to use it in my EDA . df.drop([&quot;Preprint Link&quot;,&quot;DOI&quot;,&quot;Date of Upload&quot;],axis=1,inplace=True) . Our first plot in the notebook is a simple one and I have done it using Altair, here I plot the number of abstracts(or generally papers ) submitted per day -X= day in the year | -Y= the count of the number of abstracts(research papers) published in that particular day | . the below plot is a bubble plot which increases its size as the number of abstracts per day, you can hover for more info . &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; import altair as alt alt.Chart(df.groupby([&quot;day_in_year&quot;]).count().reset_index()).mark_point().encode( x=&#39;day_in_year&#39;, y=&#39;Abstract&#39;, tooltip=[&quot;Abstract&quot;,&quot;day_in_year&quot;], size=&quot;Abstract&quot; ).interactive() . df[df[&quot;day_in_year&quot;]==137] . Title of preprint Abstract Number of Authors Authors Author(s) Institutions Uploaded Site day month year day_in_year . 0 MRI of the lungs in patients with COVID-19: cl... | Objective: To evaluate an applicability of lun... | 16 | [&#39;Yuriy Vasilev&#39;, &#39;Kristina Sergunova&#39;, &#39;Alexa... | {&quot;Research and Practical Clinical Center for D... | medrxiv | 16 | 5 | 2020 | 137 | . we can see above that after a certain day (about 137th day of the year) which is 16th May 2020 . Below we are going to plot the number of abstracts(research papers) published in the given months of 2020, we first filter out only months of 2020 . alt.Chart(df[df[&quot;year&quot;]==2020].groupby([&quot;month&quot;]).count().reset_index()).mark_area( line={&#39;color&#39;:&#39;darkblue&#39;}, color=alt.Gradient( gradient=&#39;linear&#39;, stops=[alt.GradientStop(color=&#39;white&#39;, offset=0), alt.GradientStop(color=&#39;blue&#39;, offset=1)], x1=1, x2=1, y1=1, y2=0 ) ).encode( alt.X(&#39;month&#39;), alt.Y(&#39;Abstract&#39;,title=&quot;Abstract Count published&quot;), tooltip=[&quot;month&quot;,&quot;Abstract&quot;] ).interactive() . Over here we will import nltk stopwords and remove the stopwords from the abstract and I have made a function for it, next I have done is as follows: I creat a function(get_top_n_words) for doing all in one | it first removes stopwords using Countvectorerizer | our function which takes n as input is the top n word frequencies we want | using vecotor transform and bag of words we get the frequencies and the words of the whole abstract column which we will input as the agrument corpus | we then sort it according to the most number of frequencies and return the top n words and their frequencies in a list of tuples | . . import nltk from nltk.corpus import stopwords from nltk.tokenize import word_tokenize stop_words=set(stopwords.words(&#39;english&#39;)) def removeSW(x): x=x.lower() word_tokens = word_tokenize(x) filtered_sentence = [w for w in word_tokens if not w in stop_words] return &quot; &quot;.join(filtered_sentence) df[&quot;Abstract&quot;]=df[&quot;Abstract&quot;].apply(removeSW) from sklearn.feature_extraction.text import CountVectorizer def get_top_n_words(corpus, n=None): vec = CountVectorizer(stop_words = &#39;english&#39;).fit(corpus) bag_of_words = vec.transform(corpus) sum_words = bag_of_words.sum(axis=0) words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()] words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True) return words_freq[:n] . Lets get our unigrams of the abstract column and the title column using this function . unigrams=get_top_n_words(df[&quot;Abstract&quot;],20) unigrams_title=get_top_n_words(df[&quot;Title of preprint&quot;],20) . unigrams . [(&#39;19&#39;, 9637), (&#39;covid&#39;, 9419), (&#39;sars&#39;, 5431), (&#39;cov&#39;, 5304), (&#39;patients&#39;, 4950), (&#39;cases&#39;, 3385), (&#39;data&#39;, 2727), (&#39;disease&#39;, 2597), (&#39;infection&#39;, 2334), (&#39;model&#39;, 2211), (&#39;coronavirus&#39;, 2189), (&#39;results&#39;, 2181), (&#39;number&#39;, 2127), (&#39;study&#39;, 1979), (&#39;2020&#39;, 1972), (&#39;2019&#39;, 1848), (&#39;health&#39;, 1808), (&#39;pandemic&#39;, 1797), (&#39;time&#39;, 1698), (&#39;epidemic&#39;, 1679)] . unigrams_title . [(&#39;19&#39;, 2135), (&#39;covid&#39;, 2128), (&#39;sars&#39;, 945), (&#39;cov&#39;, 898), (&#39;coronavirus&#39;, 491), (&#39;patients&#39;, 397), (&#39;2019&#39;, 359), (&#39;analysis&#39;, 345), (&#39;study&#39;, 272), (&#39;pandemic&#39;, 269), (&#39;infection&#39;, 267), (&#39;china&#39;, 264), (&#39;disease&#39;, 257), (&#39;novel&#39;, 256), (&#39;epidemic&#39;, 242), (&#39;clinical&#39;, 227), (&#39;model&#39;, 226), (&#39;based&#39;, 214), (&#39;outbreak&#39;, 209), (&#39;using&#39;, 198)] . Yes! I know both the list of tuples looks very much same since many of the title and abstract words are column, now lets get down in making a seperate dataframe and join this 2 list of tuples as rows and their values as column ,also a column for type indicates that wether it is abstract or title . d={&quot;word&quot;:[],&quot;count&quot;:[],&quot;type&quot;:[]} for i in unigrams: d[&quot;word&quot;].append(i[0]) d[&quot;count&quot;].append(i[1]) d[&quot;type&quot;].append(&quot;Abstract&quot;) for i in unigrams_title: d[&quot;word&quot;].append(i[0]) d[&quot;count&quot;].append(i[1]) d[&quot;type&quot;].append(&quot;Title&quot;) count_df=pd.DataFrame(d) . We plot!! . source = count_df alt.Chart(source).mark_bar().encode( tooltip=[&quot;word&quot;,&quot;count&quot;], column=&#39;type&#39;, x=&#39;word&#39;, y=&#39;count&#39;, color=&#39;type&#39; ) . Now we modify the function before and create a new function called get_top_gram It takes one more argument called grams, which will be the number of grams we want, bigram trigram etc and after that I will plot the bigram as similarly as the unigrams, and then I plot the pentagrams!!&lt;/p&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; def get_top_gram(corpus,grams, n=None): vec = CountVectorizer(ngram_range=grams,stop_words = &#39;english&#39;).fit(corpus) bag_of_words = vec.transform(corpus) sum_words = bag_of_words.sum(axis=0) words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()] words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True) return words_freq[:n] . bigrams=get_top_gram(df[&quot;Abstract&quot;],(2,2),20) bigrams_title=get_top_gram(df[&quot;Title of preprint&quot;],(2,2),20) d={&quot;word&quot;:[],&quot;count&quot;:[],&quot;type&quot;:[]} for i in bigrams: d[&quot;word&quot;].append(i[0]) d[&quot;count&quot;].append(i[1]) d[&quot;type&quot;].append(&quot;Abstract&quot;) for i in bigrams_title: d[&quot;word&quot;].append(i[0]) d[&quot;count&quot;].append(i[1]) d[&quot;type&quot;].append(&quot;Title&quot;) count_df=pd.DataFrame(d) source = count_df alt.Chart(source).mark_bar().encode( tooltip=[&quot;word&quot;,&quot;count&quot;], column=&#39;type&#39;, x=&#39;word&#39;, y=&#39;count&#39;, color=&#39;type&#39; ) . fivegrams=get_top_gram(df[&quot;Abstract&quot;],(5,5),20) fivegrams_title=get_top_gram(df[&quot;Title of preprint&quot;],(5,5),20) d={&quot;word&quot;:[],&quot;count&quot;:[],&quot;type&quot;:[]} for i in fivegrams: d[&quot;word&quot;].append(i[0]) d[&quot;count&quot;].append(i[1]) d[&quot;type&quot;].append(&quot;Abstract&quot;) for i in fivegrams_title: d[&quot;word&quot;].append(i[0]) d[&quot;count&quot;].append(i[1]) d[&quot;type&quot;].append(&quot;Title&quot;) count_df=pd.DataFrame(d) source = count_df alt.Chart(source).mark_bar().encode( tooltip=[&quot;word&quot;,&quot;count&quot;], column=&#39;type&#39;, x=&#39;word&#39;, y=&#39;count&#39;, color=&#39;type&#39; ) . Now lets get onto the authors and the Universities!! . Here I use json to extract the dictionary present in the column Author(s) Institutions . after that I show the length of the set and list created, basically the set contains the number of unique Institutions and the list contains everytime the Institution is mentioned in the dataframe, we will use it ahead . import json myset=set() mylist=[] for i in df[&quot;Author(s) Institutions&quot;].index: l=set(json.loads(df.loc[i,&quot;Author(s) Institutions&quot;]).keys()) for j in l: myset.add(j) mylist.append(j) . len(myset) . 9134 . len(mylist) . 12343 . Another function phew!!!, well this is quite simple we use the list and count how many times each intistute has appeared and then a dictionary example ({&quot;university&quot;:10}) . basically what we want to do is that the frequency represents how many times the institution has published a paper since its occurence in the dataframe in each row is a count for its publications!!! . def CountFrequency(my_list): count = {} for i in my_list: count[i] = count.get(i, 0) + 1 return count frequency=CountFrequency(mylist) d={&quot;UNI&quot;:[],&quot;Publishes&quot;:[]} for i in frequency.keys(): d[&quot;UNI&quot;].append(i) d[&quot;Publishes&quot;].append(frequency[i]) uni_df=pd.DataFrame(d) uni_df=uni_df.sort_values(by=[&quot;Publishes&quot;],ascending=False) for i in uni_df.index: if len(uni_df.loc[i].UNI)&lt;4: uni_df.drop(i,axis=0,inplace=True) . source = uni_df.iloc[:50,:] alt.Chart(source).mark_bar().encode( x=&#39;Publishes&#39;, y=&quot;UNI&quot;, tooltip=[&quot;Publishes&quot;] ).properties(height=700) . Voila! the graph above shows how many publications done by the institutions(here I plotted only the top 50 )HOVER!!! and we find out Oxford University has published 61 times!!! . Now, lets check for the authors , below I used the previous dictionary of frequencies we outputed and change the values to list of values example {&quot;uni&quot;:1}===&gt;{&quot;uni&quot;:[1,0]} . here we add the 0 to all as we are going to store the authors count for this value!!! . . for i in frequency.keys(): frequency[i]=[frequency[i],0] . See the name of the institue is given and also the value with it are the authors number! which we did not consider above , well now we are!! . df[&quot;Author(s) Institutions&quot;].head() . 0 {&#34;Research and Practical Clinical Center for D... 1 {&#34;Michigan State University&#34;: 9, &#34;University o... 2 {&#34;University of Patras&#34;: 2, &#34;Department of Phy... 3 {&#34;Northwell Health Laboratories&#34;: 5, &#34;Northwel... 4 {&#34;Shanghai Jiao Tong University&#34;: 11, &#34;Fudan U... Name: Author(s) Institutions, dtype: object . We now again use json to get the values of author institution column but we will extract the names of the Uni as a key for our dictionary and will upadte the auhors value to it . mylist=[] for i in df[&quot;Author(s) Institutions&quot;].index: l=set(json.loads(df.loc[i,&quot;Author(s) Institutions&quot;]).keys()) for j in l: frequency[j][1]=frequency[j][1]+json.loads(df.loc[i,&quot;Author(s) Institutions&quot;])[j] . d={&quot;UNI&quot;:[],&quot;AuthorCount&quot;:[]} for i in frequency.keys(): d[&quot;UNI&quot;].append(i) d[&quot;AuthorCount&quot;].append(frequency[i][1]) uni_df=pd.DataFrame(d) uni_df=uni_df.sort_values(by=[&quot;AuthorCount&quot;],ascending=False) for i in uni_df.index: if len(uni_df.loc[i].UNI)&lt;4: uni_df.drop(i,axis=0,inplace=True) . We did the same procdeure and created a new dataframe for university and their authors count! . uni_df . UNI AuthorCount . 1402 Icahn School of Medicine at Mount Sinai | 236 | . 450 University of Oxford | 183 | . 2716 Centers for Disease Control and Prevention | 180 | . 784 Imperial College London | 172 | . 485 Stanford University | 160 | . ... ... | ... | . 5354 National Institute of Advanced Industrial Scie... | 1 | . 5353 National Institute for Infectious Diseases | 1 | . 5352 Nagahama Institute of Bioscience and Technology | 1 | . 2214 6School of Medicine, Shiraz University of Medi... | 1 | . 9133 Loyola University of Chicago Stritch School of... | 1 | . 9107 rows × 2 columns . Finally we plot it!!, and hovering over the data we can see Icahn School of Medicine has the highest amount of authors who have published for covid-19 around 470 . source = uni_df.iloc[:50,:] alt.Chart(source).mark_bar().encode( x=&#39;AuthorCount&#39;, y=&quot;UNI&quot;, tooltip=[&quot;AuthorCount&quot;,&quot;UNI&quot;] ).properties(height=700) . Thank You! . &lt;/div&gt; .",
            "url": "https://omegaji.github.io/DataScienceBlog/2020/05/23/copy_preprint.html",
            "relUrl": "/2020/05/23/copy_preprint.html",
            "date": " • May 23, 2020"
        }
        
    
  
    
        ,"post1": {
            "title": "COVID-19",
            "content": "Hello welcome to my notebook on Covid DataAnalysis&lt;/p&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; import pandas as pd import numpy as np train_df=pd.read_csv(&quot;train.csv&quot;) train_df=train_df.drop(&quot;Province_State&quot;,axis=1) from datetime import datetime train_df[&quot;Date&quot;]=train_df[&#39;Date&#39;].apply(lambda x:datetime.strptime(x, &#39;%Y-%m-%d&#39;)) train_df[&quot;week&quot;]=&quot;week_&quot;+ str(train_df[&quot;Date&quot;].dt.week) train_df[&quot;week&quot;]=train_df[&quot;Date&quot;].dt.week.apply(lambda x: x) train_df[&quot;day&quot;]=train_df[&quot;Date&quot;].dt.day.apply(lambda x: x) . Above we used the data column to generate days and week number for out dataset starting from week 1 to week 17 . &lt;/p&gt; now below i will print the dataframe . &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; train_df.head() . Id Country_Region Date ConfirmedCases Fatalities week day . 0 1 | Afghanistan | 2020-01-22 | 0.0 | 0.0 | 4 | 22 | . 1 2 | Afghanistan | 2020-01-23 | 0.0 | 0.0 | 4 | 23 | . 2 3 | Afghanistan | 2020-01-24 | 0.0 | 0.0 | 4 | 24 | . 3 4 | Afghanistan | 2020-01-25 | 0.0 | 0.0 | 4 | 25 | . 4 5 | Afghanistan | 2020-01-26 | 0.0 | 0.0 | 4 | 26 | . from sklearn.preprocessing import LabelEncoder,StandardScaler,MinMaxScaler from sklearn.model_selection import train_test_split . now below we do the necessary processing of our data to make it more viable for our Scaling and model preparation . train_df[&quot;month&quot;]=train_df[&quot;Date&quot;].dt.month.apply(lambda x: int(x)) . train_df[&#39;ConfirmedCases&#39;] = train_df[&#39;ConfirmedCases&#39;].apply(int) train_df[&#39;Fatalities&#39;] = train_df[&#39;Fatalities&#39;].apply(int) . now we will take out the data related to our country India in a seperate dataframe . india_df=train_df[train_df[&quot;Country_Region&quot;]==&quot;India&quot;] india_df.head() . Id Country_Region Date ConfirmedCases Fatalities week day month . 13160 15961 | India | 2020-01-22 | 0 | 0 | 4 | 22 | 1 | . 13161 15962 | India | 2020-01-23 | 0 | 0 | 4 | 23 | 1 | . 13162 15963 | India | 2020-01-24 | 0 | 0 | 4 | 24 | 1 | . 13163 15964 | India | 2020-01-25 | 0 | 0 | 4 | 25 | 1 | . 13164 15965 | India | 2020-01-26 | 0 | 0 | 4 | 26 | 1 | . import matplotlib.pyplot as plt #print(plt.style.available) plt.style.use(&#39;seaborn-darkgrid&#39;) . india_df.groupby([&quot;week&quot;]).max().reset_index().plot(kind=&quot;bar&quot;,x=&quot;week&quot;,y=&quot;ConfirmedCases&quot;) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7f43c0b401d0&gt; . india_df.groupby([&quot;week&quot;]).max().reset_index().plot(kind=&quot;bar&quot;,x=&quot;week&quot;,y=&quot;Fatalities&quot;) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7f43c1e07e90&gt; . The proceeses done are seperate the confirmed cases to a seperate variable called case(Y variable) | we use the LabelEncoder() instead of one hot encoding to encode the country regions | we then use the fit_transform function to label encode | We then use MinMaxScaler for normalizing the values as there are various values from 0 to more than 1000 | &lt;/p&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; cases = train_df.ConfirmedCases fatal=train_df.Fatalities lb = LabelEncoder() del train_df[&quot;Date&quot;] del train_df[&quot;Fatalities&quot;] del train_df[&quot;ConfirmedCases&quot;] #del train_df[&quot;Id&quot;] train_df[&#39;Country_Region&#39;] = lb.fit_transform(train_df[&#39;Country_Region&#39;]) scaler = MinMaxScaler() x_train = scaler.fit_transform(train_df.values) X_train, X_test, y_train, y_test = train_test_split(train_df, cases, test_size=0.2, random_state=0) X_train = scaler.fit_transform(X_train.values) X_test = scaler.fit_transform(X_test.values) . Here We can see we Check on XGBoost Regressor with below hyperparameters . from xgboost import XGBRegressor model = XGBRegressor(n_estimators = 2500 , random_state = 0 , max_depth = 27) model.fit(X_train,y_train) . XGBRegressor(base_score=0.5, booster=None, colsample_bylevel=1, colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1, importance_type=&#39;gain&#39;, interaction_constraints=None, learning_rate=0.300000012, max_delta_step=0, max_depth=27, min_child_weight=1, missing=nan, monotone_constraints=None, n_estimators=2500, n_jobs=0, num_parallel_tree=1, objective=&#39;reg:squarederror&#39;, random_state=0, reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1, tree_method=None, validate_parameters=False, verbosity=None) . model.score(X_train,y_train) . 0.99999999999996 . Here we plot the number of cases in india month wise(1,1.5..4(april))&lt;/p&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; import seaborn as sns ig=india_df.groupby([&quot;month&quot;]).max().reset_index() sns.lineplot(ig[&quot;month&quot;],ig[&quot;ConfirmedCases&quot;],ci=None) ig[&quot;month&quot;] . 0 1 1 2 2 3 3 4 Name: month, dtype: int64 . below we predict the values using our X_test,y_test we created . Next we also plot the predicted values and acutal values of the model in a scatter plot and as we can see the points form more of a X=Y line(meaning that the predicted and actual values are almost equal) . print(&quot;the score for the test dataaset is &quot;+str(model.score(X_test,y_test))) predicted=model.predict(X_test) import seaborn as sns import matplotlib.pyplot as plt #sns.lineplot(y_train) plt.xlabel(&quot;Predicted Value&quot;) plt.ylabel(&quot;Actual Value&quot;) plt.title(&quot;the graph is similar to x=y curve&quot;) plt.scatter(predicted,y_test) . the score for the test dataaset is 0.991170270240357 . &lt;matplotlib.collections.PathCollection at 0x7f43c1eee590&gt; . below we predict the values using our X_test,y_test we created Next we also plot the predicted values and acutal values of the model in a scatter plot and as we can see the points form more of a X=Y line(meaning that the predicted and actual values are almost equal) . train_predict=model.predict(X_train) plt.xlabel(&quot;Predicted Value(training)&quot;) plt.ylabel(&quot;Actual Value(traiing)&quot;) plt.title(&quot;the graph is similar to x=y curve&quot;) plt.scatter(train_predict,y_train) . &lt;matplotlib.collections.PathCollection at 0x7f43c1f4e490&gt; . from sklearn.metrics import explained_variance_score y_pred=model.predict(X_test) . Now we Calculate 3 types of errors, explained variance score, max error score,mean squared error&lt;/p&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; from sklearn.metrics import explained_variance_score from sklearn.metrics import max_error from sklearn.metrics import mean_squared_error print(&quot;The expalined variance score is==&gt;&quot;+ str(explained_variance_score(y_test,y_pred))) print(&quot;The max error score is ==&gt;&quot;+str(max_error(y_test,y_pred))) print(&quot;The mean squared error is ==&gt;&quot;+str(mean_squared_error(y_test,y_pred))) . The expalined variance score is==&gt;0.9912504663704602 The max error score is ==&gt;24370.158203125 The mean squared error is ==&gt;1096115.2159031187 . pred_df=pd.DataFrame() pred_df[&quot;Predictions&quot;]=pd.Series(np.around(y_pred,decimals=0)) pred_df[&quot;True_values&quot;]=pd.Series(y_test.values) . Here we form a dataframe consisting predicted data and true data&lt;/p&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; pred_df . Predictions True_values . 0 0.0 | 0 | . 1 417.0 | 447 | . 2 2.0 | 0 | . 3 319.0 | 318 | . 4 0.0 | 0 | . ... ... | ... | . 5880 -0.0 | 0 | . 5881 245.0 | 245 | . 5882 -0.0 | 0 | . 5883 0.0 | 0 | . 5884 -0.0 | 0 | . 5885 rows × 2 columns . here we use the STACKING ALGORITHIM we choose 3 algorithims XGBoost(the one above we used) | RandomForest regressor | SVM Regressor | &lt;/p&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; from sklearn.ensemble import RandomForestRegressor from sklearn import svm rf = RandomForestRegressor(n_estimators = 500, random_state = 42) models = [ svm.SVR(), RandomForestRegressor(random_state=0, n_jobs=-1, n_estimators=500, max_depth=3), model ] . !pip install vecstack . Requirement already satisfied: vecstack in /home/omegaji/anaconda3/lib/python3.7/site-packages (0.4.0) Requirement already satisfied: numpy in /home/omegaji/anaconda3/lib/python3.7/site-packages (from vecstack) (1.18.1) Requirement already satisfied: scipy in /home/omegaji/anaconda3/lib/python3.7/site-packages (from vecstack) (1.4.1) Requirement already satisfied: scikit-learn&gt;=0.18 in /home/omegaji/anaconda3/lib/python3.7/site-packages (from vecstack) (0.22.1) Requirement already satisfied: joblib&gt;=0.11 in /home/omegaji/anaconda3/lib/python3.7/site-packages (from scikit-learn&gt;=0.18-&gt;vecstack) (0.14.1) . from vecstack import stacking from sklearn.metrics import r2_score S_train, S_test = stacking(models, X_train, y_train, X_test, regression=True, mode=&#39;oof_pred_bag&#39;, needs_proba=False, save_dir=None, metric=r2_score, n_folds=5, #stratified=True, shuffle=True, random_state=0, verbose=2) . task: [regression] metric: [r2_score] mode: [oof_pred_bag] n_models: [3] model 0: [SVR] fold 0: [-0.01849384] fold 1: [-0.02135447] fold 2: [-0.01936172] fold 3: [-0.01934595] fold 4: [-0.01877062] - MEAN: [-0.01946532] + [0.00100202] FULL: [-0.01898844] model 1: [RandomForestRegressor] fold 0: [0.16408069] fold 1: [0.23211451] fold 2: [0.11892497] fold 3: [0.13180512] fold 4: [0.14895139] - MEAN: [0.15917534] + [0.03954222] FULL: [0.15628386] model 2: [XGBRegressor] fold 0: [0.98925196] fold 1: [0.99284200] fold 2: [0.99307733] fold 3: [0.99309471] fold 4: [0.99233834] - MEAN: [0.99212087] + [0.00146021] FULL: [0.99187249] . final_model = model.fit(S_train, y_train) y_pred = final_model.predict(S_test) . the training score For the Stacked Model is predicted below . final_model.score(S_train,y_train) . 0.9999999999999664 . The testing score is predicted below . final_model.score(S_test,y_test) . 0.9869640680032792 . Similary we plot the predicted data vs actual confirmed cases for both training and testing accuracy as shown bellow . . we can see the plot still resembles and x=y curve hence a good prediction value close to the actual value . plt.xlabel(&quot;Predicted Value(training)&quot;) train_pred=model.predict(S_train) plt.ylabel(&quot;Actual Value(training)&quot;) plt.title(&quot;the graph is similar to x=y curve&quot;) plt.scatter(train_pred,y_train) . &lt;matplotlib.collections.PathCollection at 0x7f43c1f4ead0&gt; . plt.xlabel(&quot;Predicted Value(testing)&quot;) train_pred=model.predict(S_train) plt.ylabel(&quot;Actual Value(testing)&quot;) plt.title(&quot;the graph is similar to x=y curve&quot;) plt.scatter(y_pred,y_test) . &lt;matplotlib.collections.PathCollection at 0x7f43c2a34550&gt; . Now we test each mode individually on how it scores and out best model is the Xgboost which we tested earlier and also the curve was very similar to x=y We start with using SVM Regressor which is the 0th model in our MODELS list (hence below code contains model[0]) . . print(&quot;USING SVM REGRESSOR MODEL &quot;) models[0].fit(S_train,y_train) print(&quot;the training score is&quot;) print(models[0].score(S_train,y_train)) print(&quot;the testing score is&quot;) print(models[0].score(S_test,y_test)) . USING SVM REGRESSOR MODEL the training score is -0.0017980567557247529 the testing score is 0.001936770722922643 . we can see the training accuracy and testing accuracy are not very efficient and cannot even be consider a good model it is a bad model . following code plots the models training predictions to actual values and testing predictions to actual test values . train_pred=models[0].predict(S_train) plt.xlabel(&quot;Predicted Value(training)&quot;) plt.ylabel(&quot;Actual Value(training)&quot;) plt.title(&quot;the graph is NOT an x=y curve at any level &quot;) plt.scatter(train_pred,y_train) . &lt;matplotlib.collections.PathCollection at 0x7f43c287a410&gt; . plt.xlabel(&quot;Predicted Value(testing)&quot;) train_pred=models[0].predict(S_test) plt.ylabel(&quot;Actual Value(testing)&quot;) plt.title(&quot;the graph is also NOT X=y&quot;) plt.scatter(train_pred,y_test) . &lt;matplotlib.collections.PathCollection at 0x7f43c33a2c50&gt; . NOW WE DO THE SAME FOR OUR RANDOM FOREST MODEL WHICH SHOWS A GOOD SCORE TOO&lt;/p&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; print(&quot;USING RANDOM FOREST MODEL &quot;) models[1].fit(S_train,y_train) print(&quot;the training score is&quot;) print(models[1].score(S_train,y_train)) print(&quot;the testing score is&quot;) print(models[1].score(S_test,y_test)) print(models[1]) . USING RANDOM FOREST MODEL the training score is 0.9850564601052662 the testing score is 0.9839541082692184 RandomForestRegressor(bootstrap=True, ccp_alpha=0.0, criterion=&#39;mse&#39;, max_depth=3, max_features=&#39;auto&#39;, max_leaf_nodes=None, max_samples=None, min_impurity_decrease=0.0, min_impurity_split=None, min_samples_leaf=1, min_samples_split=2, min_weight_fraction_leaf=0.0, n_estimators=500, n_jobs=-1, oob_score=False, random_state=0, verbose=0, warm_start=False) . train_pred=models[1].predict(S_train) plt.xlabel(&quot;Predicted Value(training)&quot;) plt.ylabel(&quot;Actual Value(training)&quot;) plt.title(&quot;the graph is NOT an x=y curve at any level&quot; ) plt.scatter(train_pred,y_train) . &lt;matplotlib.collections.PathCollection at 0x7f43e08e5150&gt; . plt.xlabel(&quot;Predicted Value(testing)&quot;) train_pred=models[1].predict(S_test) plt.ylabel(&quot;Actual Value(testing)&quot;) plt.title(&quot;the graph is also NOT X=y&quot;) plt.scatter(train_pred,y_test) . &lt;matplotlib.collections.PathCollection at 0x7f43c1dfee10&gt; . THANK You!!&lt;/p&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; # from sklearn import neighbors # knn = neighbors.KNeighborsRegressor(1000, weights=&quot;uniform&quot;) # from sklearn.linear_model import ElasticNet # en=ElasticNet(random_state=0) # models = [ # knn, # RandomForestRegressor(random_state=0, n_jobs=-1, # n_estimators=1000, max_depth=3), # model, # en # ] # S_train, S_test = stacking(models, # X_train, y_train[&quot;ConfirmedCases&quot;], X_test, # regression=True, # mode=&#39;oof_pred_bag&#39;, # needs_proba=False, # save_dir=None, # metric=r2_score, # shuffle=True, # random_state=0, # verbose=2) # final_model_2 = model.fit(S_train, y_train[&quot;ConfirmedCases&quot;]) # y_pred = final_model_2.predict(S_test) # final_model.score(S_train,y_train[&quot;ConfirmedCases&quot;]) . &lt;/div&gt; . . . .",
            "url": "https://omegaji.github.io/DataScienceBlog/2020/05/22/try_train.html",
            "relUrl": "/2020/05/22/try_train.html",
            "date": " • May 22, 2020"
        }
        
    
  
    
        ,"post2": {
            "title": "google_mobility",
            "content": "import pandas as pd google_df=pd.read_csv(&quot;google_mobility.csv&quot;) . Here I try and find out the country regions how many are there . len(google_df.country_region.unique()) . 132 . google_df[&quot;retail_and_recreation_percent_change_from_baseline&quot;] . 0 0.0 1 1.0 2 -1.0 3 -2.0 4 -2.0 ... 284484 -57.0 284485 -56.0 284486 -57.0 284487 -58.0 284488 -53.0 Name: retail_and_recreation_percent_change_from_baseline, Length: 284489, dtype: float64 . from this link here we know that the baseline from which the columns retail,workplaces....The baseline is the median value, for the corresponding day of the week, during the 5-week period Jan 3–Feb 6, 2020.&lt;/p&gt; Let us assume the baseline to be 100 so -1 is basically 99 +1 is 101 ... and so on we change the columns . &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; google_df[&quot;parks_percent_change_from_baseline&quot;] . 0 5.0 1 4.0 2 5.0 3 5.0 4 4.0 ... 284484 -42.0 284485 -42.0 284486 -46.0 284487 -41.0 284488 -35.0 Name: parks_percent_change_from_baseline, Length: 284489, dtype: float64 . def changeFromPercent(x): if x!=0: return (1+(x/100))*100 else: return 100 . google_df[&quot;retail_and_recreation_percent_change_from_baseline&quot;]=google_df[&quot;retail_and_recreation_percent_change_from_baseline&quot;].apply(changeFromPercent) google_df[&quot;grocery_and_pharmacy_percent_change_from_baseline&quot;]=google_df[&quot;grocery_and_pharmacy_percent_change_from_baseline&quot;].apply(changeFromPercent) google_df[&quot;parks_percent_change_from_baseline&quot;]=google_df[&quot;parks_percent_change_from_baseline&quot;].apply(changeFromPercent) google_df[&quot;transit_stations_percent_change_from_baseline&quot;]=google_df[&quot;transit_stations_percent_change_from_baseline&quot;].apply(changeFromPercent) google_df[&quot;workplaces_percent_change_from_baseline&quot;]=google_df[&quot;workplaces_percent_change_from_baseline&quot;].apply(changeFromPercent) google_df[&#39;residential_percent_change_from_baseline&#39;]=google_df[&#39;residential_percent_change_from_baseline&#39;].apply(changeFromPercent) . google_df . country_region_code lat lng wikidata short_code country_region sub_region_1 sub_region_2 date retail_and_recreation_percent_change_from_baseline grocery_and_pharmacy_percent_change_from_baseline parks_percent_change_from_baseline transit_stations_percent_change_from_baseline workplaces_percent_change_from_baseline residential_percent_change_from_baseline . 0 AE | 23.78333 | 54.00000 | Q878 | AE | United Arab Emirates | NaN | NaN | 2020-02-15 | 100.0 | 104.0 | 105.0 | 100.0 | 102.0 | 101.0 | . 1 AE | 23.78333 | 54.00000 | Q878 | AE | United Arab Emirates | NaN | NaN | 2020-02-16 | 101.0 | 104.0 | 104.0 | 101.0 | 102.0 | 101.0 | . 2 AE | 23.78333 | 54.00000 | Q878 | AE | United Arab Emirates | NaN | NaN | 2020-02-17 | 99.0 | 101.0 | 105.0 | 101.0 | 102.0 | 101.0 | . 3 AE | 23.78333 | 54.00000 | Q878 | AE | United Arab Emirates | NaN | NaN | 2020-02-18 | 98.0 | 101.0 | 105.0 | 100.0 | 102.0 | 101.0 | . 4 AE | 23.78333 | 54.00000 | Q878 | AE | United Arab Emirates | NaN | NaN | 2020-02-19 | 98.0 | 100.0 | 104.0 | 99.0 | 102.0 | 101.0 | . ... ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | . 284484 ZW | -19.01667 | 30.01667 | Q954 | ZW | Zimbabwe | NaN | NaN | 2020-04-22 | 43.0 | 57.0 | 58.0 | 29.0 | 42.0 | 137.0 | . 284485 ZW | -19.01667 | 30.01667 | Q954 | ZW | Zimbabwe | NaN | NaN | 2020-04-23 | 44.0 | 56.0 | 58.0 | 27.0 | 42.0 | 138.0 | . 284486 ZW | -19.01667 | 30.01667 | Q954 | ZW | Zimbabwe | NaN | NaN | 2020-04-24 | 43.0 | 57.0 | 54.0 | 26.0 | 44.0 | 139.0 | . 284487 ZW | -19.01667 | 30.01667 | Q954 | ZW | Zimbabwe | NaN | NaN | 2020-04-25 | 42.0 | 58.0 | 59.0 | 27.0 | 57.0 | 130.0 | . 284488 ZW | -19.01667 | 30.01667 | Q954 | ZW | Zimbabwe | NaN | NaN | 2020-04-26 | 47.0 | 58.0 | 65.0 | 30.0 | 77.0 | 128.0 | . 284489 rows × 15 columns . Just deleting some unwanted columns&lt;/p&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; google_df.drop([&quot;wikidata&quot;,&quot;sub_region_1&quot;,&quot;sub_region_2&quot;],axis=1,inplace=True) . Now using datetime from python we seperate the datetime given into days months and year column for&lt;/p&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; import datetime string = &quot;2020-02-15&quot; date = datetime.datetime.strptime(string, &quot;%Y-%m-%d&quot;) print(date.timetuple().tm_yday) print(date.day) . 46 15 . knowing that from datetime I can easily get day,month,year I also extract the day of the year, so I can use it easily in plotting as a variable&lt;/p&gt; so lets get to it!&lt;/p&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; def stripthedate(x): return datetime.datetime.strptime(x, &quot;%Y-%m-%d&quot;) google_df[&quot;day&quot;]=google_df[&quot;date&quot;].apply(stripthedate) google_df[&quot;month&quot;]=google_df[&quot;date&quot;].apply(stripthedate) google_df[&quot;year&quot;]=google_df[&quot;date&quot;].apply(stripthedate) google_df[&quot;day_in_year&quot;]=google_df[&quot;date&quot;].apply(stripthedate) google_df[&quot;day&quot;]=google_df[&quot;day&quot;].apply(lambda x: int(x.day)) google_df[&quot;month&quot;]=google_df[&quot;month&quot;].apply(lambda x: int(x.month)) google_df[&quot;year&quot;]=google_df[&quot;year&quot;].apply(lambda x: int(x.year)) google_df[&quot;day_in_year&quot;]=google_df[&quot;day_in_year&quot;].apply(lambda x: int(x.timetuple().tm_yday)) . retail=&#39;retail_and_recreation_percent_change_from_baseline&#39; grocery=&#39;grocery_and_pharmacy_percent_change_from_baseline&#39; parks= &#39;parks_percent_change_from_baseline&#39; transit= &#39;transit_stations_percent_change_from_baseline&#39; workplace= &#39;workplaces_percent_change_from_baseline&#39; residential= &#39;residential_percent_change_from_baseline&#39; . india_df=google_df[google_df[&quot;country_region&quot;]==&quot;India&quot;] india_df[&quot;day&quot;] . 46396 15 46397 16 46398 17 46399 18 46400 19 .. 49055 22 49056 23 49057 24 49058 25 49059 26 Name: day, Length: 2664, dtype: int64 . #alt.data_transformers.disable_max_rows() import altair as alt # alt.renderers.enable(&#39;notebook&#39;) # RendererRegistry.enable(&#39;notebook&#39;) a=alt.Chart(india_df).transform_fold([grocery,parks,residential,workplace,transit],as_=[&quot;pro&quot;,&quot;values&quot;]).mark_area().encode( alt.X(&#39;day_in_year:Q&#39;), alt.Y(&#39;values:Q&#39;), alt.Color(&#39;pro:N&#39;), alt.Row(&quot;pro:N&quot;), ).properties( ).interactive() a . alt.Chart(india_df.groupby([&quot;month&quot;]).sum().reset_index()).transform_fold([grocery,parks,residential,workplace,transit],as_=[&quot;pro&quot;,&quot;values&quot;]).mark_bar().encode( alt.X(&#39;pro:O&#39;,axis=alt.Axis(labels=False),title=None), alt.Y(&#39;values:Q&#39;,axis=alt.Axis(labels=False)), alt.Color(&#39;pro:N&#39;,title=None), alt.Column(&#39;month:N&#39;) ).interactive() . apple_df=pd.read_csv(&quot;apple_mobility.csv&quot;) . About the Data The CSV file and charts on this site show a relative volume of directions requests per country/region, sub-region or city compared to a baseline volume on 13 January 2020. We define our day as midnight-to-midnight, US Pacific time. Cities are defined as the greater metropolitan area and their geographic boundaries remain constant across the data set. In many countries/regions, sub-regions and cities, relative volume has increased since 13 January, consistent with normal, seasonal usage of Apple Maps. Day of week effects are important to normalise as you use this data. Data that is sent from users’ devices to the Maps service is associated with random, rotating identifiers so Apple does not have a profile of individual movements and searches. Apple Maps has no demographic information about users, so we cannot make any statements about the representativeness of our usage against the overall population.&lt;/p&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; apple_df[&quot;2020-04-23&quot;] . 0 33.94 1 34.80 2 28.73 3 20.39 4 62.87 ... 1153 20.81 1154 107.07 1155 106.38 1156 108.22 1157 73.95 Name: 2020-04-23, Length: 1158, dtype: float64 . &lt;/div&gt; . .",
            "url": "https://omegaji.github.io/DataScienceBlog/2020/05/22/google_apple.html",
            "relUrl": "/2020/05/22/google_apple.html",
            "date": " • May 22, 2020"
        }
        
    
  
    
        ,"post3": {
            "title": "Title",
            "content": "#&quot;Covid-19 India analysis and prediction&quot; &gt; &quot;Covid-19 India analysis and prediction&quot; - toc: false - branch: master - badges: true - comments: true - categories: [fastpages, jupyter] - image: images/some_folder/your_image.png - hide: false - search_exclude: true - metadata_key1: metadata_value1 - metadata_key2: metadata_value2 from IPython.display import HTML HTML(fig.to_html()) . Hello Welcome to my kernel this is my first Proper kernel with some EDA and choropleth maps DO UPVOTE IF YOU LIKE IT :D let&#39;s dive into what I have done below i have simply loaded the kaggle provided datasets . # This Python 3 environment comes with many helpful analytics libraries installed # It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python # For example, here&#39;s several helpful packages to load in import numpy as np # linear algebra import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv) # Input data files are available in the &quot;../input/&quot; directory. # For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory import os for dirname, _, filenames in os.walk(&#39;/kaggle/input&#39;): for filename in filenames: print(os.path.join(dirname, filename)) # Any results you write to the current directory are saved as output. . /kaggle/input/covid19-global-forecasting-week-4/test.csv /kaggle/input/covid19-global-forecasting-week-4/train.csv /kaggle/input/covid19-global-forecasting-week-4/submission.csv /kaggle/input/covid19-in-india/StatewiseTestingDetails.csv /kaggle/input/covid19-in-india/IndividualDetails.csv /kaggle/input/covid19-in-india/covid_19_india.csv /kaggle/input/covid19-in-india/HospitalBedsIndia.csv /kaggle/input/covid19-in-india/ICMRTestingDetails.csv /kaggle/input/covid19-in-india/population_india_census2011.csv /kaggle/input/covid19-in-india/AgeGroupDetails.csv /kaggle/input/covid19-in-india/ICMRTestingLabs.csv /kaggle/input/india-shape/ind_shape/IND_adm1.prj /kaggle/input/india-shape/ind_shape/IND_adm2.shx /kaggle/input/india-shape/ind_shape/IND_adm3.shx /kaggle/input/india-shape/ind_shape/IND_adm0.dbf /kaggle/input/india-shape/ind_shape/IND_adm3.csv /kaggle/input/india-shape/ind_shape/IND_adm0.csv /kaggle/input/india-shape/ind_shape/IND_adm1.csv /kaggle/input/india-shape/ind_shape/IND_adm0.shp /kaggle/input/india-shape/ind_shape/IND_adm3.prj /kaggle/input/india-shape/ind_shape/IND_adm1.dbf /kaggle/input/india-shape/ind_shape/IND_adm2.shp /kaggle/input/india-shape/ind_shape/IND_adm3.shp /kaggle/input/india-shape/ind_shape/IND_adm0.shx /kaggle/input/india-shape/ind_shape/IND_adm1.cpg /kaggle/input/india-shape/ind_shape/IND_adm2.cpg /kaggle/input/india-shape/ind_shape/IND_adm2.prj /kaggle/input/india-shape/ind_shape/IND_adm0.cpg /kaggle/input/india-shape/ind_shape/license.txt /kaggle/input/india-shape/ind_shape/IND_adm2.csv /kaggle/input/india-shape/ind_shape/IND_adm2.dbf /kaggle/input/india-shape/ind_shape/IND_adm0.prj /kaggle/input/india-shape/ind_shape/IND_adm3.dbf /kaggle/input/india-shape/ind_shape/IND_adm1.shx /kaggle/input/india-shape/ind_shape/IND_adm1.shp /kaggle/input/india-shape/ind_shape/IND_adm3.cpg /kaggle/input/countryshape/ne_110m_admin_0_countries.shp /kaggle/input/countryshape/ne_110m_admin_0_countries.README.html /kaggle/input/countryshape/ne_110m_admin_0_countries.VERSION.txt /kaggle/input/countryshape/ne_110m_admin_0_countries.dbf /kaggle/input/countryshape/ne_110m_admin_0_countries.prj /kaggle/input/countryshape/ne_110m_admin_0_countries.shx /kaggle/input/countryshape/ne_110m_admin_0_countries.cpg . train_df=pd.read_csv(&quot;/kaggle/input/covid19-global-forecasting-week-4/train.csv&quot;) . Below I have taken the india part out of the dataframe provided and did some plotting . train_df[train_df[&quot;Country_Region&quot;]==&quot;India&quot;] . Id Province_State Country_Region Date ConfirmedCases Fatalities . 13440 15961 | NaN | India | 2020-01-22 | 0.0 | 0.0 | . 13441 15962 | NaN | India | 2020-01-23 | 0.0 | 0.0 | . 13442 15963 | NaN | India | 2020-01-24 | 0.0 | 0.0 | . 13443 15964 | NaN | India | 2020-01-25 | 0.0 | 0.0 | . 13444 15965 | NaN | India | 2020-01-26 | 0.0 | 0.0 | . ... ... | ... | ... | ... | ... | ... | . 13531 16052 | NaN | India | 2020-04-22 | 21370.0 | 681.0 | . 13532 16053 | NaN | India | 2020-04-23 | 23077.0 | 721.0 | . 13533 16054 | NaN | India | 2020-04-24 | 24530.0 | 780.0 | . 13534 16055 | NaN | India | 2020-04-25 | 26283.0 | 825.0 | . 13535 16056 | NaN | India | 2020-04-26 | 27890.0 | 881.0 | . 96 rows × 6 columns . india_df=train_df[train_df[&quot;Country_Region&quot;]==&quot;India&quot;] #india_df[&quot;day&quot;]=india_df[&quot;Date&quot;].apply(lambda x:int(x[-2:]) ) #india_df[&quot;Month&quot;]=india_df[&quot;Date&quot;].apply(months ) india_df[&quot;ConfirmedCases&quot;]=india_df[&quot;ConfirmedCases&quot;].apply(lambda x: int(x)) . /opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:5: SettingWithCopyWarning: A value is trying to be set on a copy of a slice from a DataFrame. Try using .loc[row_indexer,col_indexer] = value instead See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy &#34;&#34;&#34; . NOW IN THIS WHOLE NOTEBOOK I HAVE SPLIT THE DATE INTO WEEKS.....IF YOU SEE WEEK 4 IT MEANS IT IS THE 4th WEEK OF THE YEAR!!!!! NOT MONTH . from datetime import datetime india_df[&quot;Date&quot;]=india_df[&#39;Date&#39;].apply(lambda x:datetime.strptime(x, &#39;%Y-%m-%d&#39;)) . /opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:2: SettingWithCopyWarning: A value is trying to be set on a copy of a slice from a DataFrame. Try using .loc[row_indexer,col_indexer] = value instead See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy . . Below I have made a week column and added it to the dataframe and did some simple plottings I have done using SEABORN CATPLOT . india_df[&quot;week&quot;]=&quot;week_&quot;+ str(india_df[&quot;Date&quot;].dt.week) . /opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:1: SettingWithCopyWarning: A value is trying to be set on a copy of a slice from a DataFrame. Try using .loc[row_indexer,col_indexer] = value instead See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy &#34;&#34;&#34;Entry point for launching an IPython kernel. . india_df[&quot;week&quot;]=india_df[&quot;Date&quot;].dt.week.apply(lambda x: x) . /opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:1: SettingWithCopyWarning: A value is trying to be set on a copy of a slice from a DataFrame. Try using .loc[row_indexer,col_indexer] = value instead See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy &#34;&#34;&#34;Entry point for launching an IPython kernel. . import matplotlib.pyplot as plt import seaborn as sns #fig.set_size_inches(12, 18) sns.catplot(data=india_df.groupby([&quot;week&quot;]).max().reset_index(),x=&quot;week&quot;,y=&quot;ConfirmedCases&quot;,kind=&quot;bar&quot;) . &lt;seaborn.axisgrid.FacetGrid at 0x7f1bad636f98&gt; . train_df[&quot;day&quot;]=train_df[&quot;Date&quot;].apply(lambda x:int(x[-2:]) ) #train_df[&quot;Month&quot;]=train_df[&quot;Date&quot;].apply(months ) train_df[&quot;ConfirmedCases&quot;]=train_df[&quot;ConfirmedCases&quot;].apply(lambda x: int(x)) . sns.catplot(data=india_df.groupby([&quot;week&quot;]).max().reset_index(),x=&quot;week&quot;,y=&quot;Fatalities&quot;,kind=&quot;bar&quot;) . &lt;seaborn.axisgrid.FacetGrid at 0x7f1be5865898&gt; . train_df=train_df.drop(&quot;Province_State&quot;,axis=1) from datetime import datetime from datetime import datetime train_df[&quot;Date&quot;]=train_df[&#39;Date&#39;].apply(lambda x:datetime.strptime(x, &#39;%Y-%m-%d&#39;)) train_df[&quot;week&quot;]=&quot;week_&quot;+ str(train_df[&quot;Date&quot;].dt.week) train_df[&quot;week&quot;]=train_df[&quot;Date&quot;].dt.week.apply(lambda x: x) train_df[&quot;day&quot;]=train_df[&quot;Date&quot;].dt.day.apply(lambda x: x) train_df[&quot;month&quot;]=train_df[&quot;Date&quot;].dt.month.apply(lambda x: int(x)) . train_df=train_df.drop(&quot;Date&quot;,axis=1) . Below I have used Plotly to create a CHOROPLETH Map of The CoronaVirus to the latest week . import numpy as np import pandas as pd import plotly as py import plotly.express as px import plotly.graph_objs as go from plotly.subplots import make_subplots from plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot init_notebook_mode(connected=True) country_df=train_df.groupby([&#39;Country_Region&#39;, &#39;week&#39;]).max().reset_index().sort_values(&#39;week&#39;, ascending=False) country_df = country_df.drop_duplicates(subset = [&#39;Country_Region&#39;]) country_df = country_df[country_df[&#39;ConfirmedCases&#39;]&gt;0] data = dict(type=&#39;choropleth&#39;, locations = country_df[&#39;Country_Region&#39;], locationmode = &#39;country names&#39;, z = country_df[&#39;ConfirmedCases&#39;], text = country_df[&#39;Country_Region&#39;], colorbar = {&#39;title&#39;:&#39;CONFIRMED CASES&#39;}, colorscale=[[0, &#39;rgb(224,255,255)&#39;], [0.01, &#39;rgb(166,206,227)&#39;], [0.02, &#39;rgb(31,120,180)&#39;], [0.03, &#39;rgb(178,223,138)&#39;], [0.05, &#39;rgb(51,160,44)&#39;], [0.10, &#39;rgb(251,154,153)&#39;], [0.20, &#39;rgb(255,255,0)&#39;], [1, &#39;rgb(227,26,28)&#39;]], reversescale = False ) layout = dict(title=&#39;COVID-19 CASES AROUND THE WORLD&#39;, geo = dict(showframe = True, projection={&#39;type&#39;:&#39;mercator&#39;})) choromap = go.Figure(data = [data], layout = layout) iplot(choromap, validate=False) . This again using plotly I have created the map which you can interact with the slider to see how the spread of coronavirus has affected the Countries starting from the 4th week of the year that was in January and till now in April YOU CAN HOVER FOR INFO OF THE CASES . df_countrydate = train_df[train_df[&#39;ConfirmedCases&#39;]&gt;0] df_countrydate = df_countrydate.groupby([&#39;week&#39;,&#39;Country_Region&#39;]).max().reset_index() df_countrydate fig = px.choropleth(df_countrydate, locations=&quot;Country_Region&quot;, locationmode = &quot;country names&quot;, color=&quot;ConfirmedCases&quot;, hover_name=&quot;Country_Region&quot;, animation_frame=&quot;week&quot;, color_continuous_scale=&quot;Greens&quot; ) fig.update_layout( title_text = &#39;Global Spread of Coronavirus&#39;, title_x = 0.5, geo=dict( showframe = False, showcoastlines = False, )) fig.show() . df_countrydate[df_countrydate[&quot;Country_Region&quot;]==&quot;India&quot;] . week Country_Region Id ConfirmedCases Fatalities day month . 19 5 | India | 15972 | 2 | 0.0 | 31 | 2 | . 45 6 | India | 15979 | 3 | 0.0 | 9 | 2 | . 72 7 | India | 15986 | 3 | 0.0 | 16 | 2 | . 99 8 | India | 15993 | 3 | 0.0 | 23 | 2 | . 146 9 | India | 16000 | 3 | 0.0 | 29 | 3 | . 223 10 | India | 16007 | 39 | 0.0 | 8 | 3 | . 342 11 | India | 16014 | 113 | 2.0 | 15 | 3 | . 494 12 | India | 16021 | 396 | 7.0 | 22 | 3 | . 665 13 | India | 16028 | 1024 | 27.0 | 29 | 3 | . 844 14 | India | 16035 | 3588 | 99.0 | 31 | 4 | . 1027 15 | India | 16042 | 9205 | 331.0 | 12 | 4 | . 1211 16 | India | 16049 | 17615 | 559.0 | 19 | 4 | . 1395 17 | India | 16056 | 27890 | 881.0 | 26 | 4 | . I would like to thank Mr. SRK for the dataset on COVID-19 IN INDIA ===&gt; https://www.kaggle.com/sudalairajkumar/covid19-in-india which i have used below . df_india=pd.read_csv(&quot;/kaggle/input/covid19-in-india/covid_19_india.csv&quot;) df_india[&quot;Date&quot;]=df_india[&#39;Date&#39;].apply(lambda x:datetime.strptime(x, &#39;%d/%m/%y&#39;)) df_india[&quot;week&quot;]=&quot;week_&quot;+ str(df_india[&quot;Date&quot;].dt.week) df_india[&quot;week&quot;]=df_india[&quot;Date&quot;].dt.week.apply(lambda x: x) df_india.head() df_india_grouped=df_india.groupby([&quot;State/UnionTerritory&quot;,&quot;week&quot;]).max().reset_index().sort_values(&quot;week&quot;,ascending=False) df_india_grouped=df_india_grouped.drop_duplicates(subset=[&quot;State/UnionTerritory&quot;]) . The Bar Plot provides info About the Statewise Confirmed Cases, You can hover on them . df_india_grouped fig = px.scatter(df_india_grouped, x=&quot;Confirmed&quot;, y=&quot;State/UnionTerritory&quot;, title=&quot;COVID CASES CONFIRMED IN INDIAN STATES&quot;, labels={&quot;COVID CASES CONFIRMED IN INDIAN STATES&quot;} # customize axis label ) fig = px.bar(df_india_grouped, x=&#39;Confirmed&#39;, y=&#39;State/UnionTerritory&#39;, hover_data=[&#39;Confirmed&#39;, &#39;State/UnionTerritory&#39;], color=&#39;Confirmed&#39;, orientation=&#39;h&#39;, text=&quot;Confirmed&quot;, height=1400) fig.update_traces( textposition=&#39;outside&#39;) fig.update_layout(uniformtext_minsize=8, uniformtext_mode=&#39;show&#39;) fig.show() . Here I have added a new column called pending which is basically how many patients are still being treated , I am going to use this in the below piechart I have created for each state depicting the states and how many cases are cured, deaths and pending . df_india_grouped[&quot;pending&quot;]=df_india_grouped[&quot;Confirmed&quot;]-df_india_grouped[&quot;Deaths&quot;]-df_india_grouped[&quot;Cured&quot;] . &#39;&#39;&#39;df_india_grouped labels=df_india_grouped[&quot;State/UnionTerritory&quot;] values=df_india_grouped[&quot;Confirmed&quot;] fig = go.Figure(data=[go.Pie(labels=labels, values=values, hole=.3)]) fig.show()&#39;&#39;&#39; l=list(df_india_grouped[&quot;State/UnionTerritory&quot;]) fig = make_subplots(rows=11, cols=3,subplot_titles=l,specs=[[{&#39;type&#39;:&#39;domain&#39;}, {&#39;type&#39;:&#39;domain&#39;},{&#39;type&#39;:&#39;domain&#39;}],[{&#39;type&#39;:&#39;domain&#39;}, {&#39;type&#39;:&#39;domain&#39;},{&#39;type&#39;:&#39;domain&#39;}],[{&#39;type&#39;:&#39;domain&#39;}, {&#39;type&#39;:&#39;domain&#39;},{&#39;type&#39;:&#39;domain&#39;}],[{&#39;type&#39;:&#39;domain&#39;}, {&#39;type&#39;:&#39;domain&#39;},{&#39;type&#39;:&#39;domain&#39;}],[{&#39;type&#39;:&#39;domain&#39;}, {&#39;type&#39;:&#39;domain&#39;},{&#39;type&#39;:&#39;domain&#39;}],[{&#39;type&#39;:&#39;domain&#39;}, {&#39;type&#39;:&#39;domain&#39;},{&#39;type&#39;:&#39;domain&#39;}],[{&#39;type&#39;:&#39;domain&#39;}, {&#39;type&#39;:&#39;domain&#39;},{&#39;type&#39;:&#39;domain&#39;}],[{&#39;type&#39;:&#39;domain&#39;}, {&#39;type&#39;:&#39;domain&#39;},{&#39;type&#39;:&#39;domain&#39;}],[{&#39;type&#39;:&#39;domain&#39;}, {&#39;type&#39;:&#39;domain&#39;},{&#39;type&#39;:&#39;domain&#39;}],[{&#39;type&#39;:&#39;domain&#39;}, {&#39;type&#39;:&#39;domain&#39;},{&#39;type&#39;:&#39;domain&#39;}],[{&#39;type&#39;:&#39;domain&#39;}, {&#39;type&#39;:&#39;domain&#39;},{&#39;type&#39;:&#39;domain&#39;}]]) a=1 b=1 for i in l: temp_df=df_india_grouped[df_india_grouped[&quot;State/UnionTerritory&quot;]==i] #print(int(temp_df[&quot;Deaths&quot;])) values=[int(temp_df[&quot;Deaths&quot;]),int(temp_df[&quot;Cured&quot;]),int(temp_df[&quot;pending&quot;])] labels=[&quot;Deaths&quot;,&quot;Cured&quot;,&quot;pending&quot;] #annot.append(dict(text=i,font_size=10, showarrow=False)) fig.add_trace(go.Pie(labels=labels, textposition=&quot;inside&quot;,values=values, name=i),a, b) if b==3 and a&lt;11: a=a+1 if b+1&gt;3: b=1 else: b=b+1 fig.update_traces(hole=.4) fig.update_layout( height=1900,width=1000 ) fig.update(layout_title_text=&#39;StateWise analysis of Positive cases&#39;) #fig = go.Figure(fig) fig.show() #iplot(fig) . testing_df=pd.read_csv(&quot;/kaggle/input/covid19-in-india/StatewiseTestingDetails.csv&quot;) . testing_df[&quot;Date&quot;]=testing_df[&#39;Date&#39;].apply(lambda x:datetime.strptime(x, &#39;%Y-%m-%d&#39;)) testing_df[&quot;week&quot;]=&quot;week_&quot;+ str(testing_df[&quot;Date&quot;].dt.week) testing_df[&quot;week&quot;]=testing_df[&quot;Date&quot;].dt.week.apply(lambda x: x) testing_df.head() testing_df_grouped=testing_df.groupby([&quot;State&quot;,&quot;week&quot;]).max().reset_index().sort_values(&quot;week&quot;,ascending=False) . testing_df_grouped=testing_df_grouped.drop_duplicates(subset=[&quot;State&quot;]) . states=list(testing_df_grouped[&quot;State&quot;]) fig = go.Figure(data=[ go.Bar(name=&#39;Negative&#39;, x=states, y=list(testing_df_grouped[&quot;Negative&quot;])), go.Bar(name=&#39;Positive&#39;, x=states, y=list(testing_df_grouped[&quot;Positive&quot;])), ]) fig.update_layout(barmode=&#39;stack&#39;) fig.show() . Now it is time to plot another choropleth map but this time for India staetwise, for this I added a dataset containing the shape files indian state . import geopandas as gpd shapefile=&quot;/kaggle/input/india-shape/ind_shape/IND_adm1.shp&quot; gdf=gpd.read_file(shapefile)[[&quot;NAME_1&quot;,&quot;geometry&quot;]] gdf.columns = [&#39;states&#39;,&#39;geometry&#39;] gdf.loc[31,&quot;states&quot;]=&quot;Telengana&quot; gdf.loc[34,&quot;states&quot;]=&quot;Uttarakhand&quot; gdf.loc[25,&quot;states&quot;]=&quot;Odisha&quot; #gdf[gdf[&quot;states&quot;]==&quot;Orissa&quot;] . Below I have merged the Geopandas dataframe containing geometry and state names with our dataset of covid-19 indian states and used a json converted to convert it into json . merged_grouped = gdf.merge(df_india_grouped, left_on = &#39;states&#39;, right_on = &#39;State/UnionTerritory&#39;).drop([&quot;Date&quot;],axis=1) import json merged_json_grouped = json.loads(merged_grouped.to_json()) json_data_grouped = json.dumps(merged_json_grouped) . I have used Bokeh instaed of plotly here instead of plotly to demonstarte another method that we can create Choropleth map although we can see it requires more code and can get complicated . from bokeh.io import output_notebook, show, output_file from bokeh.plotting import figure from bokeh.models import GeoJSONDataSource, LinearColorMapper, ColorBar,LabelSet from bokeh.palettes import brewer from bokeh.models import Slider, HoverTool geosource = GeoJSONDataSource(geojson = json_data_grouped) palette = brewer[&#39;YlGnBu&#39;][8] palette = palette[::-1] color_mapper = LinearColorMapper(palette = palette, low = 0, high = max(merged_grouped[&quot;Confirmed&quot;])) tick_labels = {&#39;0&#39;: &#39;0&#39;, &#39;100&#39;: &#39;100&#39;, &#39;200&#39;:&#39;200&#39;, &#39;400&#39;:&#39;400&#39;, &#39;800&#39;:&#39;800&#39;, &#39;1200&#39;:&#39;1200&#39;, &#39;1400&#39;:&#39;1400&#39;,&#39;1800&#39;:&#39;1800&#39;, &#39;2000&#39;: &#39;2000&#39;} hover = HoverTool(tooltips = [ (&#39;states&#39;,&#39;@states&#39;),(&#39;Confirmed_Cases&#39;, &#39;@Confirmed&#39;)]) color_bar = ColorBar(color_mapper=color_mapper, label_standoff=8,width = 500, height = 20, border_line_color=None,location = (0,0), orientation = &#39;horizontal&#39;, major_label_overrides = tick_labels) p = figure(title = &#39;CoronaVirus Confirmed States(HOVER MOUSE FOR INFO)&#39;, plot_height = 600 , plot_width = 950, toolbar_location = None,tools=[hover]) p.xgrid.grid_line_color = None p.ygrid.grid_line_color = None p.patches(&#39;xs&#39;,&#39;ys&#39;, source = geosource,fill_color = {&#39;field&#39; :&#39;Confirmed&#39;, &#39;transform&#39; : color_mapper},name=&quot;states&quot;, line_color = &#39;black&#39;, line_width = 0.25, fill_alpha = 1) labels = LabelSet(x=&#39;xs&#39;, y=&#39;ys&#39;, text=&#39;states&#39;, x_offset=5, y_offset=5, source=geosource) p.add_layout(color_bar, &#39;below&#39;) output_notebook() #Display figure. show(p) . Loading BokehJS ... country_df=df_india.groupby([&quot;week&quot;,&quot;State/UnionTerritory&quot;]).max().reset_index() country_df.drop([&quot;Date&quot;,&quot;ConfirmedIndianNational&quot;,&quot;ConfirmedForeignNational&quot;,&quot;Deaths&quot;,&quot;Cured&quot;,&quot;Time&quot;],axis=1,inplace=True) . Now over here the same way we created the animation of the world map before , we want to create it similary for inidian states, we are using plotly instead of Bokeh because for Bokeh we needed to create a bokeh server to get that interactivity , but we can simply get it more easily with plotly ALSO NOTE:** Below in the code i have used geoseries function SIMPLIFY() as the plot created was very laggy due to the multiploygon geometry of the indian states so using SIMPLIFY(Tolerance=0.02) which kind of straightens some wiggles and curves to a line, but still I think a 0.02 tolerance provides an accurate shape of the map . shapefile=&quot;/kaggle/input/india-shape/ind_shape/IND_adm1.shx&quot; gdf=gpd.read_file(shapefile)[[&quot;NAME_1&quot;,&quot;geometry&quot;]] gdf[&quot;geometry&quot;]=gdf[&quot;geometry&quot;].simplify(0.02, preserve_topology=True) gdf gdf.columns = [&#39;states&#39;,&#39;geometry&#39;] gdf.loc[31,&quot;states&quot;]=&quot;Telengana&quot; gdf.loc[34,&quot;states&quot;]=&quot;Uttarakhand&quot; gdf.loc[25,&quot;states&quot;]=&quot;Odisha&quot; merged_grouped = gdf#.merge(df_india_grouped[[&quot;State/UnionTerritory&quot;,&quot;geo&quot;]], left_on = &#39;states&#39;, right_on = &#39;State/UnionTerritory&#39;) merged_json_grouped = json.loads(merged_grouped.to_json()) json_data_grouped = json.dumps(merged_json_grouped) for i in merged_json_grouped[&quot;features&quot;]: i[&quot;id&quot;]=i[&quot;properties&quot;][&quot;states&quot;] . country_df=df_india.groupby([&quot;State/UnionTerritory&quot;,&quot;week&quot;]).max().reset_index().sort_values(&quot;week&quot;,ascending=True) country_df=country_df.drop([&#39;Sno&#39;, &#39;Date&#39;, &#39;Time&#39;, &#39;ConfirmedIndianNational&#39;, &#39;ConfirmedForeignNational&#39;, &#39;Cured&#39;, &#39;Deaths&#39;,],axis=1) . This Map created using plotly is interactive starting from week 4 , we can see it started from kerala and within few week it was massively spread over the Indian States . fig = px.choropleth(country_df, geojson=merged_json_grouped, locations=&quot;State/UnionTerritory&quot;, color=&quot;Confirmed&quot;, hover_name=&quot;State/UnionTerritory&quot;, animation_frame=&quot;week&quot;, color_continuous_scale=[&quot;yellow&quot;,&quot;orange&quot;,&quot;red&quot;], labels={&#39;Confirmed&#39;:&#39;Confirmed&#39;} ) fig.update_geos(fitbounds=&quot;locations&quot;, visible=False,projection_type=&quot;natural earth&quot;) fig.update_layout( title_text = &#39;India Spread of Coronavirus&#39;, title_x = 0.5, geo=dict( showframe = False, showcoastlines = False, )) fig.show() . Now we go for modelling our data , I am going to use XGBOOST although I am still working and on different models so this could be updated again , If you have any suggestions please do tell me in the comments :D . newtestdf=pd.read_csv(&quot;/kaggle/input/covid19-global-forecasting-week-4/test.csv&quot;) newtestdf . ForecastId Province_State Country_Region Date . 0 1 | NaN | Afghanistan | 2020-04-02 | . 1 2 | NaN | Afghanistan | 2020-04-03 | . 2 3 | NaN | Afghanistan | 2020-04-04 | . 3 4 | NaN | Afghanistan | 2020-04-05 | . 4 5 | NaN | Afghanistan | 2020-04-06 | . ... ... | ... | ... | ... | . 13454 13455 | NaN | Zimbabwe | 2020-05-10 | . 13455 13456 | NaN | Zimbabwe | 2020-05-11 | . 13456 13457 | NaN | Zimbabwe | 2020-05-12 | . 13457 13458 | NaN | Zimbabwe | 2020-05-13 | . 13458 13459 | NaN | Zimbabwe | 2020-05-14 | . 13459 rows × 4 columns . using inbuilt pandas encoder i encoded the names of the country regions . train_df.head() . Id Country_Region ConfirmedCases Fatalities day week month . 0 1 | Afghanistan | 0 | 0.0 | 22 | 4 | 1 | . 1 2 | Afghanistan | 0 | 0.0 | 23 | 4 | 1 | . 2 3 | Afghanistan | 0 | 0.0 | 24 | 4 | 1 | . 3 4 | Afghanistan | 0 | 0.0 | 25 | 4 | 1 | . 4 5 | Afghanistan | 0 | 0.0 | 26 | 4 | 1 | . train_df.columns . Index([&#39;Id&#39;, &#39;Country_Region&#39;, &#39;ConfirmedCases&#39;, &#39;Fatalities&#39;, &#39;day&#39;, &#39;week&#39;, &#39;month&#39;], dtype=&#39;object&#39;) . I took inspiration of the hyperparamters from here https://www.kaggle.com/pradeepkumarrajkumar/xgb-regressor . We scale using minmaxscaler and also transform the country data into numeric using label encoding(not get_dummies as i did not get a good score before :D) . from sklearn.preprocessing import LabelEncoder,StandardScaler,MinMaxScaler train_df[&#39;ConfirmedCases&#39;] = train_df[&#39;ConfirmedCases&#39;].apply(int) train_df[&#39;Fatalities&#39;] = train_df[&#39;Fatalities&#39;].apply(int) cases = train_df.ConfirmedCases fatal=train_df.Fatalities lb = LabelEncoder() del train_df[&quot;Fatalities&quot;] del train_df[&quot;ConfirmedCases&quot;] #del train_df[&quot;Id&quot;] train_df[&#39;Country_Region&#39;] = lb.fit_transform(train_df[&#39;Country_Region&#39;]) scaler = MinMaxScaler() X_train = scaler.fit_transform(train_df.drop([&quot;Id&quot;,&quot;week&quot;],axis=1).values) . from xgboost import XGBRegressor model = XGBRegressor(n_estimators = 2500 , random_state = 0 , max_depth = 27) model.fit(X_train,cases) . XGBRegressor(base_score=0.5, booster=None, colsample_bylevel=1, colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1, importance_type=&#39;gain&#39;, interaction_constraints=None, learning_rate=0.300000012, max_delta_step=0, max_depth=27, min_child_weight=1, missing=nan, monotone_constraints=None, n_estimators=2500, n_jobs=0, num_parallel_tree=1, objective=&#39;reg:squarederror&#39;, random_state=0, reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1, tree_method=None, validate_parameters=False, verbosity=None) . model.score(X_train,cases) . 0.5526724827782268 . newtestdf[&quot;Date&quot;]=newtestdf[&#39;Date&#39;].apply(lambda x:datetime.strptime(x, &#39;%Y-%m-%d&#39;)) newtestdf[&quot;week&quot;]=&quot;week_&quot;+ str(newtestdf[&quot;Date&quot;].dt.week) newtestdf[&quot;week&quot;]=newtestdf[&quot;Date&quot;].dt.week.apply(lambda x: x) newtestdf[&quot;day&quot;]=newtestdf[&quot;Date&quot;].dt.day.apply(lambda x: x) newtestdf[&quot;month&quot;]=newtestdf[&quot;Date&quot;].dt.month.apply(lambda x: int(x)) newtestdf[&#39;Country_Region&#39;] = lb.fit_transform(newtestdf[&#39;Country_Region&#39;]) newtestdf=newtestdf.drop([&quot;Province_State&quot;,&quot;Date&quot;],axis=1) newtestdf . ForecastId Country_Region week day month . 0 1 | 0 | 14 | 2 | 4 | . 1 2 | 0 | 14 | 3 | 4 | . 2 3 | 0 | 14 | 4 | 4 | . 3 4 | 0 | 14 | 5 | 4 | . 4 5 | 0 | 15 | 6 | 4 | . ... ... | ... | ... | ... | ... | . 13454 13455 | 183 | 19 | 10 | 5 | . 13455 13456 | 183 | 20 | 11 | 5 | . 13456 13457 | 183 | 20 | 12 | 5 | . 13457 13458 | 183 | 20 | 13 | 5 | . 13458 13459 | 183 | 20 | 14 | 5 | . 13459 rows × 5 columns . X_test = scaler.fit_transform(newtestdf.drop([&quot;ForecastId&quot;,&quot;week&quot;],axis=1).values) cases_pred = model.predict(X_test) . cases_pred = np.around(cases_pred,decimals = 0) x_train_cas = [] for i in range(len(X_train)): x = list(X_train[i]) x.append(cases[i]) x_train_cas.append(x) . x_train_cas = np.array(x_train_cas) model = XGBRegressor(n_estimators = 2500 , random_state = 0 , max_depth = 27) model.fit(x_train_cas,fatal) . XGBRegressor(base_score=0.5, booster=None, colsample_bylevel=1, colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1, importance_type=&#39;gain&#39;, interaction_constraints=None, learning_rate=0.300000012, max_delta_step=0, max_depth=27, min_child_weight=1, missing=nan, monotone_constraints=None, n_estimators=2500, n_jobs=0, num_parallel_tree=1, objective=&#39;reg:squarederror&#39;, random_state=0, reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1, tree_method=None, validate_parameters=False, verbosity=None) . x_test_cas = [] for i in range(len(X_test)): x = list(X_test[i]) x.append(cases_pred[i]) x_test_cas.append(x) x_test_cas[0] . [0.0, 0.034482758620689655, 0.0, -0.0] . x_test_cas = np.array(x_test_cas) fatalities_pred =model.predict(x_test_cas) fatalities_pred = np.around(fatalities_pred,decimals = 0) . submission = pd.read_csv(&quot;../input/covid19-global-forecasting-week-4/submission.csv&quot;) submission[&#39;ConfirmedCases&#39;] = cases_pred submission[&#39;Fatalities&#39;] = fatalities_pred submission.to_csv(&quot;submission.csv&quot; , index = False) . THANK YOU :D PLEASE DO UPVOTE!!!!! . THANK YOU!!!! . File &#34;&lt;ipython-input-44-99bf4749447d&gt;&#34;, line 1 THANK YOU!!!! ^ SyntaxError: invalid syntax .",
            "url": "https://omegaji.github.io/DataScienceBlog/2020/05/21/covid-19-india-maps-eda-xgboost.html",
            "relUrl": "/2020/05/21/covid-19-india-maps-eda-xgboost.html",
            "date": " • May 21, 2020"
        }
        
    
  
    
        ,"post4": {
            "title": "Fastpages Notebook Blog Post",
            "content": "About . This notebook is a demonstration of some of capabilities of fastpages with notebooks. . With fastpages you can save your jupyter notebooks into the _notebooks folder at the root of your repository, and they will be automatically be converted to Jekyll compliant blog posts! . Front Matter . The first cell in your Jupyter Notebook or markdown blog post contains front matter. Front matter is metadata that can turn on/off options in your Notebook. It is formatted like this: . # &quot;My Title&quot; &gt; &quot;Awesome summary&quot; - toc:true- branch: master- badges: true- comments: true - author: Hamel Husain &amp; Jeremy Howard - categories: [fastpages, jupyter] . Setting toc: true will automatically generate a table of contents | Setting badges: true will automatically include GitHub and Google Colab links to your notebook. | Setting comments: true will enable commenting on your blog post, powered by utterances. | . The title and description need to be enclosed in double quotes only if they include special characters such as a colon. More details and options for front matter can be viewed on the front matter section of the README. . Markdown Shortcuts . A #hide comment at the top of any code cell will hide both the input and output of that cell in your blog post. . A #hide_input comment at the top of any code cell will only hide the input of that cell. . The comment #hide_input was used to hide the code that produced this. . put a #collapse-hide flag at the top of any cell if you want to hide that cell by default, but give the reader the option to show it: . #collapse-hide import pandas as pd import altair as alt . . put a #collapse-show flag at the top of any cell if you want to show that cell by default, but give the reader the option to hide it: . #collapse-show cars = &#39;https://vega.github.io/vega-datasets/data/cars.json&#39; movies = &#39;https://vega.github.io/vega-datasets/data/movies.json&#39; sp500 = &#39;https://vega.github.io/vega-datasets/data/sp500.csv&#39; stocks = &#39;https://vega.github.io/vega-datasets/data/stocks.csv&#39; flights = &#39;https://vega.github.io/vega-datasets/data/flights-5k.json&#39; . . Interactive Charts With Altair . Charts made with Altair remain interactive. Example charts taken from this repo, specifically this notebook. . Example 1: DropDown . # single-value selection over [Major_Genre, MPAA_Rating] pairs # use specific hard-wired values as the initial selected values selection = alt.selection_single( name=&#39;Select&#39;, fields=[&#39;Major_Genre&#39;, &#39;MPAA_Rating&#39;], init={&#39;Major_Genre&#39;: &#39;Drama&#39;, &#39;MPAA_Rating&#39;: &#39;R&#39;}, bind={&#39;Major_Genre&#39;: alt.binding_select(options=genres), &#39;MPAA_Rating&#39;: alt.binding_radio(options=mpaa)} ) # scatter plot, modify opacity based on selection alt.Chart(movies).mark_circle().add_selection( selection ).encode( x=&#39;Rotten_Tomatoes_Rating:Q&#39;, y=&#39;IMDB_Rating:Q&#39;, tooltip=&#39;Title:N&#39;, opacity=alt.condition(selection, alt.value(0.75), alt.value(0.05)) ) . Example 2: Tooltips . alt.Chart(movies).mark_circle().add_selection( alt.selection_interval(bind=&#39;scales&#39;, encodings=[&#39;x&#39;]) ).encode( x=&#39;Rotten_Tomatoes_Rating:Q&#39;, y=alt.Y(&#39;IMDB_Rating:Q&#39;, axis=alt.Axis(minExtent=30)), # use min extent to stabilize axis title placement tooltip=[&#39;Title:N&#39;, &#39;Release_Date:N&#39;, &#39;IMDB_Rating:Q&#39;, &#39;Rotten_Tomatoes_Rating:Q&#39;] ).properties( width=600, height=400 ) . Example 3: More Tooltips . # select a point for which to provide details-on-demand label = alt.selection_single( encodings=[&#39;x&#39;], # limit selection to x-axis value on=&#39;mouseover&#39;, # select on mouseover events nearest=True, # select data point nearest the cursor empty=&#39;none&#39; # empty selection includes no data points ) # define our base line chart of stock prices base = alt.Chart().mark_line().encode( alt.X(&#39;date:T&#39;), alt.Y(&#39;price:Q&#39;, scale=alt.Scale(type=&#39;log&#39;)), alt.Color(&#39;symbol:N&#39;) ) alt.layer( base, # base line chart # add a rule mark to serve as a guide line alt.Chart().mark_rule(color=&#39;#aaa&#39;).encode( x=&#39;date:T&#39; ).transform_filter(label), # add circle marks for selected time points, hide unselected points base.mark_circle().encode( opacity=alt.condition(label, alt.value(1), alt.value(0)) ).add_selection(label), # add white stroked text to provide a legible background for labels base.mark_text(align=&#39;left&#39;, dx=5, dy=-5, stroke=&#39;white&#39;, strokeWidth=2).encode( text=&#39;price:Q&#39; ).transform_filter(label), # add text labels for stock prices base.mark_text(align=&#39;left&#39;, dx=5, dy=-5).encode( text=&#39;price:Q&#39; ).transform_filter(label), data=stocks ).properties( width=700, height=400 ) . Data Tables . You can display tables per the usual way in your blog: . movies = &#39;https://vega.github.io/vega-datasets/data/movies.json&#39; df = pd.read_json(movies) # display table with pandas df[[&#39;Title&#39;, &#39;Worldwide_Gross&#39;, &#39;Production_Budget&#39;, &#39;Distributor&#39;, &#39;MPAA_Rating&#39;, &#39;IMDB_Rating&#39;, &#39;Rotten_Tomatoes_Rating&#39;]].head() . Title Worldwide_Gross Production_Budget Distributor MPAA_Rating IMDB_Rating Rotten_Tomatoes_Rating . 0 The Land Girls | 146083.0 | 8000000.0 | Gramercy | R | 6.1 | NaN | . 1 First Love, Last Rites | 10876.0 | 300000.0 | Strand | R | 6.9 | NaN | . 2 I Married a Strange Person | 203134.0 | 250000.0 | Lionsgate | None | 6.8 | NaN | . 3 Let&#39;s Talk About Sex | 373615.0 | 300000.0 | Fine Line | None | NaN | 13.0 | . 4 Slam | 1087521.0 | 1000000.0 | Trimark | R | 3.4 | 62.0 | . Images . Local Images . You can reference local images and they will be copied and rendered on your blog automatically. You can include these with the following markdown syntax: . ![](my_icons/fastai_logo.png) . . Remote Images . Remote images can be included with the following markdown syntax: . ![](https://image.flaticon.com/icons/svg/36/36686.svg) . . Animated Gifs . Animated Gifs work, too! . ![](https://upload.wikimedia.org/wikipedia/commons/7/71/ChessPawnSpecialMoves.gif) . . Captions . You can include captions with markdown images like this: . ![](https://www.fast.ai/images/fastai_paper/show_batch.png &quot;Credit: https://www.fast.ai/2020/02/13/fastai-A-Layered-API-for-Deep-Learning/&quot;) . . Other Elements . GitHub Flavored Emojis . Typing I give this post two :+1:! will render this: . I give this post two :+1:! . Tweetcards . Typing &gt; twitter: https://twitter.com/jakevdp/status/1204765621767901185?s=20 will render this: Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 . Youtube Videos . Typing &gt; youtube: https://youtu.be/XfoYk_Z5AkI will render this: . Boxes / Callouts . Typing &gt; Warning: There will be no second warning! will render this: . Warning: There will be no second warning! . Typing &gt; Important: Pay attention! It&#39;s important. will render this: . Important: Pay attention! It&#8217;s important. . Typing &gt; Tip: This is my tip. will render this: . Tip: This is my tip. . Typing &gt; Note: Take note of this. will render this: . Note: Take note of this. . Typing &gt; Note: A doc link to [an example website: fast.ai](https://www.fast.ai/) should also work fine. will render in the docs: . Note: A doc link to an example website: fast.ai should also work fine. . Footnotes . You can have footnotes in notebooks, however the syntax is different compared to markdown documents. This guide provides more detail about this syntax, which looks like this: . For example, here is a footnote {% fn 1 %}. And another {% fn 2 %} {{ &#39;This is the footnote.&#39; | fndetail: 1 }} {{ &#39;This is the other footnote. You can even have a [link](www.github.com)!&#39; | fndetail: 2 }} . For example, here is a footnote 1. . And another 2 . 1. This is the footnote.↩ . 2. This is the other footnote. You can even have a link!↩ .",
            "url": "https://omegaji.github.io/DataScienceBlog/jupyter/2020/02/20/test.html",
            "relUrl": "/jupyter/2020/02/20/test.html",
            "date": " • Feb 20, 2020"
        }
        
    
  
    
        ,"post5": {
            "title": "An Example Markdown Post",
            "content": "Example Markdown Post . Basic setup . Jekyll requires blog post files to be named according to the following format: . YEAR-MONTH-DAY-filename.md . Where YEAR is a four-digit number, MONTH and DAY are both two-digit numbers, and filename is whatever file name you choose, to remind yourself what this post is about. .md is the file extension for markdown files. . The first line of the file should start with a single hash character, then a space, then your title. This is how you create a “level 1 heading” in markdown. Then you can create level 2, 3, etc headings as you wish but repeating the hash character, such as you see in the line ## File names above. . Basic formatting . You can use italics, bold, code font text, and create links. Here’s a footnote 1. Here’s a horizontal rule: . . Lists . Here’s a list: . item 1 | item 2 | . And a numbered list: . item 1 | item 2 | Boxes and stuff . This is a quotation . . You can include alert boxes …and… . . You can include info boxes Images . . Code . You can format text and code per usual . General preformatted text: . # Do a thing do_thing() . Python code and output: . # Prints &#39;2&#39; print(1+1) . 2 . Formatting text as shell commands: . echo &quot;hello world&quot; ./some_script.sh --option &quot;value&quot; wget https://example.com/cat_photo1.png . Formatting text as YAML: . key: value - another_key: &quot;another value&quot; . Tables . Column 1 Column 2 . A thing | Another thing | . Tweetcards . Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 Footnotes . This is the footnote. &#8617; . |",
            "url": "https://omegaji.github.io/DataScienceBlog/markdown/2020/01/14/test-markdown-post.html",
            "relUrl": "/markdown/2020/01/14/test-markdown-post.html",
            "date": " • Jan 14, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "This is where you put the contents of your About page. Like all your pages, it’s in Markdown format. . This website is powered by fastpages 1. . a blogging platform that natively supports Jupyter notebooks in addition to other formats. &#8617; . |",
          "url": "https://omegaji.github.io/DataScienceBlog/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://omegaji.github.io/DataScienceBlog/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}