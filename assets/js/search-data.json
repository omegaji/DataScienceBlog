{
  
    
        "post0": {
            "title": "Bots Detection the beginning",
            "content": "Welcome to my kernel. I have processed some columns to get better meaning of them including the user_agent strings and I have created new csv file containing the parsed UA as it took time in parsing so I did not want others to waste their time on parsing it :D . Please do upvote if you find this kernel helpful or else just upvote :D . THE NEW DATA CREATED CAN BE FOUND HERE YOU CAN USE IT IN YOUR KERNELS!! . List of Contents . Os Name Extraction and shortening/Grouping | Plotting User views and visits | What are user agents? | Parsing User Agent String with DeviceDetector | Parsing The first half sample | Detecting Bot or not from user agent | we plot how many bots are from which os | we plot how many bots are from which browser | Convert the first half of the data to csv | Convert the second half | Concatenate both and final dataframe savse as csv | Future work? | . # This Python 3 environment comes with many helpful analytics libraries installed # It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python # For example, here&#39;s several helpful packages to load import numpy as np # linear algebra import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv) # Input data files are available in the read-only &quot;../input/&quot; directory # For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory import os for dirname, _, filenames in os.walk(&#39;/kaggle/input&#39;): for filename in filenames: print(os.path.join(dirname, filename)) # You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using &quot;Save &amp; Run All&quot; # You can also write temporary files to /kaggle/temp/, but they won&#39;t be saved outside of the current session . /kaggle/input/bot-detection/ibm_data.csv . df=pd.read_csv(&quot;/kaggle/input/bot-detection/ibm_data.csv&quot;) df . Unnamed: 0 ctry_name intgrtd_mngmt_name intgrtd_operating_team_name city st sec_lvl_domn device_type operating_sys ip_addr user_agent VISIT ENGD_VISIT VIEWS page_url wk mth yr page_vw_ts . 0 0 | United States | United States | North America | SLIDELL | LOUISIANA | CHARTER.COM | MOBILEPHONE | IOS_12.1.4 | 287e8e9aeedb50e963906f10cca7ca26ae830154e69220... | MOZILLA/5.0 (IPHONE; CPU IPHONE OS 12_1_4 LIKE... | 1 | 0 | 1 | www.ibm.com/watson/campaign | 11 | 3 | 2019 | 2019-06-04 05:05:18.023100 | . 1 1 | Japan | Japan | Japan | TOKYO | TOKYO | MOPERA.NET | TABLET | ANDROID_6.0 | d7746df5cc2de7f79584d57c2c082b9acc7697602021a1... | MOZILLA/5.0 (LINUX; ANDROID 6.0; D-01J BUILD/H... | 1 | 0 | 1 | www.ibm.com/privacy/us/en | 11 | 3 | 2019 | 2019-06-04 05:07:11.014300 | . 2 2 | United States | United States | North America | ELK GROVE | CALIFORNIA | COMCASTBUSINESS.NET | MOBILEPHONE | IOS_12.1.2 | 8540464f5f376c7a160d63632f8cbedc96c61158daf9ae... | MOZILLA/5.0 (IPHONE; CPU IPHONE OS 12_1_2 LIKE... | 1 | 0 | 1 | www.ibm.com/account/reg/us-en/signup?formid=ur... | 11 | 3 | 2019 | 2019-06-04 05:08:46.081900 | . 3 3 | Brazil | Brazil | Latin America | SAO FRANCISCO DE GOIAS | GOIAS | VIVOZAP.COM.BR | MOBILEPHONE | ANDROID_6.0 | cb9ffa7be250fc62426a431a4f08bc0c8222f63514ba39... | MOZILLA/5.0 (LINUX; ANDROID 6.0; PT-BR; 5010E ... | 1 | 0 | 1 | www.ibm.com/analytics/br/pt/business-intelligence | 11 | 3 | 2019 | 2019-06-04 05:07:22.033300 | . 4 4 | France | France | Europe | BEZONS | VAL-D&#39;OISE | PROXAD.NET | MOBILEPHONE | ANDROID_9 | 7ce278be1b02a0253cc0219fa9ceddfe8e91846be343a4... | MOZILLA/5.0 (LINUX; ANDROID 9; SM-G965F BUILD/... | 1 | 0 | 1 | www.ibm.com/watson/fr-fr | 11 | 3 | 2019 | 2019-06-04 05:05:27.027700 | . ... ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | . 1048568 1048568 | United States | United States | North America | CUPERTINO | CALIFORNIA | NaN | NaN | MACINTOSH_OS X 10.14 | e37ade1326801a855c9b2d1bdb1f84399547c27178a7e7... | MOZILLA/5.0 (MACINTOSH; INTEL MAC OS X 10_14_2... | 1 | 0 | 1 | www.ibm.com/talent-management/news/talent-acqu... | 11 | 3 | 2019 | 2019-06-04 13:27:37.059600 | . 1048569 1048569 | Australia | Australia/NZ | Asia Pacific | SYDNEY | NEW SOUTH WALES | OPTUSNET.COM.AU | NaN | MACINTOSH_OS X 10.13 | e926236cbc5da5db8bad29cdbb3537b867aa26e73a5f79... | MOZILLA/5.0 (MACINTOSH; INTEL MAC OS X 10_13_1... | 1 | 0 | 1 | www.ustream.tv/channel/nba4live | 11 | 3 | 2019 | 2019-06-04 13:27:04.035100 | . 1048570 1048570 | United States | United States | North America | CLEVELAND | OHIO | PARKER.COM | NaN | MICROSOFT_WINDOWS10 | 3440973e4b965b19fa7e67b02c10ecb1d6ccc3c62e0d92... | MOZILLA/5.0 (WINDOWS NT 10.0; WIN64; X64) APPL... | 1 | 1 | 1 | www.ibm.com/support/knowledgecenter/en/search/... | 11 | 3 | 2019 | 2019-06-04 13:40:47.060500 | . 1048571 1048571 | Japan | Japan | Japan | CHIYODA-KU | TOKYO | ARGO-GRAPH.CO.JP | NaN | MICROSOFT_WINDOWS10 | 0ff0318b41d99bf2240173368a7d861ef512a85d0abf15... | MOZILLA/5.0 (WINDOWS NT 10.0; WIN64; X64) APPL... | 1 | 1 | 1 | www-945.ibm.com/support/fixcentral/options | 11 | 3 | 2019 | 2019-06-04 13:22:12.001900 | . 1048572 1048572 | United States | United States | North America | CHICAGO | ILLINOIS | NaN | NaN | MICROSOFT_WINDOWS10 | 1d2922b3be5a885203c932991051bfc9269596e700e51b... | MOZILLA/5.0 (WINDOWS NT 10.0; WOW64) APPLEWEBK... | 1 | 0 | 1 | www.ibm.com/support/knowledgecenter/en/ssepgg_... | 11 | 3 | 2019 | 2019-06-04 13:39:50.074200 | . 1048573 rows × 19 columns . checking for nulls in operating systems column and replacing them with notgiven, we then find out the unique values of this column . print(df.operating_sys.isnull().sum()) df.operating_sys.fillna(&quot;NotGiven&quot;,inplace=True) print(df.operating_sys.unique()) . 1820 [&#39;IOS_12.1.4&#39; &#39;ANDROID_6.0&#39; &#39;IOS_12.1.2&#39; &#39;ANDROID_9&#39; &#39;ANDROID_8.0.0&#39; &#39;IOS_12.1&#39; &#39;IOS_11.3&#39; &#39;IOS_12.1.1&#39; &#39;ANDROID_8.1.0&#39; &#39;MICROSOFT_WINDOWS10&#39; &#39;ANDROID_7.0&#39; &#39;ANDROID_7.1.1&#39; &#39;MICROSOFT_WINDOWS7&#39; &#39;ANDROID_5.1.1&#39; &#39;ANDROID_8.0&#39; &#39;ANDROID_7.1.2&#39; &#39;MACINTOSH_OS X 10.12&#39; &#39;MACINTOSH_OS X 10.14&#39; &#39;MICROSOFT_WINDOWS8.1&#39; &#39;IOS_9.3.5&#39; &#39;MACINTOSH_OS X 10.13&#39; &#39;LINUX&#39; &#39;ANDROID_5.0.2&#39; &#39;ANDROID_6.0.1&#39; &#39;CHROMEOS&#39; &#39;MICROSOFT_WINDOWS8&#39; &#39;MACINTOSH_OS X 10.11&#39; &#39;OTHER&#39; &#39;IOS_12.0&#39; &#39;IOS_12.0.1&#39; &#39;MICROSOFT_WINXP&#39; &#39;IOS_11.4.1&#39; &#39;IOS_10.2&#39; &#39;ANDROID_5.1&#39; &#39;IOS_11.2.5&#39; &#39;IOS_11.2.2&#39; &#39;NotGiven&#39; &#39;IOS_11.4&#39; &#39;MACINTOSH_OS X 10.10&#39; &#39;ANDROID_2.3.4&#39; &#39;ANDROID_4.2.2&#39; &#39;IOS_11.0.3&#39; &#39;ANDROID_8.1&#39; &#39;IOS_10.3.3&#39; &#39;ANDROID_4.4.4&#39; &#39;ANDROID_4.3&#39; &#39;IOS_12.2&#39; &#39;ANDROID_4.4.2&#39; &#39;IOS_10.0.1&#39; &#39;IOS_12.1.3&#39; &#39;ANDROID_4.1.2&#39; &#39;IOS_11.2.1&#39; &#39;MACINTOSH_OS X 10.7&#39; &#39;NETCAST&#39; &#39;MACINTOSH_OS X 10.8&#39; &#39;IOS_9.3.2&#39; &#39;ANDROID_4.0.4&#39; &#39;IOS_9.2.1&#39; &#39;IOS_11.2.6&#39; &#39;IOS_11.0.1&#39; &#39;MACINTOSH_OS X 10.9&#39; &#39;MICROSOFT_WINVISTA&#39; &#39;IOS_9.1&#39; &#39;MACINTOSH_OS X 10.4&#39; &#39;MICROSOFT_WINNT&#39; &#39;ANDROID_5.0.1&#39; &#39;ANDROID_9.0&#39; &#39;ANDROID_4.0.3&#39; &#39;IOS_11.1.2&#39; &#39;IOS_10.2.1&#39; &#39;IOS_10.0.2&#39; &#39;IOS_10.3.2&#39; &#39;MACINTOSH_OS X 10.6&#39; &#39;ANDROID_4.4.3&#39; &#39;IOS_11.0.2&#39; &#39;IOS_10.1.1&#39; &#39;SONY PROPRIETARY&#39; &#39;ANDROID_5.0&#39; &#39;IOS_11.1&#39; &#39;WINDOWSPHONE_8.1&#39; &#39;IOS_9.0.2&#39; &#39;IOS_9.2&#39; &#39;IOS_10.3.1&#39; &#39;IOS_11.0&#39; &#39;IOS_10.1&#39; &#39;IOS_10.0.3&#39; &#39;IOS_8.0.2&#39; &#39;RIMOS_BB10&#39; &#39;IOS_8.0&#39; &#39;UNIX&#39; &#39;IOS_7.1&#39; &#39;SUNOSI86PC&#39; &#39;IOS_9.3&#39; &#39;FREEBSD&#39; &#39;IOS_10.3&#39; &#39;IOS_11.1.1&#39; &#39;ANDROID_4.4&#39; &#39;IOS_3.2&#39; &#39;IOS_9.3.4&#39; &#39;ANDROID&#39; &#39;IOS_11.2&#39; &#39;MICROSOFT_WIN98&#39; &#39;IOS_9.3.3&#39; &#39;ANDROID_4.2.1&#39; &#39;IOS_5.0&#39; &#39;OS/2_OS/2&#39; &#39;IOS_5.0.1&#39; &#39;IOS&#39; &#39;ANDROID_7.0.0&#39; &#39;IOS_9.3.1&#39; &#39;IOS_4&#39; &#39;WINDOWSPHONE_10.0&#39; &#39;RIMOS_6.0.0.546&#39; &#39;IOS_5.1&#39; &#39;IOS_7.0.4&#39; &#39;ANDROID_9.0.99&#39; &#39;IOS_12&#39; &#39;ANDROID_4.0.2&#39; &#39;WINDOWSRT_NT 6.3&#39; &#39;IOS_11.3.1&#39; &#39;MACINTOSH_OS X&#39; &#39;IOS_9.0.1&#39; &#39;WINDOWSPHONE_7.0&#39; &#39;ANDROID_4.0&#39; &#39;IOS_9.0&#39; &#39;MICROSOFT_WINDOWSME&#39; &#39;MACINTOSH_OS X 10.5&#39; &#39;ANDROID_4.1&#39; &#39;ANDROID_8.0ZH-CN&#39; &#39;MICROSOFT_WINDOWSANDROID10.0&#39; &#39;ANDROID_7.0.1&#39; &#39;SAILFISH&#39; &#39;IOS_8.1.1&#39; &#39;SYMBIANOS_9.1&#39; &#39;ANDROID_6.1&#39; &#39;ANDROID_2.2.1&#39; &#39;SUNOSSUN4U&#39; &#39;MACINTOSH_OS X 10.15&#39; &#39;ANDROID_9ZH-CN&#39; &#39;ANDROID_8.0ZH-TW&#39; &#39;MICROSOFT_WIN2000&#39; &#39;SYMBIANOS_9.4&#39; &#39;ANDROID_10.2.1&#39; &#39;ANDROID_7.1&#39; &#39;IOS_8.3&#39; &#39;IOS_4.0&#39; &#39;ANDROID_4.1.1&#39; &#39;0&#39; &#39;IOS_5.1.1&#39; &#39;IOS_7.0&#39; &#39;WEBOS&#39; &#39;SYMBIANOS&#39; &#39;WINDOWSPHONE_8.0&#39; &#39;IOS_4.1&#39; &#39;ANDROID_8.0.0EN-US&#39; &#39;IOS_7.1.1&#39; &#39;IOS_10.0&#39; &#39;IOS_6.0.2&#39; &#39;IOS_4.2.1&#39; &#39;IOS_8.4&#39; &#39;RIMOS_6.0.0.448&#39; &#39;ANDROID_4.0.1&#39; &#39;ANDROID_8.0.0ZH-CN&#39; &#39;IOS_6.0&#39; &#39;OPENBSD&#39; &#39;WINDOWSMOBILE_10.0&#39; &#39;ANDROID_6.0.1ZH-CN&#39; &#39;IOS_8.2&#39; &#39;IOS_9.1.1&#39; &#39;MACINTOSH_OTHER&#39; &#39;ANDROID_6.0ZH-CN&#39; &#39;IOS_7.1.2&#39; &#39;WINDOWSPHONE_7&#39; &#39;ANDROID_2.3.7&#39; &#39;NUCLEUSPLUS&#39;] . I create a function that will classify all different variants of each os as the same os for example MICROSOFT_WINDOWS8.1 MICROSOFT_WINXP are to be renamed to MICROSOFT PC . I do the same for the other types of os including mobile phones, I classify the ones not falling into these main types as others . def shortenos(x): #print(x) if &quot;microsoft&quot; in x.lower().split(&quot;_&quot;)[0]: x=&quot;MICROSOFT PC&quot; return x elif &quot;windowsphone&quot; in x.lower().split(&quot;_&quot;)[0]: x=&quot;WINDOWS MOBILE&quot; return x elif &quot;windowsmobile&quot; in x.lower().split(&quot;_&quot;)[0]: x=&quot;WINDOWS MOBILE&quot; return x elif &quot;macintosh&quot; in x.lower().split(&quot;_&quot;)[0]: x=&quot;MACOS PC&quot; return x elif &quot;ios&quot; in x.lower().split(&quot;_&quot;)[0]: x=&quot;IOS PHONE&quot; return x elif &quot;android&quot; in x.lower().split(&quot;_&quot;)[0]: x=&quot;ANDROID&quot; return x elif &quot;linux&quot; in x.lower().split(&quot;_&quot;)[0]: x=&quot;LINUX&quot; return x elif x.lower()==&quot;notgiven&quot;: x=&quot;NotGiven&quot; return x else: x=&quot;OTHER&quot; return x df[&quot;os&quot;]=df.operating_sys.apply(shortenos) . Now lets groupby the new os column we created and sum the values for VIEWS and VISIT . This will help us in analysing which of these users are belonging to which OS . os_df=df.groupby([&quot;os&quot;]).sum().reset_index() os_df . os Unnamed: 0 VISIT ENGD_VISIT VIEWS wk mth yr . 0 ANDROID | 61939089291 | 124822 | 28151 | 149597 | 1301520 | 354960 | 238888080 | . 1 IOS PHONE | 45202927075 | 92695 | 16871 | 103294 | 950125 | 259125 | 174391125 | . 2 LINUX | 13001138033 | 33489 | 8030 | 39737 | 272404 | 74292 | 49998516 | . 3 MACOS PC | 46572317434 | 94995 | 34432 | 118822 | 978395 | 266835 | 179579955 | . 4 MICROSOFT PC | 380205015829 | 810167 | 272009 | 1061213 | 7973196 | 2174508 | 1463443884 | . 5 NotGiven | 947605872 | 1914 | 99 | 1998 | 20020 | 5460 | 3674580 | . 6 OTHER | 1815171681 | 3583 | 1098 | 4471 | 37037 | 10101 | 6797973 | . 7 WINDOWS MOBILE | 68878663 | 157 | 21 | 178 | 1606 | 438 | 294774 | . Lets PLOT! I plotted barplot on the visit and views count you can hover for the value . import altair as alt import altair_render_script alt.data_transformers.disable_max_rows() base=alt.Chart(os_df).mark_bar().encode( x=&quot;os&quot;, y=&quot;VISIT&quot;, tooltip=[&quot;VISIT&quot;] ) base2=alt.Chart(os_df).mark_bar().encode( x=&quot;os&quot;, y=&quot;VIEWS&quot;,tooltip=[&quot;VIEWS&quot;] ) alt.hconcat(base,base2) . /kaggle/input/bot-detection/ibm_data.csv Define and register the kaggle renderer. Enable with alt.renderers.enable(&#39;kaggle&#39;) . As we saw microsoft leads in both VISIT and VIEWS as expected of their huge userbase . now comes the hard part...well for me because I did not understand the user agent part until some googling . What are user agents? . Every time your web browser makes a request to a website, it sends a HTTP Header called the &quot;User Agent&quot;. The User Agent string contains information about your web browser name, operating system, device type and lots of other useful bits of information. . But every browser sends its user agent in a different format, so decoding them can be very tricky.&lt;/p&gt; . you have many apis and libarries that decode them for you and even tell you if the area bot or not! . user-agents from pypi click here | device-detector(we will use this as it is faster) click here | what is my browser provides apis for this too well it is not free but there are some trial plans which might interest you click here | . &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; df.user_agent.dropna(inplace=True) . Installing device detector . !pip install device_detector . Collecting device_detector Downloading device_detector-0.10-py3-none-any.whl (552 kB) |████████████████████████████████| 552 kB 2.7 MB/s eta 0:00:01 Requirement already satisfied: pyyaml in /opt/conda/lib/python3.7/site-packages (from device_detector) (5.3.1) Requirement already satisfied: regex in /opt/conda/lib/python3.7/site-packages (from device_detector) (2020.4.4) Installing collected packages: device-detector Successfully installed device-detector-0.10 . you can check the docs over here https://pypi.org/project/device-detector/ . device = SoftwareDetector(ua).parse() . device.client_name() # &gt;&gt;&gt; Chrome Mobile device.client_short_name() # &gt;&gt;&gt; CM device.client_type() # &gt;&gt;&gt; browser device.client_version() # &gt;&gt;&gt; 58.0.3029.83 . device.os_name() # &gt;&gt;&gt; Android device.os_version() # &gt;&gt;&gt; 6.0 device.engine() # &gt;&gt;&gt; WebKit . device.device_brand_name() # &gt;&gt;&gt; &#39;&#39; device.device_brand() # &gt;&gt;&gt; &#39;&#39; device.device_model() # &gt;&gt;&gt; &#39;&#39; device.device_type() # &gt;&gt;&gt; &#39;&#39; . WHAT AM I EXTRACTING? . The Browser used | The os(although we already have &quot;os&quot; column but I did for the sake of experimenting | THE MAIN THING IS IT A BOT????(TRUE AND FALSE) | . from device_detector import SoftwareDetector . I create some functions which will be used in apply one will give the client name[browser name] and the other will give out the os name . NOTE:I will be trying these functions on sample data of 400000 as the original dataset takes too much time if you have any recommendation please do tell :D . NOTE: I will then convert this to a csv filer and save it and do the same process on the remaining data and convert that too a csv file too, so 2 csv files will be created and I will make it public so that we can easily load it without worrying about the user agents parsing . def parse_family(x): return SoftwareDetector(x).parse().client_name() def parse_os(x): return SoftwareDetector(x).parse().os_name() . here as I stated Ill be taking the 400000 as my sample dataset . sample_df=df[:400000] sample_df.user_agent.dropna(inplace=True) . /opt/conda/lib/python3.7/site-packages/pandas/core/series.py:4494: SettingWithCopyWarning: A value is trying to be set on a copy of a slice from a DataFrame See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy self._update_inplace(result) . The parse family function will parse the browser . x=sample_df[&quot;user_agent&quot;].apply(parse_family) . dropping some null values and the parse os will parse the os . sample_df[&quot;user_browser&quot;]=x sample_df.user_agent.dropna(inplace=True) sample_df[&quot;user_os&quot;]=sample_df.user_agent.apply(parse_os) . /opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:2: SettingWithCopyWarning: A value is trying to be set on a copy of a slice from a DataFrame. Try using .loc[row_indexer,col_indexer] = value instead See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy . sample_df . Unnamed: 0 ctry_name intgrtd_mngmt_name intgrtd_operating_team_name city st sec_lvl_domn device_type operating_sys ip_addr ... ENGD_VISIT VIEWS page_url wk mth yr page_vw_ts os user_browser user_os . 0 0 | United States | United States | North America | SLIDELL | LOUISIANA | CHARTER.COM | MOBILEPHONE | IOS_12.1.4 | 287e8e9aeedb50e963906f10cca7ca26ae830154e69220... | ... | 0 | 1 | www.ibm.com/watson/campaign | 11 | 3 | 2019 | 2019-06-04 05:05:18.023100 | IOS PHONE | Mobile Safari | iOS | . 1 1 | Japan | Japan | Japan | TOKYO | TOKYO | MOPERA.NET | TABLET | ANDROID_6.0 | d7746df5cc2de7f79584d57c2c082b9acc7697602021a1... | ... | 0 | 1 | www.ibm.com/privacy/us/en | 11 | 3 | 2019 | 2019-06-04 05:07:11.014300 | ANDROID | Chrome | Android | . 2 2 | United States | United States | North America | ELK GROVE | CALIFORNIA | COMCASTBUSINESS.NET | MOBILEPHONE | IOS_12.1.2 | 8540464f5f376c7a160d63632f8cbedc96c61158daf9ae... | ... | 0 | 1 | www.ibm.com/account/reg/us-en/signup?formid=ur... | 11 | 3 | 2019 | 2019-06-04 05:08:46.081900 | IOS PHONE | Mobile Safari | iOS | . 3 3 | Brazil | Brazil | Latin America | SAO FRANCISCO DE GOIAS | GOIAS | VIVOZAP.COM.BR | MOBILEPHONE | ANDROID_6.0 | cb9ffa7be250fc62426a431a4f08bc0c8222f63514ba39... | ... | 0 | 1 | www.ibm.com/analytics/br/pt/business-intelligence | 11 | 3 | 2019 | 2019-06-04 05:07:22.033300 | ANDROID | Chrome Webview | Android | . 4 4 | France | France | Europe | BEZONS | VAL-D&#39;OISE | PROXAD.NET | MOBILEPHONE | ANDROID_9 | 7ce278be1b02a0253cc0219fa9ceddfe8e91846be343a4... | ... | 0 | 1 | www.ibm.com/watson/fr-fr | 11 | 3 | 2019 | 2019-06-04 05:05:27.027700 | ANDROID | Chrome Webview | Android | . ... ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | . 399995 399995 | United States | United States | North America | CAMDEN | NEW JERSEY | VERIZON.NET | NaN | MICROSOFT_WINDOWS10 | 30b77951ff6d3f82bc23144c19ce59c1c1792bd4105eba... | ... | 1 | 1 | www.ibm.com/support/knowledgecenter/en/ssd29g_... | 11 | 3 | 2019 | 2019-06-04 06:37:25.045000 | MICROSOFT PC | Chrome | Windows | . 399996 399996 | United States | United States | North America | LOS ANGELES | CALIFORNIA | NaN | OTHER | OTHER | 15ad89bfad51c167a34488ead8d6c07877df2b748fd60d... | ... | 0 | 1 | www.ibm.com/privacy/details/es/es | 11 | 3 | 2019 | 2019-06-04 07:03:33.086800 | OTHER | Samsung Browser | GNU/Linux | . 399997 399997 | Germany | DACH | Europe | NEUOSTHEIM | BADEN-WUERTTEMBERG | CARAT-GRUPPE.DE | NaN | MICROSOFT_WINDOWS7 | ba4fe7ecb08b3e9c1f127f3a34ee50b523353f82b4069e... | ... | 1 | 1 | www.ibm.com/de-de/products/category/business | 11 | 3 | 2019 | 2019-06-04 07:00:50.074700 | MICROSOFT PC | Firefox | Windows | . 399998 399998 | Italy | Italy | Europe | LUNETTA | MANTOVA | MYNET.IT | NaN | MICROSOFT_WINDOWS10 | 050668202ee75f6f132deafd0e81dd4304944b3415c8bd... | ... | 1 | 2 | www.ibm.com/partnerworld/commerce/programs/ser... | 11 | 3 | 2019 | 2019-06-04 06:46:29.068900 | MICROSOFT PC | Chrome | Windows | . 399999 399999 | China | Greater China | Greater China Group | TANGSHAN | HEBEI | NaN | MOBILEPHONE | ANDROID_8.1.0 | e336d4b5623673a0402ddafdd013d41418a171d063b091... | ... | 0 | 1 | www-31.ibm.com/ibm/cn/smart | 11 | 3 | 2019 | 2019-06-04 07:01:47.049700 | ANDROID | Chrome Webview | Android | . 400000 rows × 22 columns . Note we import Device Detector below which could have been imported above as well instead of SoftwareDetector, but SoftwareDetector detects the software specifically and it faster than device_detector which is said in the docs :D, but device detector below will be needed to classify as bots or not . from device_detector import DeviceDetector def parse_is_bot(x): return DeviceDetector(x).parse().is_bot() sample_df.user_agent.dropna(inplace=True) sample_df[&quot;is_bot&quot;]=sample_df[&quot;user_agent&quot;].apply(parse_is_bot) . /opt/conda/lib/python3.7/site-packages/pandas/core/series.py:4494: SettingWithCopyWarning: A value is trying to be set on a copy of a slice from a DataFrame See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy self._update_inplace(result) /opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:5: SettingWithCopyWarning: A value is trying to be set on a copy of a slice from a DataFrame. Try using .loc[row_indexer,col_indexer] = value instead See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy &#34;&#34;&#34; . Some oses were empty because it couldnt be detected so I added unknown string to it . I then group the sample_df by users_os(the one from the User Agent string) and is bot column and use the Count() on each column . def replace_empty_user(x): if x==&quot;&quot;: return &quot;Unknown&quot; else: return x sample_df[&quot;user_os&quot;]=sample_df[&quot;user_os&quot;].apply(replace_empty_user) user_os_df=sample_df.groupby([&quot;user_os&quot;,&quot;is_bot&quot;]).count().reset_index() . /opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:6: SettingWithCopyWarning: A value is trying to be set on a copy of a slice from a DataFrame. Try using .loc[row_indexer,col_indexer] = value instead See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy . Renaming the column unamed to count . user_os_df.rename(columns={&quot;Unnamed: 0&quot;: &quot;count&quot;},inplace=True) user_os_df . user_os is_bot count ctry_name intgrtd_mngmt_name intgrtd_operating_team_name city st sec_lvl_domn device_type ... VISIT ENGD_VISIT VIEWS page_url wk mth yr page_vw_ts os user_browser . 0 Android | False | 45088 | 45088 | 45088 | 45088 | 42723 | 42786 | 24783 | 45076 | ... | 45088 | 45088 | 45088 | 45088 | 45088 | 45088 | 45088 | 44979 | 45088 | 45088 | . 1 Android | True | 26 | 26 | 26 | 26 | 2 | 2 | 26 | 26 | ... | 26 | 26 | 26 | 26 | 26 | 26 | 26 | 26 | 26 | 26 | . 2 BlackBerry OS | False | 14 | 14 | 14 | 14 | 14 | 14 | 8 | 14 | ... | 14 | 14 | 14 | 14 | 14 | 14 | 14 | 14 | 14 | 14 | . 3 CentOS | True | 24 | 24 | 24 | 24 | 24 | 24 | 0 | 24 | ... | 24 | 24 | 24 | 24 | 24 | 24 | 24 | 24 | 24 | 24 | . 4 Chrome OS | False | 693 | 693 | 693 | 693 | 685 | 685 | 507 | 0 | ... | 693 | 693 | 693 | 693 | 693 | 693 | 693 | 693 | 693 | 693 | . 5 Fedora | False | 465 | 465 | 465 | 465 | 461 | 461 | 393 | 0 | ... | 465 | 465 | 465 | 465 | 465 | 465 | 465 | 462 | 465 | 465 | . 6 FreeBSD | False | 3 | 3 | 3 | 3 | 3 | 3 | 3 | 0 | ... | 3 | 3 | 3 | 3 | 3 | 3 | 3 | 3 | 3 | 3 | . 7 GNU/Linux | False | 6495 | 6495 | 6495 | 6495 | 6381 | 6381 | 4644 | 75 | ... | 6495 | 6495 | 6495 | 6495 | 6495 | 6495 | 6495 | 6480 | 6495 | 6495 | . 8 GNU/Linux | True | 31 | 31 | 31 | 31 | 0 | 0 | 31 | 3 | ... | 31 | 31 | 31 | 31 | 31 | 31 | 31 | 31 | 31 | 31 | . 9 KaiOS | False | 359 | 359 | 359 | 359 | 359 | 359 | 1 | 359 | ... | 359 | 359 | 359 | 359 | 359 | 359 | 359 | 357 | 359 | 359 | . 10 Mac | False | 34064 | 34064 | 34064 | 34064 | 33435 | 33451 | 25702 | 0 | ... | 34064 | 34064 | 34064 | 34064 | 34064 | 34064 | 34064 | 33973 | 34064 | 34064 | . 11 Mint | False | 1 | 1 | 1 | 1 | 1 | 1 | 1 | 0 | ... | 1 | 1 | 1 | 1 | 1 | 1 | 1 | 1 | 1 | 1 | . 12 OS/2 | False | 2 | 2 | 2 | 2 | 2 | 2 | 2 | 0 | ... | 2 | 2 | 2 | 2 | 2 | 2 | 2 | 2 | 2 | 2 | . 13 Ordissimo | False | 1 | 1 | 1 | 1 | 1 | 1 | 1 | 0 | ... | 1 | 1 | 1 | 1 | 1 | 1 | 1 | 1 | 1 | 1 | . 14 PlayStation | False | 77 | 77 | 77 | 77 | 76 | 76 | 69 | 77 | ... | 77 | 77 | 77 | 77 | 77 | 77 | 77 | 75 | 77 | 77 | . 15 Sailfish OS | False | 1 | 1 | 1 | 1 | 1 | 1 | 1 | 1 | ... | 1 | 1 | 1 | 1 | 1 | 1 | 1 | 1 | 1 | 1 | . 16 Solaris | False | 3 | 3 | 3 | 3 | 3 | 3 | 3 | 0 | ... | 3 | 3 | 3 | 3 | 3 | 3 | 3 | 3 | 3 | 3 | . 17 Symbian | False | 1 | 1 | 1 | 1 | 1 | 1 | 1 | 1 | ... | 1 | 1 | 1 | 1 | 1 | 1 | 1 | 1 | 1 | 1 | . 18 Symbian OS Series 60 | False | 1 | 1 | 1 | 1 | 1 | 1 | 1 | 1 | ... | 1 | 1 | 1 | 1 | 1 | 1 | 1 | 1 | 1 | 1 | . 19 Tizen | False | 63 | 63 | 63 | 63 | 63 | 63 | 40 | 62 | ... | 63 | 63 | 63 | 63 | 63 | 63 | 63 | 63 | 63 | 63 | . 20 Ubuntu | False | 2589 | 2589 | 2589 | 2589 | 2567 | 2567 | 1979 | 0 | ... | 2589 | 2589 | 2589 | 2589 | 2589 | 2589 | 2589 | 2582 | 2589 | 2589 | . 21 Unknown | False | 142 | 142 | 142 | 142 | 140 | 140 | 140 | 2 | ... | 142 | 142 | 142 | 142 | 142 | 142 | 142 | 141 | 142 | 142 | . 22 Unknown | True | 471 | 471 | 471 | 471 | 471 | 471 | 471 | 0 | ... | 471 | 471 | 471 | 471 | 471 | 471 | 471 | 471 | 471 | 471 | . 23 Windows | False | 276437 | 276437 | 276437 | 276437 | 262092 | 262430 | 193998 | 11 | ... | 276437 | 276437 | 276437 | 276436 | 276437 | 276437 | 276437 | 275773 | 276437 | 276437 | . 24 Windows Phone | False | 61 | 61 | 61 | 61 | 59 | 59 | 33 | 61 | ... | 61 | 61 | 61 | 61 | 61 | 61 | 61 | 61 | 61 | 61 | . 25 Windows RT | False | 10 | 10 | 10 | 10 | 10 | 10 | 8 | 10 | ... | 10 | 10 | 10 | 10 | 10 | 10 | 10 | 10 | 10 | 10 | . 26 iOS | False | 32864 | 32864 | 32864 | 32864 | 32561 | 32578 | 25600 | 32863 | ... | 32864 | 32864 | 32864 | 32864 | 32864 | 32864 | 32864 | 32760 | 32864 | 32864 | . 27 iOS | True | 11 | 11 | 11 | 11 | 10 | 10 | 1 | 11 | ... | 11 | 11 | 11 | 11 | 11 | 11 | 11 | 11 | 11 | 11 | . 28 rows × 23 columns . Plotting 2 barplots one for bots detected and one for not bots hover for more info . base=alt.Chart(user_os_df[user_os_df.is_bot==True]).mark_bar().encode( x=&quot;user_os&quot;, y=&quot;count&quot;, tooltip=[&quot;count&quot;] ).properties(title=&quot;BOTS&quot;) base1=alt.Chart(user_os_df[user_os_df.is_bot==False]).mark_bar().encode( x=&quot;user_os&quot;, y=&quot;count&quot;, tooltip=[&quot;count&quot;] ).properties(title=&quot;Not_BOTS&quot;) alt.hconcat(base,base1) . We do the same but instead of user os we count for browser . NOTE: There are some browser names parsed by the parse like _CT_OBJ...etc I do not have much information about them If you have any knowledge or info about them please do comment :D . browser_df=sample_df.groupby([&quot;user_browser&quot;,&quot;is_bot&quot;]).count().reset_index() browser_df.rename(columns={&quot;Unnamed: 0&quot;: &quot;count&quot;},inplace=True) base=alt.Chart(browser_df[browser_df.is_bot==True]).mark_bar().encode( x=&quot;user_browser&quot;, y=&quot;count&quot;, tooltip=[&quot;count&quot;] ).properties(title=&quot;BOTS&quot;) base1=alt.Chart(browser_df[browser_df.is_bot==False]).mark_bar().encode( x=&quot;user_browser&quot;, y=&quot;count&quot;, tooltip=[&quot;count&quot;] ).properties(title=&quot;Not_BOTS&quot;) alt.hconcat(base,base1) . I convert this dataframe to first half csv which I will be planning to use in other notebooks . sample_df.to_csv(&quot;bots_firsthalf.csv&quot;,index=False) . Now the same procedure for the rest of the data . sample_df2=df[400000:] sample_df2.user_agent.dropna(inplace=True) x=sample_df2[&quot;user_agent&quot;].apply(parse_family) sample_df2[&quot;user_browser&quot;]=x sample_df2.user_agent.dropna(inplace=True) sample_df2[&quot;user_os&quot;]=sample_df2.user_agent.apply(parse_os) . /opt/conda/lib/python3.7/site-packages/pandas/core/series.py:4494: SettingWithCopyWarning: A value is trying to be set on a copy of a slice from a DataFrame See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy self._update_inplace(result) /opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:4: SettingWithCopyWarning: A value is trying to be set on a copy of a slice from a DataFrame. Try using .loc[row_indexer,col_indexer] = value instead See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy after removing the cwd from sys.path. /opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:6: SettingWithCopyWarning: A value is trying to be set on a copy of a slice from a DataFrame. Try using .loc[row_indexer,col_indexer] = value instead See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy . sample_df2.user_agent.dropna(inplace=True) sample_df2[&quot;is_bot&quot;]=sample_df2[&quot;user_agent&quot;].apply(parse_is_bot) . /opt/conda/lib/python3.7/site-packages/pandas/core/series.py:4494: SettingWithCopyWarning: A value is trying to be set on a copy of a slice from a DataFrame See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy self._update_inplace(result) /opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:2: SettingWithCopyWarning: A value is trying to be set on a copy of a slice from a DataFrame. Try using .loc[row_indexer,col_indexer] = value instead See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy . The secondHalf of the cvs . sample_df.to_csv(&quot;bots_secondhalf.csv&quot;,index=False) . concatenate both dataframe to the original big dataframe and then save that dataframe as a csv . | . the_big_df=pd.concat([sample_df,sample_df2]) . the_big_df.to_csv(&quot;bots_full.csv&quot;,index=False) . TO BE CONTINUED!!! I WILL WORK ON A NEW NOTEBOOK OTHER THAN THIS STAY TUNED!! . &lt;/div&gt; .",
            "url": "https://omegaji.github.io/DataScienceBlog/2020/06/17/the-beginning-of-bots-detect-eda-and-new-csv.html",
            "relUrl": "/2020/06/17/the-beginning-of-bots-detect-eda-and-new-csv.html",
            "date": " • Jun 17, 2020"
        }
        
    
  
    
        ,"post1": {
            "title": "Temperature Forecasting and Analysis",
            "content": "Welcome to my weather EDA FORECAST . import pandas as pd . df=pd.read_csv(&quot;city_temperature.csv&quot;) . /home/omegaji/anaconda3/lib/python3.7/site-packages/IPython/core/interactiveshell.py:3063: DtypeWarning: Columns (2) have mixed types.Specify dtype option on import or set low_memory=False. interactivity=interactivity, compiler=compiler, result=result) . import altair as alt . df=df[df.AvgTemperature&gt;-99.0] . df_year=df.groupby([&quot;Year&quot;,&quot;Country&quot;,&quot;Region&quot;]).mean().reset_index() list_countries=df_year[&quot;Country&quot;].unique() print(len(list_countries)) . 125 . df_year . Year Country Region Month Day AvgTemperature . 0 1995 | Algeria | Africa | 6.517906 | 15.754821 | 64.410468 | . 1 1995 | Argentina | South/Central America &amp; Carribean | 6.526027 | 15.720548 | 61.701370 | . 2 1995 | Australia | Australia/South Pacific | 6.526027 | 15.720548 | 61.501753 | . 3 1995 | Austria | Europe | 6.526027 | 15.720548 | 50.251233 | . 4 1995 | Bahamas | South/Central America &amp; Carribean | 6.510989 | 15.717033 | 77.110989 | . ... ... | ... | ... | ... | ... | ... | . 3059 2020 | Uruguay | South/Central America &amp; Carribean | 2.738806 | 14.798507 | 68.826119 | . 3060 2020 | Uzbekistan | Asia | 2.738806 | 14.798507 | 51.388060 | . 3061 2020 | Venezuela | South/Central America &amp; Carribean | 2.721805 | 14.812030 | 80.261654 | . 3062 2020 | Vietnam | Asia | 2.738806 | 14.798507 | 70.750746 | . 3063 2020 | Yugoslavia | Europe | 2.738806 | 14.798507 | 46.367164 | . 3064 rows × 6 columns . df_months=df.groupby([&quot;Country&quot;,&quot;Month&quot;,&quot;Year&quot;]).mean().reset_index() . alt.data_transformers.disable_max_rows() . DataTransformerRegistry.enable(&#39;default&#39;) . # A dropdown filter some_countries=df_year.Country.unique() country_dropdown = alt.binding_select(options=some_countries) country_select = alt.selection_single(fields=[&#39;Country&#39;], bind=country_dropdown, name=&quot;Country&quot;, init={&quot;Country&quot;:&quot;India&quot;}) year_dropdown = alt.binding_select(options=df_year.Year.unique()) year_select = alt.selection_single(fields=[&#39;Year&#39;], bind=year_dropdown, name=&quot;Year&quot;, init={&quot;Year&quot;:2000}) base=alt.Chart(df_year).mark_line(point=True).encode( alt.X(&quot;Year:N&quot;), alt.Y(&quot;AvgTemperature&quot;,scale=alt.Scale(zero=False)), tooltip=[&quot;AvgTemperature&quot;,&quot;Country&quot;,&quot;Year&quot;] ) nbase=alt.Chart(df_months).mark_line(point=True).encode( alt.X(&quot;Month:N&quot;,axis=alt.Axis(tickCount=12)), alt.Y(&quot;AvgTemperature&quot;,scale=alt.Scale(zero=False)), tooltip=[&quot;AvgTemperature&quot;,&quot;Country&quot;,&quot;Month&quot;] ) filter_countries = alt.hconcat(base.add_selection( country_select ).transform_filter( country_select ).properties(title=&quot;Dropdown Filtering&quot;),nbase.transform_filter(country_select).add_selection( year_select ).transform_filter(year_select)) filter_countries . #some_countries=[&quot;India&quot;,&quot;Australia&quot;,&quot;Canada&quot;,&quot;China&quot;,&quot;Japan&quot;,&quot;South Africa&quot;,&quot;Bangladesh&quot;,&quot;Brazil&quot;,&quot;North Korea&quot;] base=alt.hconcat() main=alt.vconcat() for i in range(125): if i%25==0 and i!=0: main=alt.vconcat(main,base) base=alt.hconcat() else: temp=alt.Chart(df_year[df_year.Country==some_countries[i]]).mark_line( ).encode( alt.X(&quot;Year:N&quot;,), alt.Y(&quot;AvgTemperature&quot;,scale=alt.Scale(zero=False))).properties(title=some_countries[i]) base=alt.hconcat(base,temp) main . df_total=df_year.groupby([&quot;Year&quot;]).mean().reset_index() df_total alt.Chart(df_total).mark_line().encode(alt.X(&quot;Year&quot;), alt.Y(&quot;AvgTemperature&quot;,scale=alt.Scale(zero=False))).properties(title=&quot;The Avg Temperature of all Regions Over the years&quot;) . df=df.sort_values([&quot;Year&quot;,&quot;Country&quot;,&quot;City&quot;,&quot;Month&quot;,&quot;Day&quot;]) . df=df.drop([&quot;Region&quot;],axis=1) . import numpy as np TempList=df.AvgTemperature.values print(TempList) newList=np.array([64.2]) newList=np.append(newList,TempList) print(newList) newList=newList[:len(TempList)] newList . [64.2 49.4 48.8 ... 72.5 61.5 51.8] [64.2 64.2 49.4 ... 72.5 61.5 51.8] . array([64.2, 64.2, 49.4, ..., 70.7, 72.5, 61.5]) . df[&quot;yTemperature&quot;]=newList . df=df.drop([&quot;State&quot;],axis=1) . from sklearn import preprocessing le = preprocessing.LabelEncoder() Xcity=le.fit_transform(df[&quot;City&quot;]) Xcountry=le.fit_transform(df[&quot;Country&quot;]) . df[&quot;Country&quot;]=Xcountry df[&quot;City&quot;]=Xcity . X_test=df[df.Year==2020] y_test=X_test.pop(&quot;AvgTemperature&quot;) X_train=df[df.Year!=2020] y_train=X_train.pop(&quot;AvgTemperature&quot;) #y=df.pop(&quot;AvgTemperature&quot;) . from xgboost import XGBRegressor model = XGBRegressor(n_estimators = 20 , random_state = 0 , max_depth = 3) model.fit(X_train,y_train) model.score(X_train,y_train) . 0.923897782736382 . model.score(X_test,y_test) . 0.8829977655019752 .",
            "url": "https://omegaji.github.io/DataScienceBlog/2020/06/17/TemperatureForecast.html",
            "relUrl": "/2020/06/17/TemperatureForecast.html",
            "date": " • Jun 17, 2020"
        }
        
    
  
    
        ,"post2": {
            "title": "Udemy_EDA_InDepth",
            "content": "import numpy as np import pandas as pd . df=pd.read_csv(&quot;/kaggle/input/udemy-courses/udemy_courses.csv&quot;) . here we add altair as alt, whats the altair render script? well to render altair on kaggle notebooks(local machine no need) you need to import it, but before that you have to add this as an utility script that&#39;s all!!!&lt;/p&gt; (just click on file and you get the utility script option!!) . &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; import altair as alt import altair_render_script . /kaggle/input/udemy-courses/udemy_courses.csv Define and register the kaggle renderer. Enable with alt.renderers.enable(&#39;kaggle&#39;) . let start with a basic barplot, here we first create a sortdf, what is in this? and what are we doing? first take the columns of only the PAID courses | then we sort the values according to our number of subscribers in descending order and we select the first 20 | now using alt.chart we input of sortdf give the make_bar for the barchart creation and inside encode we give the x and y | x=&gt; course title | y=&gt; the number of subscribers So we are just plotting the top most subscribed course and we also added a toolkit which you can HOVER | &lt;/ul&gt; &lt;/h3&gt; Hover for more info,in all graphs I have added the hover option!! . &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; sortdf=df[df[&quot;is_paid&quot;]==True].sort_values([&quot;num_subscribers&quot;],ascending=False)[:20] alt.Chart(sortdf).mark_bar().encode(alt.X(&quot;course_title&quot;),alt.Y(&quot;num_subscribers&quot;),tooltip=[&quot;course_title&quot;,&quot;num_subscribers&quot;]).properties(width=600) #alt.Chart().mark_bar().encode(x=[1,2,3,4],y=[10,20,30,40]) . Now we convert the datetime given in the column into days,months,year . from datetime import datetime def extractdate(x): return datetime.strptime(x[:10],&quot;%Y-%m-%d&quot;) df[&quot;day&quot;]=df[&quot;published_timestamp&quot;].apply(extractdate) df[&quot;day&quot;]=df[&quot;day&quot;].apply(lambda x: int(x.day)) df[&quot;month&quot;]=df[&quot;published_timestamp&quot;].apply(extractdate) df[&quot;month&quot;]=df[&quot;month&quot;].apply(lambda x: int(x.month)) df[&quot;day_in_year&quot;]=df[&quot;published_timestamp&quot;].apply(extractdate) df[&quot;day_in_year&quot;]=df[&quot;day_in_year&quot;].apply(lambda x: int(x.timetuple().tm_yday)) df[&quot;year&quot;]=df[&quot;published_timestamp&quot;].apply(extractdate) df[&quot;year&quot;]=df[&quot;year&quot;].apply(lambda x: int(x.year)) . Now lets do some more advanced!!&lt;/p&gt; we create a slider using binding_range ,this slider will be for our years present (2011 to 2017) We create a select year selector which will select the particular year the slider is on in the dataframe | &lt;/p&gt; &lt;li&gt;we then create a &lt;b&gt; base chart &lt;/b&gt; with alt&lt;/li&gt; &lt;li&gt; I have created a courses variable which stores the 4 subject category types&lt;/li&gt; &lt;li&gt; we then use the base chart and we select the rows with the first category(source[0]) , group it with months and year and then count the occurences of the course type using count() &lt;/li&gt; &lt;li&gt;we then use the selector and transform filter for filtering out the year&lt;/li&gt; &lt;li&gt; we then create c1 which will be our line plot using mark_line and we use it on base which we have already configured and filtered&lt;/li&gt; &lt;li&gt;we do the same for other courses hence 4 times &lt;/li&gt; &lt;li&gt;now we just concat these using concat for the columns and vconcat(vertical concat) for the rows&lt;/li&gt; . &lt;/ul&gt; &lt;/h3&gt; . Basically the plot is for showing the amount of courses published of a paritcular category of a particular year and per month . &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; courses=df[&quot;subject&quot;].unique() slider2 = alt.binding_range(min=2011, max=2017, step=1) select_year= alt.selection_single(name=&#39;year&#39;, fields=[&#39;year&#39;], bind=slider2, init={&#39;year&#39;: 2016}) base = alt.Chart(df[df[&quot;subject&quot;]==courses[0]].groupby([&quot;month&quot;,&quot;year&quot;]).count().reset_index()).add_selection(select_year).transform_filter(select_year) c1=base.mark_line(point=True).encode(alt.X(&quot;month&quot;), alt.Y(&quot;course_title&quot;,title=&quot;Buisness Finance Courses made&quot;),tooltip=alt.Tooltip([&quot;course_title&quot;],title=&quot;Buisness Finance Courses made&quot;)) base = alt.Chart(df[df[&quot;subject&quot;]==courses[1]].groupby([&quot;month&quot;,&quot;year&quot;]).count().reset_index()).add_selection(select_year).transform_filter(select_year) c2=base.mark_line(point=True).encode(alt.X(&quot;month&quot;), alt.Y(&quot;course_title&quot;,title=&quot;Graphic Design Courses made&quot;),tooltip=alt.Tooltip([&quot;course_title&quot;],title=&quot;Graphic Design Courses made&quot;)) base = alt.Chart(df[df[&quot;subject&quot;]==courses[2]].groupby([&quot;month&quot;,&quot;year&quot;]).count().reset_index()).add_selection(select_year).transform_filter(select_year) c3=base.mark_line(point=True).encode(alt.X(&quot;month&quot;), alt.Y(&quot;course_title&quot;,title=&quot;Musical Instruments Courses made&quot;),tooltip=alt.Tooltip([&quot;course_title&quot;],title=&quot;Musical Instruments Courses made&quot;)) base = alt.Chart(df[df[&quot;subject&quot;]==courses[3]].groupby([&quot;month&quot;,&quot;year&quot;]).count().reset_index()).add_selection(select_year).transform_filter(select_year) c4=base.mark_line(point=True).encode(alt.X(&quot;month&quot;), alt.Y(&quot;course_title&quot;,title=&quot;Web Development Courses made&quot;),tooltip=alt.Tooltip([&quot;course_title&quot;],title=&quot;Web Development Courses made&quot;)) alt.vconcat(alt.concat(c1,c2,spacing=80),alt.concat(c3,c4,spacing=80),spacing=5) . now we create the same selector slider for year . create the base like we did above only we are just filtering the subject thats all | we want to create barplots, which will tell us the price(total) of that particual level in a particular group of a particular year | EXAMPLE: In the year 2017 the 3rd month(March) under the Buisness Finance group the total courses prices was 1795 | . . courses=df[&quot;subject&quot;].unique() slider2 = alt.binding_range(min=2011, max=2017, step=1) select_year= alt.selection_single(name=&#39;year&#39;, fields=[&#39;year&#39;], bind=slider2, init={&#39;year&#39;: 2016}) base = alt.Chart(df[df[&quot;subject&quot;]==courses[0]]).add_selection(select_year).transform_filter(select_year) a=base.mark_bar(size=10).encode( alt.X(&quot;month&quot;),alt.Y(&quot;sum(price)&quot;),color=&quot;level&quot;,tooltip=alt.Tooltip([&quot;sum(price)&quot;],title=&quot;Total Price&quot;)).properties(title=&quot;Buisness Finance Prices over the month&quot;) b=base.mark_bar(size=10).encode( alt.X(&quot;month&quot;),alt.Y(&quot;sum(price)&quot;),color=&quot;level&quot;,tooltip=alt.Tooltip([&quot;sum(price)&quot;],title=&quot;Total Price&quot;)).properties(title=&quot;Graph Design Prices over the month&quot;) c=base.mark_bar(size=10).encode( alt.X(&quot;month&quot;),alt.Y(&quot;sum(price)&quot;),color=&quot;level&quot;,tooltip=alt.Tooltip([&quot;sum(price)&quot;],title=&quot;Total Price&quot;)).properties(title=&quot;Musical Instrument Prices over the month&quot;) d=base.mark_bar(size=10).encode( alt.X(&quot;month&quot;),alt.Y(&quot;sum(price)&quot;),color=&quot;level&quot;,tooltip=alt.Tooltip([&quot;sum(price)&quot;],title=&quot;Total Price&quot;)).properties(title=&quot;Web Development Prices over the month&quot;) alt.vconcat(alt.concat(a,b,spacing=20),alt.concat(c,d,spacing=20),spacing=5) . we should also plot the top subscribers for FREE courses just as we did for paid courses . sortdf=df[df[&quot;is_paid&quot;]==False].sort_values([&quot;num_subscribers&quot;],ascending=False)[:20] alt.Chart(sortdf).mark_bar().encode(alt.X(&quot;course_title&quot;),alt.Y(&quot;num_subscribers&quot;),tooltip=[&quot;course_title&quot;,&quot;num_subscribers&quot;]).properties(width=600) . NOT ENOUGH BARPLOTS :D . we are still gonna plot more :D . over here we simply plot barplots with the number of subscribers for different kinds of levels(all,beginner...) and different kinds of groups(buisness,graphic,...)&lt;/p&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; alt.Chart(df).mark_bar().encode( column=&#39;level&#39;, x=&#39;num_subscribers&#39;, y=&#39;subject&#39; ,tooltip=[&quot;num_subscribers&quot;] ).properties(width=220) . do the same but instead of number of subscribers we have number of lectures!! . alt.Chart(df).mark_bar().encode( column=&quot;level&quot;, x=&#39;num_lectures&#39;, y=&#39;subject&#39; ,tooltip=[&quot;num_lectures&quot;] ).properties(width=220) . NOT ENOUGH BARS NOT ENOUGH SLIDERS!!! :D . Here we use 2 sliders one for year and one for month . create the sliders same way as we did above | create the base chart and add the selection and filter functions for year month respectively | now create a bar chart (we also filter out and keep the paid courses only) for number of subscribers for the 4 subjects over the years and over the months :D | . slider = alt.binding_range(min=1, max=12, step=1) select_month = alt.selection_single(name=&#39;month&#39;, fields=[&#39;month&#39;], bind=slider, init={&#39;month&#39;: 1}) slider2 = alt.binding_range(min=2011, max=2017, step=1) select_year= alt.selection_single(name=&#39;year&#39;, fields=[&#39;year&#39;], bind=slider2, init={&#39;year&#39;: 2016}) base = alt.Chart(df).add_selection(select_year,select_month).transform_filter(select_year).transform_filter( select_month ) left = base.transform_filter(alt.datum.is_paid==True).encode( y=alt.Y(&#39;subject&#39;), x=alt.X(&#39;sum(num_subscribers)&#39;, title=&#39;NumOfSubscribers&#39;) ,tooltip=[&quot;sum(num_subscribers)&quot;,&quot;subject&quot;]).mark_bar(size=20).properties(title=&#39;subscribers Over the month for PAID&#39;,height=200) left . we do the same only for non paid(free) courses . right = base.transform_filter(alt.datum.is_paid==False).encode( y=alt.Y(&#39;subject&#39;), x=alt.X(&#39;sum(num_subscribers)&#39;),tooltip=[&quot;sum(num_subscribers)&quot;,&quot;subject&quot;]).mark_bar(size=20).properties(title=&#39;subscribers Over the month for FREE&#39;,height=200) right . Now lets do something with the number of lectures, we create a bar chart with bins(20 is the size(step=21)) for the number of lectures , the y axis is the count of the subjects present in this number of lecture bin,the color depicts the subject notation, also the white strips use is because I added num_lectures to toolkit list, hence it also shows the specific number of lectures that particular strip has . alt.Chart(df).mark_bar(size=10).encode( alt.X(&quot;num_lectures:Q&quot;, bin=alt.Bin(step=21)), alt.Y(&quot;count(subject)&quot;,title=&quot;subject count &quot;), row=&#39;level&#39;,color=&#39;subject&#39;,tooltip=[&quot;count(subject)&quot;,&quot;subject&quot;,&quot;num_lectures&quot;] ).properties(width=700) . we do the same for our course content duration!!! . alt.Chart(df).mark_bar(size=13).encode( alt.X(&quot;content_duration:Q&quot;, bin=alt.Bin(step=2)), alt.Y(&quot;count(subject)&quot;,title=&quot;subject count &quot;), row=&#39;level&#39;,color=&#39;subject&#39;,tooltip=[&quot;count(subject)&quot;,&quot;subject&quot;,&quot;content_duration&quot;] ).properties(width=700) . &lt;/div&gt; . | .",
            "url": "https://omegaji.github.io/DataScienceBlog/2020/05/25/Udemy_EDA.html",
            "relUrl": "/2020/05/25/Udemy_EDA.html",
            "date": " • May 25, 2020"
        }
        
    
  
    
        ,"post3": {
            "title": "COVID-19 RESEARCH PPRINT DATA EDA",
            "content": "import pandas as pd df=pd.read_csv(&quot;COVID-19-Preprint-Data_ver2.csv&quot;) . Welcome to this notebook, Here lets get a glimpse of the data and list the columns . print(df.columns) df.head() . Index([&#39;DOI&#39;, &#39;Date of Upload&#39;, &#39;Title of preprint&#39;, &#39;Preprint Link&#39;, &#39;Abstract&#39;, &#39;Number of Authors&#39;, &#39;Authors&#39;, &#39;Author(s) Institutions&#39;, &#39;Uploaded Site&#39;], dtype=&#39;object&#39;) . DOI Date of Upload Title of preprint Preprint Link Abstract Number of Authors Authors Author(s) Institutions Uploaded Site . 0 10.1101/2020.05.13.20088732 | 2020-05-16 | MRI of the lungs in patients with COVID-19: cl... | http://medrxiv.org/cgi/content/short/2020.05.1... | Objective: To evaluate an applicability of lun... | 16 | [&#39;Yuriy Vasilev&#39;, &#39;Kristina Sergunova&#39;, &#39;Alexa... | {&quot;Research and Practical Clinical Center for D... | medrxiv | . 1 10.1101/2020.05.15.098616 | 2020-05-15 | SARS-CoV2 (COVID-19) Structural/Evolution Dyna... | http://biorxiv.org/cgi/content/short/2020.05.1... | The SARS-CoV-2 pandemic, starting in 2019, has... | 22 | [&#39;Ruchir Gupta&#39;, &#39;Jacob Charron&#39;, &#39;Cynthia Ste... | {&quot;Michigan State University&quot;: 9, &quot;University o... | biorxiv | . 2 10.1101/2020.05.10.20069732 | 2020-05-15 | COVID-19 and Environmental factors. A PRISMA-c... | http://medrxiv.org/cgi/content/short/2020.05.1... | The emergence of a novel human coronavirus, SA... | 3 | [&#39;Apostolos Vantarakis&#39;, &#39;Ioanna Chatziprodrom... | {&quot;University of Patras&quot;: 2, &quot;Department of Phy... | medrxiv | . 3 10.1101/2020.05.14.097311 | 2020-05-15 | Analytical and Clinical Comparison of Three Nu... | http://biorxiv.org/cgi/content/short/2020.05.1... | Severe Acute Respiratory Syndrome Coronavirus ... | 6 | [&#39;Elizabeth Smith&#39;, &#39;Wei Zhen&#39;, &#39;Ryhana Manji&#39;... | {&quot;Northwell Health Laboratories&quot;: 5, &quot;Northwel... | biorxiv | . 4 10.1101/2020.05.14.093054 | 2020-05-15 | A single dose SARS-CoV-2 simulating particle v... | http://biorxiv.org/cgi/content/short/2020.05.1... | Coronavirus disease 2019 (COVID-19) is caused ... | 17 | [&#39;Yujia Cai&#39;, &#39;Di Yin&#39;, &#39;Sikai Ling&#39;, &#39;Xiaolon... | {&quot;Shanghai Jiao Tong University&quot;: 11, &quot;Fudan U... | biorxiv | . Ah of course the first thing I always would do is to split the Datetime into day,month,year and day in year(like 1st febuary is the 32nd day of the year , this helps in plotting :D) . from datetime import datetime df[&quot;day&quot;]=df[&quot;Date of Upload&quot;].apply(lambda x: int(datetime.strptime(x,&#39;%Y-%m-%d&#39;).day)) df[&quot;month&quot;]=df[&quot;Date of Upload&quot;].apply(lambda x:int( datetime.strptime(x,&#39;%Y-%m-%d&#39;).month)) df[&quot;year&quot;]=df[&quot;Date of Upload&quot;].apply(lambda x: int( datetime.strptime(x,&#39;%Y-%m-%d&#39;).year)) df[&quot;day_in_year&quot;]=df[&quot;Date of Upload&quot;].apply(lambda x:int( datetime.strptime(x,&#39;%Y-%m-%d&#39;).timetuple().tm_yday)) . I am going to drop the columns below as I do not aim to use it in my EDA . df.drop([&quot;Preprint Link&quot;,&quot;DOI&quot;,&quot;Date of Upload&quot;],axis=1,inplace=True) . Our first plot in the notebook is a simple one and I have done it using Altair, here I plot the number of abstracts(or generally papers ) submitted per day -X= day in the year | -Y= the count of the number of abstracts(research papers) published in that particular day | . the below plot is a bubble plot which increases its size as the number of abstracts per day, you can hover for more info . &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; import altair as alt alt.Chart(df.groupby([&quot;day_in_year&quot;]).count().reset_index()).mark_point().encode( x=&#39;day_in_year&#39;, y=&#39;Abstract&#39;, tooltip=[&quot;Abstract&quot;,&quot;day_in_year&quot;], size=&quot;Abstract&quot; ).interactive() . df[df[&quot;day_in_year&quot;]==137] . Title of preprint Abstract Number of Authors Authors Author(s) Institutions Uploaded Site day month year day_in_year . 0 MRI of the lungs in patients with COVID-19: cl... | Objective: To evaluate an applicability of lun... | 16 | [&#39;Yuriy Vasilev&#39;, &#39;Kristina Sergunova&#39;, &#39;Alexa... | {&quot;Research and Practical Clinical Center for D... | medrxiv | 16 | 5 | 2020 | 137 | . we can see above that after a certain day (about 137th day of the year) which is 16th May 2020 . Below we are going to plot the number of abstracts(research papers) published in the given months of 2020, we first filter out only months of 2020 . alt.Chart(df[df[&quot;year&quot;]==2020].groupby([&quot;month&quot;]).count().reset_index()).mark_area( line={&#39;color&#39;:&#39;darkblue&#39;}, color=alt.Gradient( gradient=&#39;linear&#39;, stops=[alt.GradientStop(color=&#39;white&#39;, offset=0), alt.GradientStop(color=&#39;blue&#39;, offset=1)], x1=1, x2=1, y1=1, y2=0 ) ).encode( alt.X(&#39;month&#39;), alt.Y(&#39;Abstract&#39;,title=&quot;Abstract Count published&quot;), tooltip=[&quot;month&quot;,&quot;Abstract&quot;] ).interactive() . Over here we will import nltk stopwords and remove the stopwords from the abstract and I have made a function for it, next I have done is as follows: I creat a function(get_top_n_words) for doing all in one | it first removes stopwords using Countvectorerizer | our function which takes n as input is the top n word frequencies we want | using vecotor transform and bag of words we get the frequencies and the words of the whole abstract column which we will input as the agrument corpus | we then sort it according to the most number of frequencies and return the top n words and their frequencies in a list of tuples | . . import nltk from nltk.corpus import stopwords from nltk.tokenize import word_tokenize stop_words=set(stopwords.words(&#39;english&#39;)) def removeSW(x): x=x.lower() word_tokens = word_tokenize(x) filtered_sentence = [w for w in word_tokens if not w in stop_words] return &quot; &quot;.join(filtered_sentence) df[&quot;Abstract&quot;]=df[&quot;Abstract&quot;].apply(removeSW) from sklearn.feature_extraction.text import CountVectorizer def get_top_n_words(corpus, n=None): vec = CountVectorizer(stop_words = &#39;english&#39;).fit(corpus) bag_of_words = vec.transform(corpus) sum_words = bag_of_words.sum(axis=0) words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()] words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True) return words_freq[:n] . Lets get our unigrams of the abstract column and the title column using this function . unigrams=get_top_n_words(df[&quot;Abstract&quot;],20) unigrams_title=get_top_n_words(df[&quot;Title of preprint&quot;],20) . unigrams . [(&#39;19&#39;, 9637), (&#39;covid&#39;, 9419), (&#39;sars&#39;, 5431), (&#39;cov&#39;, 5304), (&#39;patients&#39;, 4950), (&#39;cases&#39;, 3385), (&#39;data&#39;, 2727), (&#39;disease&#39;, 2597), (&#39;infection&#39;, 2334), (&#39;model&#39;, 2211), (&#39;coronavirus&#39;, 2189), (&#39;results&#39;, 2181), (&#39;number&#39;, 2127), (&#39;study&#39;, 1979), (&#39;2020&#39;, 1972), (&#39;2019&#39;, 1848), (&#39;health&#39;, 1808), (&#39;pandemic&#39;, 1797), (&#39;time&#39;, 1698), (&#39;epidemic&#39;, 1679)] . unigrams_title . [(&#39;19&#39;, 2135), (&#39;covid&#39;, 2128), (&#39;sars&#39;, 945), (&#39;cov&#39;, 898), (&#39;coronavirus&#39;, 491), (&#39;patients&#39;, 397), (&#39;2019&#39;, 359), (&#39;analysis&#39;, 345), (&#39;study&#39;, 272), (&#39;pandemic&#39;, 269), (&#39;infection&#39;, 267), (&#39;china&#39;, 264), (&#39;disease&#39;, 257), (&#39;novel&#39;, 256), (&#39;epidemic&#39;, 242), (&#39;clinical&#39;, 227), (&#39;model&#39;, 226), (&#39;based&#39;, 214), (&#39;outbreak&#39;, 209), (&#39;using&#39;, 198)] . Yes! I know both the list of tuples looks very much same since many of the title and abstract words are column, now lets get down in making a seperate dataframe and join this 2 list of tuples as rows and their values as column ,also a column for type indicates that wether it is abstract or title . d={&quot;word&quot;:[],&quot;count&quot;:[],&quot;type&quot;:[]} for i in unigrams: d[&quot;word&quot;].append(i[0]) d[&quot;count&quot;].append(i[1]) d[&quot;type&quot;].append(&quot;Abstract&quot;) for i in unigrams_title: d[&quot;word&quot;].append(i[0]) d[&quot;count&quot;].append(i[1]) d[&quot;type&quot;].append(&quot;Title&quot;) count_df=pd.DataFrame(d) . We plot!! . source = count_df alt.Chart(source).mark_bar().encode( tooltip=[&quot;word&quot;,&quot;count&quot;], column=&#39;type&#39;, x=&#39;word&#39;, y=&#39;count&#39;, color=&#39;type&#39; ) . Now we modify the function before and create a new function called get_top_gram It takes one more argument called grams, which will be the number of grams we want, bigram trigram etc and after that I will plot the bigram as similarly as the unigrams, and then I plot the pentagrams!!&lt;/p&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; def get_top_gram(corpus,grams, n=None): vec = CountVectorizer(ngram_range=grams,stop_words = &#39;english&#39;).fit(corpus) bag_of_words = vec.transform(corpus) sum_words = bag_of_words.sum(axis=0) words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()] words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True) return words_freq[:n] . bigrams=get_top_gram(df[&quot;Abstract&quot;],(2,2),20) bigrams_title=get_top_gram(df[&quot;Title of preprint&quot;],(2,2),20) d={&quot;word&quot;:[],&quot;count&quot;:[],&quot;type&quot;:[]} for i in bigrams: d[&quot;word&quot;].append(i[0]) d[&quot;count&quot;].append(i[1]) d[&quot;type&quot;].append(&quot;Abstract&quot;) for i in bigrams_title: d[&quot;word&quot;].append(i[0]) d[&quot;count&quot;].append(i[1]) d[&quot;type&quot;].append(&quot;Title&quot;) count_df=pd.DataFrame(d) source = count_df alt.Chart(source).mark_bar().encode( tooltip=[&quot;word&quot;,&quot;count&quot;], column=&#39;type&#39;, x=&#39;word&#39;, y=&#39;count&#39;, color=&#39;type&#39; ) . fivegrams=get_top_gram(df[&quot;Abstract&quot;],(5,5),20) fivegrams_title=get_top_gram(df[&quot;Title of preprint&quot;],(5,5),20) d={&quot;word&quot;:[],&quot;count&quot;:[],&quot;type&quot;:[]} for i in fivegrams: d[&quot;word&quot;].append(i[0]) d[&quot;count&quot;].append(i[1]) d[&quot;type&quot;].append(&quot;Abstract&quot;) for i in fivegrams_title: d[&quot;word&quot;].append(i[0]) d[&quot;count&quot;].append(i[1]) d[&quot;type&quot;].append(&quot;Title&quot;) count_df=pd.DataFrame(d) source = count_df alt.Chart(source).mark_bar().encode( tooltip=[&quot;word&quot;,&quot;count&quot;], column=&#39;type&#39;, x=&#39;word&#39;, y=&#39;count&#39;, color=&#39;type&#39; ) . Now lets get onto the authors and the Universities!! . Here I use json to extract the dictionary present in the column Author(s) Institutions . after that I show the length of the set and list created, basically the set contains the number of unique Institutions and the list contains everytime the Institution is mentioned in the dataframe, we will use it ahead . import json myset=set() mylist=[] for i in df[&quot;Author(s) Institutions&quot;].index: l=set(json.loads(df.loc[i,&quot;Author(s) Institutions&quot;]).keys()) for j in l: myset.add(j) mylist.append(j) . len(myset) . 9134 . len(mylist) . 12343 . Another function phew!!!, well this is quite simple we use the list and count how many times each intistute has appeared and then a dictionary example ({&quot;university&quot;:10}) . basically what we want to do is that the frequency represents how many times the institution has published a paper since its occurence in the dataframe in each row is a count for its publications!!! . def CountFrequency(my_list): count = {} for i in my_list: count[i] = count.get(i, 0) + 1 return count frequency=CountFrequency(mylist) d={&quot;UNI&quot;:[],&quot;Publishes&quot;:[]} for i in frequency.keys(): d[&quot;UNI&quot;].append(i) d[&quot;Publishes&quot;].append(frequency[i]) uni_df=pd.DataFrame(d) uni_df=uni_df.sort_values(by=[&quot;Publishes&quot;],ascending=False) for i in uni_df.index: if len(uni_df.loc[i].UNI)&lt;4: uni_df.drop(i,axis=0,inplace=True) . source = uni_df.iloc[:50,:] alt.Chart(source).mark_bar().encode( x=&#39;Publishes&#39;, y=&quot;UNI&quot;, tooltip=[&quot;Publishes&quot;] ).properties(height=700) . Voila! the graph above shows how many publications done by the institutions(here I plotted only the top 50 )HOVER!!! and we find out Oxford University has published 61 times!!! . Now, lets check for the authors , below I used the previous dictionary of frequencies we outputed and change the values to list of values example {&quot;uni&quot;:1}===&gt;{&quot;uni&quot;:[1,0]} . here we add the 0 to all as we are going to store the authors count for this value!!! . . for i in frequency.keys(): frequency[i]=[frequency[i],0] . See the name of the institue is given and also the value with it are the authors number! which we did not consider above , well now we are!! . df[&quot;Author(s) Institutions&quot;].head() . 0 {&#34;Research and Practical Clinical Center for D... 1 {&#34;Michigan State University&#34;: 9, &#34;University o... 2 {&#34;University of Patras&#34;: 2, &#34;Department of Phy... 3 {&#34;Northwell Health Laboratories&#34;: 5, &#34;Northwel... 4 {&#34;Shanghai Jiao Tong University&#34;: 11, &#34;Fudan U... Name: Author(s) Institutions, dtype: object . We now again use json to get the values of author institution column but we will extract the names of the Uni as a key for our dictionary and will upadte the auhors value to it . mylist=[] for i in df[&quot;Author(s) Institutions&quot;].index: l=set(json.loads(df.loc[i,&quot;Author(s) Institutions&quot;]).keys()) for j in l: frequency[j][1]=frequency[j][1]+json.loads(df.loc[i,&quot;Author(s) Institutions&quot;])[j] . d={&quot;UNI&quot;:[],&quot;AuthorCount&quot;:[]} for i in frequency.keys(): d[&quot;UNI&quot;].append(i) d[&quot;AuthorCount&quot;].append(frequency[i][1]) uni_df=pd.DataFrame(d) uni_df=uni_df.sort_values(by=[&quot;AuthorCount&quot;],ascending=False) for i in uni_df.index: if len(uni_df.loc[i].UNI)&lt;4: uni_df.drop(i,axis=0,inplace=True) . We did the same procdeure and created a new dataframe for university and their authors count! . uni_df . UNI AuthorCount . 1402 Icahn School of Medicine at Mount Sinai | 236 | . 450 University of Oxford | 183 | . 2716 Centers for Disease Control and Prevention | 180 | . 784 Imperial College London | 172 | . 485 Stanford University | 160 | . ... ... | ... | . 5354 National Institute of Advanced Industrial Scie... | 1 | . 5353 National Institute for Infectious Diseases | 1 | . 5352 Nagahama Institute of Bioscience and Technology | 1 | . 2214 6School of Medicine, Shiraz University of Medi... | 1 | . 9133 Loyola University of Chicago Stritch School of... | 1 | . 9107 rows × 2 columns . Finally we plot it!!, and hovering over the data we can see Icahn School of Medicine has the highest amount of authors who have published for covid-19 around 470 . source = uni_df.iloc[:50,:] alt.Chart(source).mark_bar().encode( x=&#39;AuthorCount&#39;, y=&quot;UNI&quot;, tooltip=[&quot;AuthorCount&quot;,&quot;UNI&quot;] ).properties(height=700) . Thank You! . &lt;/div&gt; .",
            "url": "https://omegaji.github.io/DataScienceBlog/2020/05/23/copy_preprint.html",
            "relUrl": "/2020/05/23/copy_preprint.html",
            "date": " • May 23, 2020"
        }
        
    
  
    
        ,"post4": {
            "title": "COVID-19",
            "content": "Hello welcome to my notebook on Covid DataAnalysis&lt;/p&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; import pandas as pd import numpy as np train_df=pd.read_csv(&quot;train.csv&quot;) train_df=train_df.drop(&quot;Province_State&quot;,axis=1) from datetime import datetime train_df[&quot;Date&quot;]=train_df[&#39;Date&#39;].apply(lambda x:datetime.strptime(x, &#39;%Y-%m-%d&#39;)) train_df[&quot;week&quot;]=&quot;week_&quot;+ str(train_df[&quot;Date&quot;].dt.week) train_df[&quot;week&quot;]=train_df[&quot;Date&quot;].dt.week.apply(lambda x: x) train_df[&quot;day&quot;]=train_df[&quot;Date&quot;].dt.day.apply(lambda x: x) . Above we used the data column to generate days and week number for out dataset starting from week 1 to week 17 . &lt;/p&gt; now below i will print the dataframe . &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; train_df.head() . Id Country_Region Date ConfirmedCases Fatalities week day . 0 1 | Afghanistan | 2020-01-22 | 0.0 | 0.0 | 4 | 22 | . 1 2 | Afghanistan | 2020-01-23 | 0.0 | 0.0 | 4 | 23 | . 2 3 | Afghanistan | 2020-01-24 | 0.0 | 0.0 | 4 | 24 | . 3 4 | Afghanistan | 2020-01-25 | 0.0 | 0.0 | 4 | 25 | . 4 5 | Afghanistan | 2020-01-26 | 0.0 | 0.0 | 4 | 26 | . from sklearn.preprocessing import LabelEncoder,StandardScaler,MinMaxScaler from sklearn.model_selection import train_test_split . now below we do the necessary processing of our data to make it more viable for our Scaling and model preparation . train_df[&quot;month&quot;]=train_df[&quot;Date&quot;].dt.month.apply(lambda x: int(x)) . train_df[&#39;ConfirmedCases&#39;] = train_df[&#39;ConfirmedCases&#39;].apply(int) train_df[&#39;Fatalities&#39;] = train_df[&#39;Fatalities&#39;].apply(int) . now we will take out the data related to our country India in a seperate dataframe . india_df=train_df[train_df[&quot;Country_Region&quot;]==&quot;India&quot;] india_df.head() . Id Country_Region Date ConfirmedCases Fatalities week day month . 13160 15961 | India | 2020-01-22 | 0 | 0 | 4 | 22 | 1 | . 13161 15962 | India | 2020-01-23 | 0 | 0 | 4 | 23 | 1 | . 13162 15963 | India | 2020-01-24 | 0 | 0 | 4 | 24 | 1 | . 13163 15964 | India | 2020-01-25 | 0 | 0 | 4 | 25 | 1 | . 13164 15965 | India | 2020-01-26 | 0 | 0 | 4 | 26 | 1 | . import matplotlib.pyplot as plt #print(plt.style.available) plt.style.use(&#39;seaborn-darkgrid&#39;) . india_df.groupby([&quot;week&quot;]).max().reset_index().plot(kind=&quot;bar&quot;,x=&quot;week&quot;,y=&quot;ConfirmedCases&quot;) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7f43c0b401d0&gt; . india_df.groupby([&quot;week&quot;]).max().reset_index().plot(kind=&quot;bar&quot;,x=&quot;week&quot;,y=&quot;Fatalities&quot;) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7f43c1e07e90&gt; . The proceeses done are seperate the confirmed cases to a seperate variable called case(Y variable) | we use the LabelEncoder() instead of one hot encoding to encode the country regions | we then use the fit_transform function to label encode | We then use MinMaxScaler for normalizing the values as there are various values from 0 to more than 1000 | &lt;/p&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; cases = train_df.ConfirmedCases fatal=train_df.Fatalities lb = LabelEncoder() del train_df[&quot;Date&quot;] del train_df[&quot;Fatalities&quot;] del train_df[&quot;ConfirmedCases&quot;] #del train_df[&quot;Id&quot;] train_df[&#39;Country_Region&#39;] = lb.fit_transform(train_df[&#39;Country_Region&#39;]) scaler = MinMaxScaler() x_train = scaler.fit_transform(train_df.values) X_train, X_test, y_train, y_test = train_test_split(train_df, cases, test_size=0.2, random_state=0) X_train = scaler.fit_transform(X_train.values) X_test = scaler.fit_transform(X_test.values) . Here We can see we Check on XGBoost Regressor with below hyperparameters . from xgboost import XGBRegressor model = XGBRegressor(n_estimators = 2500 , random_state = 0 , max_depth = 27) model.fit(X_train,y_train) . XGBRegressor(base_score=0.5, booster=None, colsample_bylevel=1, colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1, importance_type=&#39;gain&#39;, interaction_constraints=None, learning_rate=0.300000012, max_delta_step=0, max_depth=27, min_child_weight=1, missing=nan, monotone_constraints=None, n_estimators=2500, n_jobs=0, num_parallel_tree=1, objective=&#39;reg:squarederror&#39;, random_state=0, reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1, tree_method=None, validate_parameters=False, verbosity=None) . model.score(X_train,y_train) . 0.99999999999996 . Here we plot the number of cases in india month wise(1,1.5..4(april))&lt;/p&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; import seaborn as sns ig=india_df.groupby([&quot;month&quot;]).max().reset_index() sns.lineplot(ig[&quot;month&quot;],ig[&quot;ConfirmedCases&quot;],ci=None) ig[&quot;month&quot;] . 0 1 1 2 2 3 3 4 Name: month, dtype: int64 . below we predict the values using our X_test,y_test we created . Next we also plot the predicted values and acutal values of the model in a scatter plot and as we can see the points form more of a X=Y line(meaning that the predicted and actual values are almost equal) . print(&quot;the score for the test dataaset is &quot;+str(model.score(X_test,y_test))) predicted=model.predict(X_test) import seaborn as sns import matplotlib.pyplot as plt #sns.lineplot(y_train) plt.xlabel(&quot;Predicted Value&quot;) plt.ylabel(&quot;Actual Value&quot;) plt.title(&quot;the graph is similar to x=y curve&quot;) plt.scatter(predicted,y_test) . the score for the test dataaset is 0.991170270240357 . &lt;matplotlib.collections.PathCollection at 0x7f43c1eee590&gt; . below we predict the values using our X_test,y_test we created Next we also plot the predicted values and acutal values of the model in a scatter plot and as we can see the points form more of a X=Y line(meaning that the predicted and actual values are almost equal) . train_predict=model.predict(X_train) plt.xlabel(&quot;Predicted Value(training)&quot;) plt.ylabel(&quot;Actual Value(traiing)&quot;) plt.title(&quot;the graph is similar to x=y curve&quot;) plt.scatter(train_predict,y_train) . &lt;matplotlib.collections.PathCollection at 0x7f43c1f4e490&gt; . from sklearn.metrics import explained_variance_score y_pred=model.predict(X_test) . Now we Calculate 3 types of errors, explained variance score, max error score,mean squared error&lt;/p&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; from sklearn.metrics import explained_variance_score from sklearn.metrics import max_error from sklearn.metrics import mean_squared_error print(&quot;The expalined variance score is==&gt;&quot;+ str(explained_variance_score(y_test,y_pred))) print(&quot;The max error score is ==&gt;&quot;+str(max_error(y_test,y_pred))) print(&quot;The mean squared error is ==&gt;&quot;+str(mean_squared_error(y_test,y_pred))) . The expalined variance score is==&gt;0.9912504663704602 The max error score is ==&gt;24370.158203125 The mean squared error is ==&gt;1096115.2159031187 . pred_df=pd.DataFrame() pred_df[&quot;Predictions&quot;]=pd.Series(np.around(y_pred,decimals=0)) pred_df[&quot;True_values&quot;]=pd.Series(y_test.values) . Here we form a dataframe consisting predicted data and true data&lt;/p&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; pred_df . Predictions True_values . 0 0.0 | 0 | . 1 417.0 | 447 | . 2 2.0 | 0 | . 3 319.0 | 318 | . 4 0.0 | 0 | . ... ... | ... | . 5880 -0.0 | 0 | . 5881 245.0 | 245 | . 5882 -0.0 | 0 | . 5883 0.0 | 0 | . 5884 -0.0 | 0 | . 5885 rows × 2 columns . here we use the STACKING ALGORITHIM we choose 3 algorithims XGBoost(the one above we used) | RandomForest regressor | SVM Regressor | &lt;/p&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; from sklearn.ensemble import RandomForestRegressor from sklearn import svm rf = RandomForestRegressor(n_estimators = 500, random_state = 42) models = [ svm.SVR(), RandomForestRegressor(random_state=0, n_jobs=-1, n_estimators=500, max_depth=3), model ] . !pip install vecstack . Requirement already satisfied: vecstack in /home/omegaji/anaconda3/lib/python3.7/site-packages (0.4.0) Requirement already satisfied: numpy in /home/omegaji/anaconda3/lib/python3.7/site-packages (from vecstack) (1.18.1) Requirement already satisfied: scipy in /home/omegaji/anaconda3/lib/python3.7/site-packages (from vecstack) (1.4.1) Requirement already satisfied: scikit-learn&gt;=0.18 in /home/omegaji/anaconda3/lib/python3.7/site-packages (from vecstack) (0.22.1) Requirement already satisfied: joblib&gt;=0.11 in /home/omegaji/anaconda3/lib/python3.7/site-packages (from scikit-learn&gt;=0.18-&gt;vecstack) (0.14.1) . from vecstack import stacking from sklearn.metrics import r2_score S_train, S_test = stacking(models, X_train, y_train, X_test, regression=True, mode=&#39;oof_pred_bag&#39;, needs_proba=False, save_dir=None, metric=r2_score, n_folds=5, #stratified=True, shuffle=True, random_state=0, verbose=2) . task: [regression] metric: [r2_score] mode: [oof_pred_bag] n_models: [3] model 0: [SVR] fold 0: [-0.01849384] fold 1: [-0.02135447] fold 2: [-0.01936172] fold 3: [-0.01934595] fold 4: [-0.01877062] - MEAN: [-0.01946532] + [0.00100202] FULL: [-0.01898844] model 1: [RandomForestRegressor] fold 0: [0.16408069] fold 1: [0.23211451] fold 2: [0.11892497] fold 3: [0.13180512] fold 4: [0.14895139] - MEAN: [0.15917534] + [0.03954222] FULL: [0.15628386] model 2: [XGBRegressor] fold 0: [0.98925196] fold 1: [0.99284200] fold 2: [0.99307733] fold 3: [0.99309471] fold 4: [0.99233834] - MEAN: [0.99212087] + [0.00146021] FULL: [0.99187249] . final_model = model.fit(S_train, y_train) y_pred = final_model.predict(S_test) . the training score For the Stacked Model is predicted below . final_model.score(S_train,y_train) . 0.9999999999999664 . The testing score is predicted below . final_model.score(S_test,y_test) . 0.9869640680032792 . Similary we plot the predicted data vs actual confirmed cases for both training and testing accuracy as shown bellow . . we can see the plot still resembles and x=y curve hence a good prediction value close to the actual value . plt.xlabel(&quot;Predicted Value(training)&quot;) train_pred=model.predict(S_train) plt.ylabel(&quot;Actual Value(training)&quot;) plt.title(&quot;the graph is similar to x=y curve&quot;) plt.scatter(train_pred,y_train) . &lt;matplotlib.collections.PathCollection at 0x7f43c1f4ead0&gt; . plt.xlabel(&quot;Predicted Value(testing)&quot;) train_pred=model.predict(S_train) plt.ylabel(&quot;Actual Value(testing)&quot;) plt.title(&quot;the graph is similar to x=y curve&quot;) plt.scatter(y_pred,y_test) . &lt;matplotlib.collections.PathCollection at 0x7f43c2a34550&gt; . Now we test each mode individually on how it scores and out best model is the Xgboost which we tested earlier and also the curve was very similar to x=y We start with using SVM Regressor which is the 0th model in our MODELS list (hence below code contains model[0]) . . print(&quot;USING SVM REGRESSOR MODEL &quot;) models[0].fit(S_train,y_train) print(&quot;the training score is&quot;) print(models[0].score(S_train,y_train)) print(&quot;the testing score is&quot;) print(models[0].score(S_test,y_test)) . USING SVM REGRESSOR MODEL the training score is -0.0017980567557247529 the testing score is 0.001936770722922643 . we can see the training accuracy and testing accuracy are not very efficient and cannot even be consider a good model it is a bad model . following code plots the models training predictions to actual values and testing predictions to actual test values . train_pred=models[0].predict(S_train) plt.xlabel(&quot;Predicted Value(training)&quot;) plt.ylabel(&quot;Actual Value(training)&quot;) plt.title(&quot;the graph is NOT an x=y curve at any level &quot;) plt.scatter(train_pred,y_train) . &lt;matplotlib.collections.PathCollection at 0x7f43c287a410&gt; . plt.xlabel(&quot;Predicted Value(testing)&quot;) train_pred=models[0].predict(S_test) plt.ylabel(&quot;Actual Value(testing)&quot;) plt.title(&quot;the graph is also NOT X=y&quot;) plt.scatter(train_pred,y_test) . &lt;matplotlib.collections.PathCollection at 0x7f43c33a2c50&gt; . NOW WE DO THE SAME FOR OUR RANDOM FOREST MODEL WHICH SHOWS A GOOD SCORE TOO&lt;/p&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; print(&quot;USING RANDOM FOREST MODEL &quot;) models[1].fit(S_train,y_train) print(&quot;the training score is&quot;) print(models[1].score(S_train,y_train)) print(&quot;the testing score is&quot;) print(models[1].score(S_test,y_test)) print(models[1]) . USING RANDOM FOREST MODEL the training score is 0.9850564601052662 the testing score is 0.9839541082692184 RandomForestRegressor(bootstrap=True, ccp_alpha=0.0, criterion=&#39;mse&#39;, max_depth=3, max_features=&#39;auto&#39;, max_leaf_nodes=None, max_samples=None, min_impurity_decrease=0.0, min_impurity_split=None, min_samples_leaf=1, min_samples_split=2, min_weight_fraction_leaf=0.0, n_estimators=500, n_jobs=-1, oob_score=False, random_state=0, verbose=0, warm_start=False) . train_pred=models[1].predict(S_train) plt.xlabel(&quot;Predicted Value(training)&quot;) plt.ylabel(&quot;Actual Value(training)&quot;) plt.title(&quot;the graph is NOT an x=y curve at any level&quot; ) plt.scatter(train_pred,y_train) . &lt;matplotlib.collections.PathCollection at 0x7f43e08e5150&gt; . plt.xlabel(&quot;Predicted Value(testing)&quot;) train_pred=models[1].predict(S_test) plt.ylabel(&quot;Actual Value(testing)&quot;) plt.title(&quot;the graph is also NOT X=y&quot;) plt.scatter(train_pred,y_test) . &lt;matplotlib.collections.PathCollection at 0x7f43c1dfee10&gt; . THANK You!!&lt;/p&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; # from sklearn import neighbors # knn = neighbors.KNeighborsRegressor(1000, weights=&quot;uniform&quot;) # from sklearn.linear_model import ElasticNet # en=ElasticNet(random_state=0) # models = [ # knn, # RandomForestRegressor(random_state=0, n_jobs=-1, # n_estimators=1000, max_depth=3), # model, # en # ] # S_train, S_test = stacking(models, # X_train, y_train[&quot;ConfirmedCases&quot;], X_test, # regression=True, # mode=&#39;oof_pred_bag&#39;, # needs_proba=False, # save_dir=None, # metric=r2_score, # shuffle=True, # random_state=0, # verbose=2) # final_model_2 = model.fit(S_train, y_train[&quot;ConfirmedCases&quot;]) # y_pred = final_model_2.predict(S_test) # final_model.score(S_train,y_train[&quot;ConfirmedCases&quot;]) . &lt;/div&gt; . . . .",
            "url": "https://omegaji.github.io/DataScienceBlog/2020/05/22/try_train.html",
            "relUrl": "/2020/05/22/try_train.html",
            "date": " • May 22, 2020"
        }
        
    
  
    
        ,"post5": {
            "title": "Title",
            "content": "#&quot;Covid-19 India analysis and prediction&quot; &gt; &quot;Covid-19 India analysis and prediction&quot; - toc: false - branch: master - badges: true - comments: true - categories: [fastpages, jupyter] - image: images/some_folder/your_image.png - hide: false - search_exclude: true - metadata_key1: metadata_value1 - metadata_key2: metadata_value2 from IPython.display import HTML HTML(fig.to_html()) . Hello Welcome to my kernel this is my first Proper kernel with some EDA and choropleth maps DO UPVOTE IF YOU LIKE IT :D let&#39;s dive into what I have done below i have simply loaded the kaggle provided datasets . # This Python 3 environment comes with many helpful analytics libraries installed # It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python # For example, here&#39;s several helpful packages to load in import numpy as np # linear algebra import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv) # Input data files are available in the &quot;../input/&quot; directory. # For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory import os for dirname, _, filenames in os.walk(&#39;/kaggle/input&#39;): for filename in filenames: print(os.path.join(dirname, filename)) # Any results you write to the current directory are saved as output. . /kaggle/input/covid19-global-forecasting-week-4/test.csv /kaggle/input/covid19-global-forecasting-week-4/train.csv /kaggle/input/covid19-global-forecasting-week-4/submission.csv /kaggle/input/covid19-in-india/StatewiseTestingDetails.csv /kaggle/input/covid19-in-india/IndividualDetails.csv /kaggle/input/covid19-in-india/covid_19_india.csv /kaggle/input/covid19-in-india/HospitalBedsIndia.csv /kaggle/input/covid19-in-india/ICMRTestingDetails.csv /kaggle/input/covid19-in-india/population_india_census2011.csv /kaggle/input/covid19-in-india/AgeGroupDetails.csv /kaggle/input/covid19-in-india/ICMRTestingLabs.csv /kaggle/input/india-shape/ind_shape/IND_adm1.prj /kaggle/input/india-shape/ind_shape/IND_adm2.shx /kaggle/input/india-shape/ind_shape/IND_adm3.shx /kaggle/input/india-shape/ind_shape/IND_adm0.dbf /kaggle/input/india-shape/ind_shape/IND_adm3.csv /kaggle/input/india-shape/ind_shape/IND_adm0.csv /kaggle/input/india-shape/ind_shape/IND_adm1.csv /kaggle/input/india-shape/ind_shape/IND_adm0.shp /kaggle/input/india-shape/ind_shape/IND_adm3.prj /kaggle/input/india-shape/ind_shape/IND_adm1.dbf /kaggle/input/india-shape/ind_shape/IND_adm2.shp /kaggle/input/india-shape/ind_shape/IND_adm3.shp /kaggle/input/india-shape/ind_shape/IND_adm0.shx /kaggle/input/india-shape/ind_shape/IND_adm1.cpg /kaggle/input/india-shape/ind_shape/IND_adm2.cpg /kaggle/input/india-shape/ind_shape/IND_adm2.prj /kaggle/input/india-shape/ind_shape/IND_adm0.cpg /kaggle/input/india-shape/ind_shape/license.txt /kaggle/input/india-shape/ind_shape/IND_adm2.csv /kaggle/input/india-shape/ind_shape/IND_adm2.dbf /kaggle/input/india-shape/ind_shape/IND_adm0.prj /kaggle/input/india-shape/ind_shape/IND_adm3.dbf /kaggle/input/india-shape/ind_shape/IND_adm1.shx /kaggle/input/india-shape/ind_shape/IND_adm1.shp /kaggle/input/india-shape/ind_shape/IND_adm3.cpg /kaggle/input/countryshape/ne_110m_admin_0_countries.shp /kaggle/input/countryshape/ne_110m_admin_0_countries.README.html /kaggle/input/countryshape/ne_110m_admin_0_countries.VERSION.txt /kaggle/input/countryshape/ne_110m_admin_0_countries.dbf /kaggle/input/countryshape/ne_110m_admin_0_countries.prj /kaggle/input/countryshape/ne_110m_admin_0_countries.shx /kaggle/input/countryshape/ne_110m_admin_0_countries.cpg . train_df=pd.read_csv(&quot;/kaggle/input/covid19-global-forecasting-week-4/train.csv&quot;) . Below I have taken the india part out of the dataframe provided and did some plotting . train_df[train_df[&quot;Country_Region&quot;]==&quot;India&quot;] . Id Province_State Country_Region Date ConfirmedCases Fatalities . 13440 15961 | NaN | India | 2020-01-22 | 0.0 | 0.0 | . 13441 15962 | NaN | India | 2020-01-23 | 0.0 | 0.0 | . 13442 15963 | NaN | India | 2020-01-24 | 0.0 | 0.0 | . 13443 15964 | NaN | India | 2020-01-25 | 0.0 | 0.0 | . 13444 15965 | NaN | India | 2020-01-26 | 0.0 | 0.0 | . ... ... | ... | ... | ... | ... | ... | . 13531 16052 | NaN | India | 2020-04-22 | 21370.0 | 681.0 | . 13532 16053 | NaN | India | 2020-04-23 | 23077.0 | 721.0 | . 13533 16054 | NaN | India | 2020-04-24 | 24530.0 | 780.0 | . 13534 16055 | NaN | India | 2020-04-25 | 26283.0 | 825.0 | . 13535 16056 | NaN | India | 2020-04-26 | 27890.0 | 881.0 | . 96 rows × 6 columns . india_df=train_df[train_df[&quot;Country_Region&quot;]==&quot;India&quot;] #india_df[&quot;day&quot;]=india_df[&quot;Date&quot;].apply(lambda x:int(x[-2:]) ) #india_df[&quot;Month&quot;]=india_df[&quot;Date&quot;].apply(months ) india_df[&quot;ConfirmedCases&quot;]=india_df[&quot;ConfirmedCases&quot;].apply(lambda x: int(x)) . /opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:5: SettingWithCopyWarning: A value is trying to be set on a copy of a slice from a DataFrame. Try using .loc[row_indexer,col_indexer] = value instead See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy &#34;&#34;&#34; . NOW IN THIS WHOLE NOTEBOOK I HAVE SPLIT THE DATE INTO WEEKS.....IF YOU SEE WEEK 4 IT MEANS IT IS THE 4th WEEK OF THE YEAR!!!!! NOT MONTH . from datetime import datetime india_df[&quot;Date&quot;]=india_df[&#39;Date&#39;].apply(lambda x:datetime.strptime(x, &#39;%Y-%m-%d&#39;)) . /opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:2: SettingWithCopyWarning: A value is trying to be set on a copy of a slice from a DataFrame. Try using .loc[row_indexer,col_indexer] = value instead See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy . . Below I have made a week column and added it to the dataframe and did some simple plottings I have done using SEABORN CATPLOT . india_df[&quot;week&quot;]=&quot;week_&quot;+ str(india_df[&quot;Date&quot;].dt.week) . /opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:1: SettingWithCopyWarning: A value is trying to be set on a copy of a slice from a DataFrame. Try using .loc[row_indexer,col_indexer] = value instead See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy &#34;&#34;&#34;Entry point for launching an IPython kernel. . india_df[&quot;week&quot;]=india_df[&quot;Date&quot;].dt.week.apply(lambda x: x) . /opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:1: SettingWithCopyWarning: A value is trying to be set on a copy of a slice from a DataFrame. Try using .loc[row_indexer,col_indexer] = value instead See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy &#34;&#34;&#34;Entry point for launching an IPython kernel. . import matplotlib.pyplot as plt import seaborn as sns #fig.set_size_inches(12, 18) sns.catplot(data=india_df.groupby([&quot;week&quot;]).max().reset_index(),x=&quot;week&quot;,y=&quot;ConfirmedCases&quot;,kind=&quot;bar&quot;) . &lt;seaborn.axisgrid.FacetGrid at 0x7f1bad636f98&gt; . train_df[&quot;day&quot;]=train_df[&quot;Date&quot;].apply(lambda x:int(x[-2:]) ) #train_df[&quot;Month&quot;]=train_df[&quot;Date&quot;].apply(months ) train_df[&quot;ConfirmedCases&quot;]=train_df[&quot;ConfirmedCases&quot;].apply(lambda x: int(x)) . sns.catplot(data=india_df.groupby([&quot;week&quot;]).max().reset_index(),x=&quot;week&quot;,y=&quot;Fatalities&quot;,kind=&quot;bar&quot;) . &lt;seaborn.axisgrid.FacetGrid at 0x7f1be5865898&gt; . train_df=train_df.drop(&quot;Province_State&quot;,axis=1) from datetime import datetime from datetime import datetime train_df[&quot;Date&quot;]=train_df[&#39;Date&#39;].apply(lambda x:datetime.strptime(x, &#39;%Y-%m-%d&#39;)) train_df[&quot;week&quot;]=&quot;week_&quot;+ str(train_df[&quot;Date&quot;].dt.week) train_df[&quot;week&quot;]=train_df[&quot;Date&quot;].dt.week.apply(lambda x: x) train_df[&quot;day&quot;]=train_df[&quot;Date&quot;].dt.day.apply(lambda x: x) train_df[&quot;month&quot;]=train_df[&quot;Date&quot;].dt.month.apply(lambda x: int(x)) . train_df=train_df.drop(&quot;Date&quot;,axis=1) . Below I have used Plotly to create a CHOROPLETH Map of The CoronaVirus to the latest week . import numpy as np import pandas as pd import plotly as py import plotly.express as px import plotly.graph_objs as go from plotly.subplots import make_subplots from plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot init_notebook_mode(connected=True) country_df=train_df.groupby([&#39;Country_Region&#39;, &#39;week&#39;]).max().reset_index().sort_values(&#39;week&#39;, ascending=False) country_df = country_df.drop_duplicates(subset = [&#39;Country_Region&#39;]) country_df = country_df[country_df[&#39;ConfirmedCases&#39;]&gt;0] data = dict(type=&#39;choropleth&#39;, locations = country_df[&#39;Country_Region&#39;], locationmode = &#39;country names&#39;, z = country_df[&#39;ConfirmedCases&#39;], text = country_df[&#39;Country_Region&#39;], colorbar = {&#39;title&#39;:&#39;CONFIRMED CASES&#39;}, colorscale=[[0, &#39;rgb(224,255,255)&#39;], [0.01, &#39;rgb(166,206,227)&#39;], [0.02, &#39;rgb(31,120,180)&#39;], [0.03, &#39;rgb(178,223,138)&#39;], [0.05, &#39;rgb(51,160,44)&#39;], [0.10, &#39;rgb(251,154,153)&#39;], [0.20, &#39;rgb(255,255,0)&#39;], [1, &#39;rgb(227,26,28)&#39;]], reversescale = False ) layout = dict(title=&#39;COVID-19 CASES AROUND THE WORLD&#39;, geo = dict(showframe = True, projection={&#39;type&#39;:&#39;mercator&#39;})) choromap = go.Figure(data = [data], layout = layout) iplot(choromap, validate=False) . This again using plotly I have created the map which you can interact with the slider to see how the spread of coronavirus has affected the Countries starting from the 4th week of the year that was in January and till now in April YOU CAN HOVER FOR INFO OF THE CASES . df_countrydate = train_df[train_df[&#39;ConfirmedCases&#39;]&gt;0] df_countrydate = df_countrydate.groupby([&#39;week&#39;,&#39;Country_Region&#39;]).max().reset_index() df_countrydate fig = px.choropleth(df_countrydate, locations=&quot;Country_Region&quot;, locationmode = &quot;country names&quot;, color=&quot;ConfirmedCases&quot;, hover_name=&quot;Country_Region&quot;, animation_frame=&quot;week&quot;, color_continuous_scale=&quot;Greens&quot; ) fig.update_layout( title_text = &#39;Global Spread of Coronavirus&#39;, title_x = 0.5, geo=dict( showframe = False, showcoastlines = False, )) fig.show() . df_countrydate[df_countrydate[&quot;Country_Region&quot;]==&quot;India&quot;] . week Country_Region Id ConfirmedCases Fatalities day month . 19 5 | India | 15972 | 2 | 0.0 | 31 | 2 | . 45 6 | India | 15979 | 3 | 0.0 | 9 | 2 | . 72 7 | India | 15986 | 3 | 0.0 | 16 | 2 | . 99 8 | India | 15993 | 3 | 0.0 | 23 | 2 | . 146 9 | India | 16000 | 3 | 0.0 | 29 | 3 | . 223 10 | India | 16007 | 39 | 0.0 | 8 | 3 | . 342 11 | India | 16014 | 113 | 2.0 | 15 | 3 | . 494 12 | India | 16021 | 396 | 7.0 | 22 | 3 | . 665 13 | India | 16028 | 1024 | 27.0 | 29 | 3 | . 844 14 | India | 16035 | 3588 | 99.0 | 31 | 4 | . 1027 15 | India | 16042 | 9205 | 331.0 | 12 | 4 | . 1211 16 | India | 16049 | 17615 | 559.0 | 19 | 4 | . 1395 17 | India | 16056 | 27890 | 881.0 | 26 | 4 | . I would like to thank Mr. SRK for the dataset on COVID-19 IN INDIA ===&gt; https://www.kaggle.com/sudalairajkumar/covid19-in-india which i have used below . df_india=pd.read_csv(&quot;/kaggle/input/covid19-in-india/covid_19_india.csv&quot;) df_india[&quot;Date&quot;]=df_india[&#39;Date&#39;].apply(lambda x:datetime.strptime(x, &#39;%d/%m/%y&#39;)) df_india[&quot;week&quot;]=&quot;week_&quot;+ str(df_india[&quot;Date&quot;].dt.week) df_india[&quot;week&quot;]=df_india[&quot;Date&quot;].dt.week.apply(lambda x: x) df_india.head() df_india_grouped=df_india.groupby([&quot;State/UnionTerritory&quot;,&quot;week&quot;]).max().reset_index().sort_values(&quot;week&quot;,ascending=False) df_india_grouped=df_india_grouped.drop_duplicates(subset=[&quot;State/UnionTerritory&quot;]) . The Bar Plot provides info About the Statewise Confirmed Cases, You can hover on them . df_india_grouped fig = px.scatter(df_india_grouped, x=&quot;Confirmed&quot;, y=&quot;State/UnionTerritory&quot;, title=&quot;COVID CASES CONFIRMED IN INDIAN STATES&quot;, labels={&quot;COVID CASES CONFIRMED IN INDIAN STATES&quot;} # customize axis label ) fig = px.bar(df_india_grouped, x=&#39;Confirmed&#39;, y=&#39;State/UnionTerritory&#39;, hover_data=[&#39;Confirmed&#39;, &#39;State/UnionTerritory&#39;], color=&#39;Confirmed&#39;, orientation=&#39;h&#39;, text=&quot;Confirmed&quot;, height=1400) fig.update_traces( textposition=&#39;outside&#39;) fig.update_layout(uniformtext_minsize=8, uniformtext_mode=&#39;show&#39;) fig.show() . Here I have added a new column called pending which is basically how many patients are still being treated , I am going to use this in the below piechart I have created for each state depicting the states and how many cases are cured, deaths and pending . df_india_grouped[&quot;pending&quot;]=df_india_grouped[&quot;Confirmed&quot;]-df_india_grouped[&quot;Deaths&quot;]-df_india_grouped[&quot;Cured&quot;] . &#39;&#39;&#39;df_india_grouped labels=df_india_grouped[&quot;State/UnionTerritory&quot;] values=df_india_grouped[&quot;Confirmed&quot;] fig = go.Figure(data=[go.Pie(labels=labels, values=values, hole=.3)]) fig.show()&#39;&#39;&#39; l=list(df_india_grouped[&quot;State/UnionTerritory&quot;]) fig = make_subplots(rows=11, cols=3,subplot_titles=l,specs=[[{&#39;type&#39;:&#39;domain&#39;}, {&#39;type&#39;:&#39;domain&#39;},{&#39;type&#39;:&#39;domain&#39;}],[{&#39;type&#39;:&#39;domain&#39;}, {&#39;type&#39;:&#39;domain&#39;},{&#39;type&#39;:&#39;domain&#39;}],[{&#39;type&#39;:&#39;domain&#39;}, {&#39;type&#39;:&#39;domain&#39;},{&#39;type&#39;:&#39;domain&#39;}],[{&#39;type&#39;:&#39;domain&#39;}, {&#39;type&#39;:&#39;domain&#39;},{&#39;type&#39;:&#39;domain&#39;}],[{&#39;type&#39;:&#39;domain&#39;}, {&#39;type&#39;:&#39;domain&#39;},{&#39;type&#39;:&#39;domain&#39;}],[{&#39;type&#39;:&#39;domain&#39;}, {&#39;type&#39;:&#39;domain&#39;},{&#39;type&#39;:&#39;domain&#39;}],[{&#39;type&#39;:&#39;domain&#39;}, {&#39;type&#39;:&#39;domain&#39;},{&#39;type&#39;:&#39;domain&#39;}],[{&#39;type&#39;:&#39;domain&#39;}, {&#39;type&#39;:&#39;domain&#39;},{&#39;type&#39;:&#39;domain&#39;}],[{&#39;type&#39;:&#39;domain&#39;}, {&#39;type&#39;:&#39;domain&#39;},{&#39;type&#39;:&#39;domain&#39;}],[{&#39;type&#39;:&#39;domain&#39;}, {&#39;type&#39;:&#39;domain&#39;},{&#39;type&#39;:&#39;domain&#39;}],[{&#39;type&#39;:&#39;domain&#39;}, {&#39;type&#39;:&#39;domain&#39;},{&#39;type&#39;:&#39;domain&#39;}]]) a=1 b=1 for i in l: temp_df=df_india_grouped[df_india_grouped[&quot;State/UnionTerritory&quot;]==i] #print(int(temp_df[&quot;Deaths&quot;])) values=[int(temp_df[&quot;Deaths&quot;]),int(temp_df[&quot;Cured&quot;]),int(temp_df[&quot;pending&quot;])] labels=[&quot;Deaths&quot;,&quot;Cured&quot;,&quot;pending&quot;] #annot.append(dict(text=i,font_size=10, showarrow=False)) fig.add_trace(go.Pie(labels=labels, textposition=&quot;inside&quot;,values=values, name=i),a, b) if b==3 and a&lt;11: a=a+1 if b+1&gt;3: b=1 else: b=b+1 fig.update_traces(hole=.4) fig.update_layout( height=1900,width=1000 ) fig.update(layout_title_text=&#39;StateWise analysis of Positive cases&#39;) #fig = go.Figure(fig) fig.show() #iplot(fig) . testing_df=pd.read_csv(&quot;/kaggle/input/covid19-in-india/StatewiseTestingDetails.csv&quot;) . testing_df[&quot;Date&quot;]=testing_df[&#39;Date&#39;].apply(lambda x:datetime.strptime(x, &#39;%Y-%m-%d&#39;)) testing_df[&quot;week&quot;]=&quot;week_&quot;+ str(testing_df[&quot;Date&quot;].dt.week) testing_df[&quot;week&quot;]=testing_df[&quot;Date&quot;].dt.week.apply(lambda x: x) testing_df.head() testing_df_grouped=testing_df.groupby([&quot;State&quot;,&quot;week&quot;]).max().reset_index().sort_values(&quot;week&quot;,ascending=False) . testing_df_grouped=testing_df_grouped.drop_duplicates(subset=[&quot;State&quot;]) . states=list(testing_df_grouped[&quot;State&quot;]) fig = go.Figure(data=[ go.Bar(name=&#39;Negative&#39;, x=states, y=list(testing_df_grouped[&quot;Negative&quot;])), go.Bar(name=&#39;Positive&#39;, x=states, y=list(testing_df_grouped[&quot;Positive&quot;])), ]) fig.update_layout(barmode=&#39;stack&#39;) fig.show() . Now it is time to plot another choropleth map but this time for India staetwise, for this I added a dataset containing the shape files indian state . import geopandas as gpd shapefile=&quot;/kaggle/input/india-shape/ind_shape/IND_adm1.shp&quot; gdf=gpd.read_file(shapefile)[[&quot;NAME_1&quot;,&quot;geometry&quot;]] gdf.columns = [&#39;states&#39;,&#39;geometry&#39;] gdf.loc[31,&quot;states&quot;]=&quot;Telengana&quot; gdf.loc[34,&quot;states&quot;]=&quot;Uttarakhand&quot; gdf.loc[25,&quot;states&quot;]=&quot;Odisha&quot; #gdf[gdf[&quot;states&quot;]==&quot;Orissa&quot;] . Below I have merged the Geopandas dataframe containing geometry and state names with our dataset of covid-19 indian states and used a json converted to convert it into json . merged_grouped = gdf.merge(df_india_grouped, left_on = &#39;states&#39;, right_on = &#39;State/UnionTerritory&#39;).drop([&quot;Date&quot;],axis=1) import json merged_json_grouped = json.loads(merged_grouped.to_json()) json_data_grouped = json.dumps(merged_json_grouped) . I have used Bokeh instaed of plotly here instead of plotly to demonstarte another method that we can create Choropleth map although we can see it requires more code and can get complicated . from bokeh.io import output_notebook, show, output_file from bokeh.plotting import figure from bokeh.models import GeoJSONDataSource, LinearColorMapper, ColorBar,LabelSet from bokeh.palettes import brewer from bokeh.models import Slider, HoverTool geosource = GeoJSONDataSource(geojson = json_data_grouped) palette = brewer[&#39;YlGnBu&#39;][8] palette = palette[::-1] color_mapper = LinearColorMapper(palette = palette, low = 0, high = max(merged_grouped[&quot;Confirmed&quot;])) tick_labels = {&#39;0&#39;: &#39;0&#39;, &#39;100&#39;: &#39;100&#39;, &#39;200&#39;:&#39;200&#39;, &#39;400&#39;:&#39;400&#39;, &#39;800&#39;:&#39;800&#39;, &#39;1200&#39;:&#39;1200&#39;, &#39;1400&#39;:&#39;1400&#39;,&#39;1800&#39;:&#39;1800&#39;, &#39;2000&#39;: &#39;2000&#39;} hover = HoverTool(tooltips = [ (&#39;states&#39;,&#39;@states&#39;),(&#39;Confirmed_Cases&#39;, &#39;@Confirmed&#39;)]) color_bar = ColorBar(color_mapper=color_mapper, label_standoff=8,width = 500, height = 20, border_line_color=None,location = (0,0), orientation = &#39;horizontal&#39;, major_label_overrides = tick_labels) p = figure(title = &#39;CoronaVirus Confirmed States(HOVER MOUSE FOR INFO)&#39;, plot_height = 600 , plot_width = 950, toolbar_location = None,tools=[hover]) p.xgrid.grid_line_color = None p.ygrid.grid_line_color = None p.patches(&#39;xs&#39;,&#39;ys&#39;, source = geosource,fill_color = {&#39;field&#39; :&#39;Confirmed&#39;, &#39;transform&#39; : color_mapper},name=&quot;states&quot;, line_color = &#39;black&#39;, line_width = 0.25, fill_alpha = 1) labels = LabelSet(x=&#39;xs&#39;, y=&#39;ys&#39;, text=&#39;states&#39;, x_offset=5, y_offset=5, source=geosource) p.add_layout(color_bar, &#39;below&#39;) output_notebook() #Display figure. show(p) . Loading BokehJS ... country_df=df_india.groupby([&quot;week&quot;,&quot;State/UnionTerritory&quot;]).max().reset_index() country_df.drop([&quot;Date&quot;,&quot;ConfirmedIndianNational&quot;,&quot;ConfirmedForeignNational&quot;,&quot;Deaths&quot;,&quot;Cured&quot;,&quot;Time&quot;],axis=1,inplace=True) . Now over here the same way we created the animation of the world map before , we want to create it similary for inidian states, we are using plotly instead of Bokeh because for Bokeh we needed to create a bokeh server to get that interactivity , but we can simply get it more easily with plotly ALSO NOTE:** Below in the code i have used geoseries function SIMPLIFY() as the plot created was very laggy due to the multiploygon geometry of the indian states so using SIMPLIFY(Tolerance=0.02) which kind of straightens some wiggles and curves to a line, but still I think a 0.02 tolerance provides an accurate shape of the map . shapefile=&quot;/kaggle/input/india-shape/ind_shape/IND_adm1.shx&quot; gdf=gpd.read_file(shapefile)[[&quot;NAME_1&quot;,&quot;geometry&quot;]] gdf[&quot;geometry&quot;]=gdf[&quot;geometry&quot;].simplify(0.02, preserve_topology=True) gdf gdf.columns = [&#39;states&#39;,&#39;geometry&#39;] gdf.loc[31,&quot;states&quot;]=&quot;Telengana&quot; gdf.loc[34,&quot;states&quot;]=&quot;Uttarakhand&quot; gdf.loc[25,&quot;states&quot;]=&quot;Odisha&quot; merged_grouped = gdf#.merge(df_india_grouped[[&quot;State/UnionTerritory&quot;,&quot;geo&quot;]], left_on = &#39;states&#39;, right_on = &#39;State/UnionTerritory&#39;) merged_json_grouped = json.loads(merged_grouped.to_json()) json_data_grouped = json.dumps(merged_json_grouped) for i in merged_json_grouped[&quot;features&quot;]: i[&quot;id&quot;]=i[&quot;properties&quot;][&quot;states&quot;] . country_df=df_india.groupby([&quot;State/UnionTerritory&quot;,&quot;week&quot;]).max().reset_index().sort_values(&quot;week&quot;,ascending=True) country_df=country_df.drop([&#39;Sno&#39;, &#39;Date&#39;, &#39;Time&#39;, &#39;ConfirmedIndianNational&#39;, &#39;ConfirmedForeignNational&#39;, &#39;Cured&#39;, &#39;Deaths&#39;,],axis=1) . This Map created using plotly is interactive starting from week 4 , we can see it started from kerala and within few week it was massively spread over the Indian States . fig = px.choropleth(country_df, geojson=merged_json_grouped, locations=&quot;State/UnionTerritory&quot;, color=&quot;Confirmed&quot;, hover_name=&quot;State/UnionTerritory&quot;, animation_frame=&quot;week&quot;, color_continuous_scale=[&quot;yellow&quot;,&quot;orange&quot;,&quot;red&quot;], labels={&#39;Confirmed&#39;:&#39;Confirmed&#39;} ) fig.update_geos(fitbounds=&quot;locations&quot;, visible=False,projection_type=&quot;natural earth&quot;) fig.update_layout( title_text = &#39;India Spread of Coronavirus&#39;, title_x = 0.5, geo=dict( showframe = False, showcoastlines = False, )) fig.show() . Now we go for modelling our data , I am going to use XGBOOST although I am still working and on different models so this could be updated again , If you have any suggestions please do tell me in the comments :D . newtestdf=pd.read_csv(&quot;/kaggle/input/covid19-global-forecasting-week-4/test.csv&quot;) newtestdf . ForecastId Province_State Country_Region Date . 0 1 | NaN | Afghanistan | 2020-04-02 | . 1 2 | NaN | Afghanistan | 2020-04-03 | . 2 3 | NaN | Afghanistan | 2020-04-04 | . 3 4 | NaN | Afghanistan | 2020-04-05 | . 4 5 | NaN | Afghanistan | 2020-04-06 | . ... ... | ... | ... | ... | . 13454 13455 | NaN | Zimbabwe | 2020-05-10 | . 13455 13456 | NaN | Zimbabwe | 2020-05-11 | . 13456 13457 | NaN | Zimbabwe | 2020-05-12 | . 13457 13458 | NaN | Zimbabwe | 2020-05-13 | . 13458 13459 | NaN | Zimbabwe | 2020-05-14 | . 13459 rows × 4 columns . using inbuilt pandas encoder i encoded the names of the country regions . train_df.head() . Id Country_Region ConfirmedCases Fatalities day week month . 0 1 | Afghanistan | 0 | 0.0 | 22 | 4 | 1 | . 1 2 | Afghanistan | 0 | 0.0 | 23 | 4 | 1 | . 2 3 | Afghanistan | 0 | 0.0 | 24 | 4 | 1 | . 3 4 | Afghanistan | 0 | 0.0 | 25 | 4 | 1 | . 4 5 | Afghanistan | 0 | 0.0 | 26 | 4 | 1 | . train_df.columns . Index([&#39;Id&#39;, &#39;Country_Region&#39;, &#39;ConfirmedCases&#39;, &#39;Fatalities&#39;, &#39;day&#39;, &#39;week&#39;, &#39;month&#39;], dtype=&#39;object&#39;) . I took inspiration of the hyperparamters from here https://www.kaggle.com/pradeepkumarrajkumar/xgb-regressor . We scale using minmaxscaler and also transform the country data into numeric using label encoding(not get_dummies as i did not get a good score before :D) . from sklearn.preprocessing import LabelEncoder,StandardScaler,MinMaxScaler train_df[&#39;ConfirmedCases&#39;] = train_df[&#39;ConfirmedCases&#39;].apply(int) train_df[&#39;Fatalities&#39;] = train_df[&#39;Fatalities&#39;].apply(int) cases = train_df.ConfirmedCases fatal=train_df.Fatalities lb = LabelEncoder() del train_df[&quot;Fatalities&quot;] del train_df[&quot;ConfirmedCases&quot;] #del train_df[&quot;Id&quot;] train_df[&#39;Country_Region&#39;] = lb.fit_transform(train_df[&#39;Country_Region&#39;]) scaler = MinMaxScaler() X_train = scaler.fit_transform(train_df.drop([&quot;Id&quot;,&quot;week&quot;],axis=1).values) . from xgboost import XGBRegressor model = XGBRegressor(n_estimators = 2500 , random_state = 0 , max_depth = 27) model.fit(X_train,cases) . XGBRegressor(base_score=0.5, booster=None, colsample_bylevel=1, colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1, importance_type=&#39;gain&#39;, interaction_constraints=None, learning_rate=0.300000012, max_delta_step=0, max_depth=27, min_child_weight=1, missing=nan, monotone_constraints=None, n_estimators=2500, n_jobs=0, num_parallel_tree=1, objective=&#39;reg:squarederror&#39;, random_state=0, reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1, tree_method=None, validate_parameters=False, verbosity=None) . model.score(X_train,cases) . 0.5526724827782268 . newtestdf[&quot;Date&quot;]=newtestdf[&#39;Date&#39;].apply(lambda x:datetime.strptime(x, &#39;%Y-%m-%d&#39;)) newtestdf[&quot;week&quot;]=&quot;week_&quot;+ str(newtestdf[&quot;Date&quot;].dt.week) newtestdf[&quot;week&quot;]=newtestdf[&quot;Date&quot;].dt.week.apply(lambda x: x) newtestdf[&quot;day&quot;]=newtestdf[&quot;Date&quot;].dt.day.apply(lambda x: x) newtestdf[&quot;month&quot;]=newtestdf[&quot;Date&quot;].dt.month.apply(lambda x: int(x)) newtestdf[&#39;Country_Region&#39;] = lb.fit_transform(newtestdf[&#39;Country_Region&#39;]) newtestdf=newtestdf.drop([&quot;Province_State&quot;,&quot;Date&quot;],axis=1) newtestdf . ForecastId Country_Region week day month . 0 1 | 0 | 14 | 2 | 4 | . 1 2 | 0 | 14 | 3 | 4 | . 2 3 | 0 | 14 | 4 | 4 | . 3 4 | 0 | 14 | 5 | 4 | . 4 5 | 0 | 15 | 6 | 4 | . ... ... | ... | ... | ... | ... | . 13454 13455 | 183 | 19 | 10 | 5 | . 13455 13456 | 183 | 20 | 11 | 5 | . 13456 13457 | 183 | 20 | 12 | 5 | . 13457 13458 | 183 | 20 | 13 | 5 | . 13458 13459 | 183 | 20 | 14 | 5 | . 13459 rows × 5 columns . X_test = scaler.fit_transform(newtestdf.drop([&quot;ForecastId&quot;,&quot;week&quot;],axis=1).values) cases_pred = model.predict(X_test) . cases_pred = np.around(cases_pred,decimals = 0) x_train_cas = [] for i in range(len(X_train)): x = list(X_train[i]) x.append(cases[i]) x_train_cas.append(x) . x_train_cas = np.array(x_train_cas) model = XGBRegressor(n_estimators = 2500 , random_state = 0 , max_depth = 27) model.fit(x_train_cas,fatal) . XGBRegressor(base_score=0.5, booster=None, colsample_bylevel=1, colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1, importance_type=&#39;gain&#39;, interaction_constraints=None, learning_rate=0.300000012, max_delta_step=0, max_depth=27, min_child_weight=1, missing=nan, monotone_constraints=None, n_estimators=2500, n_jobs=0, num_parallel_tree=1, objective=&#39;reg:squarederror&#39;, random_state=0, reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1, tree_method=None, validate_parameters=False, verbosity=None) . x_test_cas = [] for i in range(len(X_test)): x = list(X_test[i]) x.append(cases_pred[i]) x_test_cas.append(x) x_test_cas[0] . [0.0, 0.034482758620689655, 0.0, -0.0] . x_test_cas = np.array(x_test_cas) fatalities_pred =model.predict(x_test_cas) fatalities_pred = np.around(fatalities_pred,decimals = 0) . submission = pd.read_csv(&quot;../input/covid19-global-forecasting-week-4/submission.csv&quot;) submission[&#39;ConfirmedCases&#39;] = cases_pred submission[&#39;Fatalities&#39;] = fatalities_pred submission.to_csv(&quot;submission.csv&quot; , index = False) . THANK YOU :D PLEASE DO UPVOTE!!!!! . THANK YOU!!!! . File &#34;&lt;ipython-input-44-99bf4749447d&gt;&#34;, line 1 THANK YOU!!!! ^ SyntaxError: invalid syntax .",
            "url": "https://omegaji.github.io/DataScienceBlog/2020/05/21/covid-19-india-maps-eda-xgboost.html",
            "relUrl": "/2020/05/21/covid-19-india-maps-eda-xgboost.html",
            "date": " • May 21, 2020"
        }
        
    
  
    
        ,"post6": {
            "title": "Fastpages Notebook Blog Post",
            "content": "About . This notebook is a demonstration of some of capabilities of fastpages with notebooks. . With fastpages you can save your jupyter notebooks into the _notebooks folder at the root of your repository, and they will be automatically be converted to Jekyll compliant blog posts! . Front Matter . The first cell in your Jupyter Notebook or markdown blog post contains front matter. Front matter is metadata that can turn on/off options in your Notebook. It is formatted like this: . # &quot;My Title&quot; &gt; &quot;Awesome summary&quot; - toc:true- branch: master- badges: true- comments: true - author: Hamel Husain &amp; Jeremy Howard - categories: [fastpages, jupyter] . Setting toc: true will automatically generate a table of contents | Setting badges: true will automatically include GitHub and Google Colab links to your notebook. | Setting comments: true will enable commenting on your blog post, powered by utterances. | . The title and description need to be enclosed in double quotes only if they include special characters such as a colon. More details and options for front matter can be viewed on the front matter section of the README. . Markdown Shortcuts . A #hide comment at the top of any code cell will hide both the input and output of that cell in your blog post. . A #hide_input comment at the top of any code cell will only hide the input of that cell. . The comment #hide_input was used to hide the code that produced this. . put a #collapse-hide flag at the top of any cell if you want to hide that cell by default, but give the reader the option to show it: . #collapse-hide import pandas as pd import altair as alt . . put a #collapse-show flag at the top of any cell if you want to show that cell by default, but give the reader the option to hide it: . #collapse-show cars = &#39;https://vega.github.io/vega-datasets/data/cars.json&#39; movies = &#39;https://vega.github.io/vega-datasets/data/movies.json&#39; sp500 = &#39;https://vega.github.io/vega-datasets/data/sp500.csv&#39; stocks = &#39;https://vega.github.io/vega-datasets/data/stocks.csv&#39; flights = &#39;https://vega.github.io/vega-datasets/data/flights-5k.json&#39; . . Interactive Charts With Altair . Charts made with Altair remain interactive. Example charts taken from this repo, specifically this notebook. . Example 1: DropDown . # single-value selection over [Major_Genre, MPAA_Rating] pairs # use specific hard-wired values as the initial selected values selection = alt.selection_single( name=&#39;Select&#39;, fields=[&#39;Major_Genre&#39;, &#39;MPAA_Rating&#39;], init={&#39;Major_Genre&#39;: &#39;Drama&#39;, &#39;MPAA_Rating&#39;: &#39;R&#39;}, bind={&#39;Major_Genre&#39;: alt.binding_select(options=genres), &#39;MPAA_Rating&#39;: alt.binding_radio(options=mpaa)} ) # scatter plot, modify opacity based on selection alt.Chart(movies).mark_circle().add_selection( selection ).encode( x=&#39;Rotten_Tomatoes_Rating:Q&#39;, y=&#39;IMDB_Rating:Q&#39;, tooltip=&#39;Title:N&#39;, opacity=alt.condition(selection, alt.value(0.75), alt.value(0.05)) ) . Example 2: Tooltips . alt.Chart(movies).mark_circle().add_selection( alt.selection_interval(bind=&#39;scales&#39;, encodings=[&#39;x&#39;]) ).encode( x=&#39;Rotten_Tomatoes_Rating:Q&#39;, y=alt.Y(&#39;IMDB_Rating:Q&#39;, axis=alt.Axis(minExtent=30)), # use min extent to stabilize axis title placement tooltip=[&#39;Title:N&#39;, &#39;Release_Date:N&#39;, &#39;IMDB_Rating:Q&#39;, &#39;Rotten_Tomatoes_Rating:Q&#39;] ).properties( width=600, height=400 ) . Example 3: More Tooltips . # select a point for which to provide details-on-demand label = alt.selection_single( encodings=[&#39;x&#39;], # limit selection to x-axis value on=&#39;mouseover&#39;, # select on mouseover events nearest=True, # select data point nearest the cursor empty=&#39;none&#39; # empty selection includes no data points ) # define our base line chart of stock prices base = alt.Chart().mark_line().encode( alt.X(&#39;date:T&#39;), alt.Y(&#39;price:Q&#39;, scale=alt.Scale(type=&#39;log&#39;)), alt.Color(&#39;symbol:N&#39;) ) alt.layer( base, # base line chart # add a rule mark to serve as a guide line alt.Chart().mark_rule(color=&#39;#aaa&#39;).encode( x=&#39;date:T&#39; ).transform_filter(label), # add circle marks for selected time points, hide unselected points base.mark_circle().encode( opacity=alt.condition(label, alt.value(1), alt.value(0)) ).add_selection(label), # add white stroked text to provide a legible background for labels base.mark_text(align=&#39;left&#39;, dx=5, dy=-5, stroke=&#39;white&#39;, strokeWidth=2).encode( text=&#39;price:Q&#39; ).transform_filter(label), # add text labels for stock prices base.mark_text(align=&#39;left&#39;, dx=5, dy=-5).encode( text=&#39;price:Q&#39; ).transform_filter(label), data=stocks ).properties( width=700, height=400 ) . Data Tables . You can display tables per the usual way in your blog: . movies = &#39;https://vega.github.io/vega-datasets/data/movies.json&#39; df = pd.read_json(movies) # display table with pandas df[[&#39;Title&#39;, &#39;Worldwide_Gross&#39;, &#39;Production_Budget&#39;, &#39;Distributor&#39;, &#39;MPAA_Rating&#39;, &#39;IMDB_Rating&#39;, &#39;Rotten_Tomatoes_Rating&#39;]].head() . Title Worldwide_Gross Production_Budget Distributor MPAA_Rating IMDB_Rating Rotten_Tomatoes_Rating . 0 The Land Girls | 146083.0 | 8000000.0 | Gramercy | R | 6.1 | NaN | . 1 First Love, Last Rites | 10876.0 | 300000.0 | Strand | R | 6.9 | NaN | . 2 I Married a Strange Person | 203134.0 | 250000.0 | Lionsgate | None | 6.8 | NaN | . 3 Let&#39;s Talk About Sex | 373615.0 | 300000.0 | Fine Line | None | NaN | 13.0 | . 4 Slam | 1087521.0 | 1000000.0 | Trimark | R | 3.4 | 62.0 | . Images . Local Images . You can reference local images and they will be copied and rendered on your blog automatically. You can include these with the following markdown syntax: . ![](my_icons/fastai_logo.png) . . Remote Images . Remote images can be included with the following markdown syntax: . ![](https://image.flaticon.com/icons/svg/36/36686.svg) . . Animated Gifs . Animated Gifs work, too! . ![](https://upload.wikimedia.org/wikipedia/commons/7/71/ChessPawnSpecialMoves.gif) . . Captions . You can include captions with markdown images like this: . ![](https://www.fast.ai/images/fastai_paper/show_batch.png &quot;Credit: https://www.fast.ai/2020/02/13/fastai-A-Layered-API-for-Deep-Learning/&quot;) . . Other Elements . GitHub Flavored Emojis . Typing I give this post two :+1:! will render this: . I give this post two :+1:! . Tweetcards . Typing &gt; twitter: https://twitter.com/jakevdp/status/1204765621767901185?s=20 will render this: Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 . Youtube Videos . Typing &gt; youtube: https://youtu.be/XfoYk_Z5AkI will render this: . Boxes / Callouts . Typing &gt; Warning: There will be no second warning! will render this: . Warning: There will be no second warning! . Typing &gt; Important: Pay attention! It&#39;s important. will render this: . Important: Pay attention! It&#8217;s important. . Typing &gt; Tip: This is my tip. will render this: . Tip: This is my tip. . Typing &gt; Note: Take note of this. will render this: . Note: Take note of this. . Typing &gt; Note: A doc link to [an example website: fast.ai](https://www.fast.ai/) should also work fine. will render in the docs: . Note: A doc link to an example website: fast.ai should also work fine. . Footnotes . You can have footnotes in notebooks, however the syntax is different compared to markdown documents. This guide provides more detail about this syntax, which looks like this: . For example, here is a footnote {% fn 1 %}. And another {% fn 2 %} {{ &#39;This is the footnote.&#39; | fndetail: 1 }} {{ &#39;This is the other footnote. You can even have a [link](www.github.com)!&#39; | fndetail: 2 }} . For example, here is a footnote 1. . And another 2 . 1. This is the footnote.↩ . 2. This is the other footnote. You can even have a link!↩ .",
            "url": "https://omegaji.github.io/DataScienceBlog/jupyter/2020/02/20/test.html",
            "relUrl": "/jupyter/2020/02/20/test.html",
            "date": " • Feb 20, 2020"
        }
        
    
  
    
        ,"post7": {
            "title": "An Example Markdown Post",
            "content": "Example Markdown Post . Basic setup . Jekyll requires blog post files to be named according to the following format: . YEAR-MONTH-DAY-filename.md . Where YEAR is a four-digit number, MONTH and DAY are both two-digit numbers, and filename is whatever file name you choose, to remind yourself what this post is about. .md is the file extension for markdown files. . The first line of the file should start with a single hash character, then a space, then your title. This is how you create a “level 1 heading” in markdown. Then you can create level 2, 3, etc headings as you wish but repeating the hash character, such as you see in the line ## File names above. . Basic formatting . You can use italics, bold, code font text, and create links. Here’s a footnote 1. Here’s a horizontal rule: . . Lists . Here’s a list: . item 1 | item 2 | . And a numbered list: . item 1 | item 2 | Boxes and stuff . This is a quotation . . You can include alert boxes …and… . . You can include info boxes Images . . Code . You can format text and code per usual . General preformatted text: . # Do a thing do_thing() . Python code and output: . # Prints &#39;2&#39; print(1+1) . 2 . Formatting text as shell commands: . echo &quot;hello world&quot; ./some_script.sh --option &quot;value&quot; wget https://example.com/cat_photo1.png . Formatting text as YAML: . key: value - another_key: &quot;another value&quot; . Tables . Column 1 Column 2 . A thing | Another thing | . Tweetcards . Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 Footnotes . This is the footnote. &#8617; . |",
            "url": "https://omegaji.github.io/DataScienceBlog/markdown/2020/01/14/test-markdown-post.html",
            "relUrl": "/markdown/2020/01/14/test-markdown-post.html",
            "date": " • Jan 14, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "This is where you put the contents of your About page. Like all your pages, it’s in Markdown format. . This website is powered by fastpages 1. . a blogging platform that natively supports Jupyter notebooks in addition to other formats. &#8617; . |",
          "url": "https://omegaji.github.io/DataScienceBlog/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://omegaji.github.io/DataScienceBlog/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}